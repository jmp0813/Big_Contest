{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T07:30:20.151038Z",
     "start_time": "2020-09-02T07:29:18.486999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (2.6.2)\n",
      "Requirement already satisfied: jdcal in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (from openpyxl) (1.4.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (from openpyxl) (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jmp08\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jmp08\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      T_ID   GDAY_DS  HEADER_NO   P_ID  START_CK  BAT_ORDER_NO   PA   AB  RBI  \\\n",
      "0       HH  20160401          0  60404         0             3    1    1    0   \n",
      "1       HH  20160401          0  62700         1             9    2    2    0   \n",
      "2       HH  20160401          0  64086         1             7    6    4    0   \n",
      "3       HH  20160401          0  66740         1             5    6    6    0   \n",
      "4       HH  20160401          0  71347         1             2    6    6    1   \n",
      "...    ...       ...        ...    ...       ...           ...  ...  ...  ...   \n",
      "18679   WO  20161009          0  74215        91           374  402  341   80   \n",
      "18680   WO  20161009          0  78168       139           177  646  560   63   \n",
      "18681   WO  20161009          0  79130        15           251   80   66    7   \n",
      "18682   WO  20161009          0  79300        13           429  106   91    9   \n",
      "18683   WO  20161009          0  79365       122           965  454  411   70   \n",
      "\n",
      "       RUN  ...  BB  IB  HP  KK  GD  ERR  LOB  P_AB_CN  P_HIT_CN  GAME_COUNT  \n",
      "0        0  ...   0   0   0   0   0    0    1        0         0           1  \n",
      "1        1  ...   0   0   0   0   0    1    0        0         0           1  \n",
      "2        0  ...   1   0   0   3   0    0    1        2         0           1  \n",
      "3        0  ...   0   0   0   0   0    0    1        1         1           1  \n",
      "4        1  ...   0   0   0   2   0    0    0        1         0           1  \n",
      "...    ...  ...  ..  ..  ..  ..  ..  ...  ...      ...       ...         ...  \n",
      "18679   72  ...  46   3   9  50  16    6   77      121        35          92  \n",
      "18680  111  ...  69   2  10  58   6   15  113      118        36         140  \n",
      "18681   10  ...  11   0   1  24   1    1   18       20         3          40  \n",
      "18682   16  ...  10   0   0  16   3    0   29       25         4          81  \n",
      "18683   44  ...  27   0   7  93  18    7   71      132        33         127  \n",
      "\n",
      "[18684 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "%run batter_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:05:08.382310Z",
     "start_time": "2020-09-02T13:05:08.376351Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:23: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  data_klasses = (pandas.Series, pandas.DataFrame, pandas.Panel)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objs as go\n",
    "import xgboost\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from statsmodels import tsa\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from plotly.offline import plot\n",
    "from plotly.offline import init_notebook_mode\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "feature, target = batter_data(\"LT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(580, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFeature = feature.loc[0:feature.shape[0]-20]\n",
    "trainTarget = target.loc[0:feature.shape[0]-20]\n",
    "\n",
    "testFeature = feature.loc[feature.shape[0]-20:].reset_index(drop = True)\n",
    "testTarget = target[feature.shape[0]-20:].reset_index(drop = True)\n",
    "trainFeature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "res = scaler.fit(trainFeature)\n",
    "res = scaler.transform(trainFeature)\n",
    "trainFeature = pd.DataFrame(res, columns = trainFeature.columns, index = list(trainFeature.index.values))\n",
    "res = scaler.transform(testFeature)\n",
    "testFeature = pd.DataFrame(res, columns = testFeature.columns, index = list(testFeature.index.values))\n",
    "\n",
    "res = scaler.fit(np.array(trainTarget).reshape(trainTarget.shape[0], 1))\n",
    "trainTarget = scaler.transform(np.array(trainTarget).reshape(trainTarget.shape[0], 1))\n",
    "testTarget = scaler.transform(np.array(testTarget).reshape(testTarget.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Input(shape = trainFeature.shape[1]))\n",
    "model.add(keras.layers.Dense(16, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(32, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(64, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(128, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(128, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(128, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(64, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(32, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(16, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(8, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(4, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(2, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(1, activation = None))\n",
    "model.compile(optimizer = \"Adam\", loss = \"mse\")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(min_delta = 0.0005, patience = 30, restore_best_weights = True)\n",
    "\n",
    "history = model.fit(trainFeature, trainTarget, epochs = 200, validation_split = 0.3, shuffle = True,\n",
    "          use_multiprocessing = True, callbacks = [early_stopping], batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.960 - 0s 1ms/sample - loss: 0.9755 - val_loss: 0.4977\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.511 - 0s 138us/sample - loss: 0.5515 - val_loss: 0.2455\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.217 - 0s 136us/sample - loss: 0.2772 - val_loss: 0.1059\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.140 - 0s 118us/sample - loss: 0.1474 - val_loss: 0.0471\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.123 - 0s 123us/sample - loss: 0.0883 - val_loss: 0.0247\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.063 - 0s 120us/sample - loss: 0.0742 - val_loss: 0.0159\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.062 - 0s 125us/sample - loss: 0.0550 - val_loss: 0.0130\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.063 - 0s 111us/sample - loss: 0.0465 - val_loss: 0.0117\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.034 - 0s 117us/sample - loss: 0.0401 - val_loss: 0.0114\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.063 - 0s 113us/sample - loss: 0.0341 - val_loss: 0.0110\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 118us/sample - loss: 0.0265 - val_loss: 0.0104\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 115us/sample - loss: 0.0318 - val_loss: 0.0097\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.030 - 0s 110us/sample - loss: 0.0269 - val_loss: 0.0090\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.027 - 0s 110us/sample - loss: 0.0240 - val_loss: 0.0086\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 123us/sample - loss: 0.0248 - val_loss: 0.0084\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 111us/sample - loss: 0.0200 - val_loss: 0.0082\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.053 - 0s 115us/sample - loss: 0.0219 - val_loss: 0.0077\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 120us/sample - loss: 0.0196 - val_loss: 0.0075\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 122us/sample - loss: 0.0186 - val_loss: 0.0075\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 130us/sample - loss: 0.0174 - val_loss: 0.0073\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 113us/sample - loss: 0.0160 - val_loss: 0.0071\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 113us/sample - loss: 0.0144 - val_loss: 0.0069\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 115us/sample - loss: 0.0145 - val_loss: 0.0069\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 117us/sample - loss: 0.0130 - val_loss: 0.0068\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 105us/sample - loss: 0.0137 - val_loss: 0.0067\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 110us/sample - loss: 0.0129 - val_loss: 0.0067\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 101us/sample - loss: 0.0129 - val_loss: 0.0066\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 111us/sample - loss: 0.0109 - val_loss: 0.0066\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 103us/sample - loss: 0.0108 - val_loss: 0.0065\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 106us/sample - loss: 0.0098 - val_loss: 0.0065\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 117us/sample - loss: 0.0107 - val_loss: 0.0065\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 117us/sample - loss: 0.0103 - val_loss: 0.0064\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0083 - val_loss: 0.0064\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 127us/sample - loss: 0.0097 - val_loss: 0.0064\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 105us/sample - loss: 0.0109 - val_loss: 0.0064\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 102us/sample - loss: 0.0087 - val_loss: 0.0064\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0088 - val_loss: 0.0064\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 104us/sample - loss: 0.0079 - val_loss: 0.0064\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0074 - val_loss: 0.0065\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 117us/sample - loss: 0.0077 - val_loss: 0.0065\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 118us/sample - loss: 0.0093 - val_loss: 0.0065\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 130us/sample - loss: 0.0076 - val_loss: 0.0065\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 125us/sample - loss: 0.0072 - val_loss: 0.0065\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0079 - val_loss: 0.0066\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0075 - val_loss: 0.0066\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0072 - val_loss: 0.0067\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0075 - val_loss: 0.0065\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0069 - val_loss: 0.0066\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0069 - val_loss: 0.0066\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 115us/sample - loss: 0.0096 - val_loss: 0.0067\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 115us/sample - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0072 - val_loss: 0.0067\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 125us/sample - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 54/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 113us/sample - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 55/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 56/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 57/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 58/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 59/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 127us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 60/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 135us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 61/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0062 - val_loss: 0.0070\n",
      "Epoch 62/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 133us/sample - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 63/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 133us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 64/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 132us/sample - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 65/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 123us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 66/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.078 - 0s 1ms/sample - loss: 0.0780 - val_loss: 0.0145\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.068 - 0s 118us/sample - loss: 0.0537 - val_loss: 0.0164\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.047 - 0s 125us/sample - loss: 0.0388 - val_loss: 0.0143\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.034 - 0s 120us/sample - loss: 0.0306 - val_loss: 0.0122\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.025 - 0s 115us/sample - loss: 0.0258 - val_loss: 0.0106\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 110us/sample - loss: 0.0207 - val_loss: 0.0096\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 115us/sample - loss: 0.0185 - val_loss: 0.0091\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 116us/sample - loss: 0.0177 - val_loss: 0.0084\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 118us/sample - loss: 0.0148 - val_loss: 0.0079\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 121us/sample - loss: 0.0130 - val_loss: 0.0075\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 116us/sample - loss: 0.0118 - val_loss: 0.0072\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 121us/sample - loss: 0.0120 - val_loss: 0.0070\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 115us/sample - loss: 0.0102 - val_loss: 0.0069\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0093 - val_loss: 0.0068\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 115us/sample - loss: 0.0099 - val_loss: 0.0068\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0096 - val_loss: 0.0067\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0082 - val_loss: 0.0067\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0080 - val_loss: 0.0068\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 118us/sample - loss: 0.0080 - val_loss: 0.0069\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 113us/sample - loss: 0.0080 - val_loss: 0.0069\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 86us/sample - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 98us/sample - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 118us/sample - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 99us/sample - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 94us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 102us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 97us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 106us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 135us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 144us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 112us/sample - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 118us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 118us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 108us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 93us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 122us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 125us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 125us/sample - loss: 0.0062 - val_loss: 0.0075\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 105us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 108us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 110us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 118us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.081 - 0s 953us/sample - loss: 0.0784 - val_loss: 0.0369\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.038 - 0s 113us/sample - loss: 0.0511 - val_loss: 0.0244\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.038 - 0s 116us/sample - loss: 0.0327 - val_loss: 0.0154\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 103us/sample - loss: 0.0311 - val_loss: 0.0113\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 119us/sample - loss: 0.0228 - val_loss: 0.0094\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 103us/sample - loss: 0.0247 - val_loss: 0.0091\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.027 - 0s 103us/sample - loss: 0.0212 - val_loss: 0.0084\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 96us/sample - loss: 0.0194 - val_loss: 0.0079\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 106us/sample - loss: 0.0157 - val_loss: 0.0075\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 102us/sample - loss: 0.0159 - val_loss: 0.0072\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 101us/sample - loss: 0.0135 - val_loss: 0.0070\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 96us/sample - loss: 0.0127 - val_loss: 0.0068\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 103us/sample - loss: 0.0120 - val_loss: 0.0067\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 118us/sample - loss: 0.0119 - val_loss: 0.0066\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 115us/sample - loss: 0.0107 - val_loss: 0.0066\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 118us/sample - loss: 0.0105 - val_loss: 0.0065\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 118us/sample - loss: 0.0088 - val_loss: 0.0065\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 117us/sample - loss: 0.0102 - val_loss: 0.0065\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 113us/sample - loss: 0.0091 - val_loss: 0.0065\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 104us/sample - loss: 0.0081 - val_loss: 0.0065\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 108us/sample - loss: 0.0085 - val_loss: 0.0065\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0076 - val_loss: 0.0066\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 125us/sample - loss: 0.0082 - val_loss: 0.0066\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 123us/sample - loss: 0.0073 - val_loss: 0.0067\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 125us/sample - loss: 0.0078 - val_loss: 0.0066\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 145us/sample - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 135us/sample - loss: 0.0077 - val_loss: 0.0068\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 142us/sample - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 120us/sample - loss: 0.0073 - val_loss: 0.0069\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 134us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 98us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 118us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 105us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 118us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 106us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 117us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 110us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 98us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 101us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.002 - 0s 101us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 106us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 106us/sample - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 110us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.091 - 0s 1ms/sample - loss: 0.0584 - val_loss: 0.0133\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.056 - 0s 138us/sample - loss: 0.0429 - val_loss: 0.0121\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.034 - 0s 120us/sample - loss: 0.0322 - val_loss: 0.0124\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 103us/sample - loss: 0.0296 - val_loss: 0.0116\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.026 - 0s 99us/sample - loss: 0.0236 - val_loss: 0.0104\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 123us/sample - loss: 0.0225 - val_loss: 0.0100\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 119us/sample - loss: 0.0194 - val_loss: 0.0090\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.017 - 0s 228us/sample - loss: 0.0176 - val_loss: 0.0084\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 162us/sample - loss: 0.0172 - val_loss: 0.0079\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 115us/sample - loss: 0.0156 - val_loss: 0.0078\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 120us/sample - loss: 0.0143 - val_loss: 0.0074\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 107us/sample - loss: 0.0122 - val_loss: 0.0072\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 108us/sample - loss: 0.0105 - val_loss: 0.0068\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 105us/sample - loss: 0.0116 - val_loss: 0.0066\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 111us/sample - loss: 0.0089 - val_loss: 0.0065\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 99us/sample - loss: 0.0106 - val_loss: 0.0064\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 103us/sample - loss: 0.0091 - val_loss: 0.0063\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 123us/sample - loss: 0.0083 - val_loss: 0.0063\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0085 - val_loss: 0.0063\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 108us/sample - loss: 0.0080 - val_loss: 0.0063\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 96us/sample - loss: 0.0076 - val_loss: 0.0063\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 83us/sample - loss: 0.0077 - val_loss: 0.0064\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0077 - val_loss: 0.0064\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 93us/sample - loss: 0.0073 - val_loss: 0.0064\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 99us/sample - loss: 0.0070 - val_loss: 0.0065\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 106us/sample - loss: 0.0070 - val_loss: 0.0065\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.002 - 0s 106us/sample - loss: 0.0069 - val_loss: 0.0065\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0073 - val_loss: 0.0065\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 98us/sample - loss: 0.0069 - val_loss: 0.0065\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 121us/sample - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 114us/sample - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0068\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.002 - 0s 91us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 88us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 99us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 99us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 96us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 96us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 98us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 108us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 91us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 90us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 97us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 96us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 94us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 92us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.182 - 0s 1ms/sample - loss: 0.1363 - val_loss: 0.0580\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.068 - 0s 145us/sample - loss: 0.0853 - val_loss: 0.0475\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.066 - 0s 128us/sample - loss: 0.0631 - val_loss: 0.0371\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.058 - 0s 121us/sample - loss: 0.0476 - val_loss: 0.0252\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.033 - 0s 108us/sample - loss: 0.0337 - val_loss: 0.0146\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 109us/sample - loss: 0.0245 - val_loss: 0.0091\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 101us/sample - loss: 0.0213 - val_loss: 0.0078\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 106us/sample - loss: 0.0201 - val_loss: 0.0078\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 125us/sample - loss: 0.0176 - val_loss: 0.0077\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0166 - val_loss: 0.0072\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 105us/sample - loss: 0.0158 - val_loss: 0.0072\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 106us/sample - loss: 0.0139 - val_loss: 0.0071\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 113us/sample - loss: 0.0140 - val_loss: 0.0070\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 113us/sample - loss: 0.0122 - val_loss: 0.0068\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 84us/sample - loss: 0.0112 - val_loss: 0.0067\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 91us/sample - loss: 0.0116 - val_loss: 0.0067\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 96us/sample - loss: 0.0105 - val_loss: 0.0067\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 95us/sample - loss: 0.0098 - val_loss: 0.0066\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 167us/sample - loss: 0.0101 - val_loss: 0.0067\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 155us/sample - loss: 0.0090 - val_loss: 0.0067\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 179us/sample - loss: 0.0086 - val_loss: 0.0067\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.009 - 0s 233us/sample - loss: 0.0092 - val_loss: 0.0069\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 0s 184us/sample - loss: 0.0090 - val_loss: 0.0067\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 138us/sample - loss: 0.0082 - val_loss: 0.0069\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 138us/sample - loss: 0.0085 - val_loss: 0.0067\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.007 - 0s 187us/sample - loss: 0.0078 - val_loss: 0.0069\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 155us/sample - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 111us/sample - loss: 0.0076 - val_loss: 0.0069\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 88us/sample - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 138us/sample - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 130us/sample - loss: 0.0077 - val_loss: 0.0069\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 143us/sample - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 102us/sample - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 95us/sample - loss: 0.0073 - val_loss: 0.0069\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 97us/sample - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 133us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 129us/sample - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 138us/sample - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 96us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 127us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 128us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 93us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 90us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 97us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 99us/sample - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 3s - loss: 0.424 - 1s 1ms/sample - loss: 0.3831 - val_loss: 0.1227\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.314 - 0s 138us/sample - loss: 0.2174 - val_loss: 0.0559\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.069 - 0s 155us/sample - loss: 0.1251 - val_loss: 0.0238\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.057 - 0s 160us/sample - loss: 0.0819 - val_loss: 0.0158\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.117 - 0s 123us/sample - loss: 0.0606 - val_loss: 0.0136\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.078 - 0s 120us/sample - loss: 0.0508 - val_loss: 0.0115\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.047 - 0s 129us/sample - loss: 0.0396 - val_loss: 0.0101\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.038 - 0s 106us/sample - loss: 0.0345 - val_loss: 0.0091\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.056 - 0s 122us/sample - loss: 0.0351 - val_loss: 0.0086\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.030 - 0s 131us/sample - loss: 0.0298 - val_loss: 0.0081\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 140us/sample - loss: 0.0255 - val_loss: 0.0078\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.020 - 0s 187us/sample - loss: 0.0205 - val_loss: 0.0075\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 140us/sample - loss: 0.0196 - val_loss: 0.0071\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 131us/sample - loss: 0.0168 - val_loss: 0.0069\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 140us/sample - loss: 0.0152 - val_loss: 0.0067\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 124us/sample - loss: 0.0153 - val_loss: 0.0066\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 97us/sample - loss: 0.0142 - val_loss: 0.0065\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 106us/sample - loss: 0.0122 - val_loss: 0.0066\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 98us/sample - loss: 0.0122 - val_loss: 0.0066\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 112us/sample - loss: 0.0106 - val_loss: 0.0066\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 106us/sample - loss: 0.0118 - val_loss: 0.0065\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 114us/sample - loss: 0.0105 - val_loss: 0.0067\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 94us/sample - loss: 0.0103 - val_loss: 0.0068\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 109us/sample - loss: 0.0098 - val_loss: 0.0068\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 96us/sample - loss: 0.0089 - val_loss: 0.0069\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 99us/sample - loss: 0.0100 - val_loss: 0.0069\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 119us/sample - loss: 0.0097 - val_loss: 0.0068\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0087 - val_loss: 0.0069\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 138us/sample - loss: 0.0083 - val_loss: 0.0069\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 91us/sample - loss: 0.0086 - val_loss: 0.0070\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 93us/sample - loss: 0.0088 - val_loss: 0.0068\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 91us/sample - loss: 0.0088 - val_loss: 0.0070\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0079 - val_loss: 0.0070\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 94us/sample - loss: 0.0087 - val_loss: 0.0070\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 90us/sample - loss: 0.0075 - val_loss: 0.0069\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 116us/sample - loss: 0.0086 - val_loss: 0.0069\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 98us/sample - loss: 0.0084 - val_loss: 0.0070\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 97us/sample - loss: 0.0080 - val_loss: 0.0069\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0078 - val_loss: 0.0070\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0075 - val_loss: 0.0069\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 101us/sample - loss: 0.0077 - val_loss: 0.0069\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 99us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 91us/sample - loss: 0.0075 - val_loss: 0.0072\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 108us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 97us/sample - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 114us/sample - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 118us/sample - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 93us/sample - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 93us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.118 - 0s 1ms/sample - loss: 0.1075 - val_loss: 0.0154\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.075 - 0s 115us/sample - loss: 0.0684 - val_loss: 0.0148\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.050 - 0s 111us/sample - loss: 0.0553 - val_loss: 0.0139\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.045 - 0s 108us/sample - loss: 0.0419 - val_loss: 0.0120\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.046 - 0s 121us/sample - loss: 0.0353 - val_loss: 0.0106\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.037 - 0s 123us/sample - loss: 0.0323 - val_loss: 0.0096\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 115us/sample - loss: 0.0265 - val_loss: 0.0089\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.035 - 0s 113us/sample - loss: 0.0204 - val_loss: 0.0081\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 111us/sample - loss: 0.0191 - val_loss: 0.0076\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 107us/sample - loss: 0.0165 - val_loss: 0.0073\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 102us/sample - loss: 0.0141 - val_loss: 0.0070\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 169us/sample - loss: 0.0131 - val_loss: 0.0068\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 177us/sample - loss: 0.0126 - val_loss: 0.0067\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 177us/sample - loss: 0.0119 - val_loss: 0.0066\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 165us/sample - loss: 0.0101 - val_loss: 0.0066\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 159us/sample - loss: 0.0095 - val_loss: 0.0065\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0089 - val_loss: 0.0065\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0081 - val_loss: 0.0065\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0083 - val_loss: 0.0066\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 112us/sample - loss: 0.0075 - val_loss: 0.0066\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 118us/sample - loss: 0.0076 - val_loss: 0.0068\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 135us/sample - loss: 0.0072 - val_loss: 0.0068\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 140us/sample - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 136us/sample - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 147us/sample - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 169us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 144us/sample - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 135us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 128us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 142us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 133us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 151us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 140us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 108us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0062 - val_loss: 0.0075\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 101us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 100us/sample - loss: 0.0062 - val_loss: 0.0075\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 101us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0061 - val_loss: 0.0073\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 108us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 91us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 99us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 89us/sample - loss: 0.0060 - val_loss: 0.0076\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.259 - 0s 1ms/sample - loss: 0.1849 - val_loss: 0.0715\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.137 - 0s 122us/sample - loss: 0.0956 - val_loss: 0.0397\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.088 - 0s 104us/sample - loss: 0.0602 - val_loss: 0.0232\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.047 - 0s 100us/sample - loss: 0.0384 - val_loss: 0.0134\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.048 - 0s 109us/sample - loss: 0.0313 - val_loss: 0.0095\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 120us/sample - loss: 0.0249 - val_loss: 0.0082\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 131us/sample - loss: 0.0231 - val_loss: 0.0075\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 114us/sample - loss: 0.0214 - val_loss: 0.0071\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 98us/sample - loss: 0.0191 - val_loss: 0.0070\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.038 - 0s 104us/sample - loss: 0.0187 - val_loss: 0.0068\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 95us/sample - loss: 0.0183 - val_loss: 0.0066\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.031 - 0s 115us/sample - loss: 0.0168 - val_loss: 0.0066\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 125us/sample - loss: 0.0146 - val_loss: 0.0065\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.027 - 0s 118us/sample - loss: 0.0140 - val_loss: 0.0064\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 108us/sample - loss: 0.0140 - val_loss: 0.0064\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 101us/sample - loss: 0.0138 - val_loss: 0.0063\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 98us/sample - loss: 0.0121 - val_loss: 0.0063\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 106us/sample - loss: 0.0123 - val_loss: 0.0063\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 107us/sample - loss: 0.0110 - val_loss: 0.0063\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 107us/sample - loss: 0.0101 - val_loss: 0.0063\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 90us/sample - loss: 0.0093 - val_loss: 0.0063\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 91us/sample - loss: 0.0096 - val_loss: 0.0064\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 98us/sample - loss: 0.0096 - val_loss: 0.0064\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 118us/sample - loss: 0.0085 - val_loss: 0.0064\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 99us/sample - loss: 0.0080 - val_loss: 0.0065\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0088 - val_loss: 0.0065\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 101us/sample - loss: 0.0081 - val_loss: 0.0065\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 93us/sample - loss: 0.0080 - val_loss: 0.0066\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 89us/sample - loss: 0.0079 - val_loss: 0.0066\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 93us/sample - loss: 0.0077 - val_loss: 0.0067\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 99us/sample - loss: 0.0079 - val_loss: 0.0067\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 99us/sample - loss: 0.0077 - val_loss: 0.0068\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 96us/sample - loss: 0.0075 - val_loss: 0.0067\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 95us/sample - loss: 0.0072 - val_loss: 0.0067\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 102us/sample - loss: 0.0072 - val_loss: 0.0067\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0075 - val_loss: 0.0068\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 97us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 97us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 100us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 96us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 114us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 106us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 108us/sample - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 106us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 94us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 3s - loss: 0.060 - 0s 1ms/sample - loss: 0.0653 - val_loss: 0.0187\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.071 - 0s 125us/sample - loss: 0.0500 - val_loss: 0.0153\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 145us/sample - loss: 0.0368 - val_loss: 0.0147\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.046 - 0s 138us/sample - loss: 0.0342 - val_loss: 0.0119\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.026 - 0s 130us/sample - loss: 0.0275 - val_loss: 0.0100\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 169us/sample - loss: 0.0230 - val_loss: 0.0101\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 161us/sample - loss: 0.0190 - val_loss: 0.0086\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.025 - 0s 143us/sample - loss: 0.0179 - val_loss: 0.0080\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 125us/sample - loss: 0.0150 - val_loss: 0.0076\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 125us/sample - loss: 0.0155 - val_loss: 0.0073\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 130us/sample - loss: 0.0127 - val_loss: 0.0072\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 142us/sample - loss: 0.0119 - val_loss: 0.0072\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 135us/sample - loss: 0.0112 - val_loss: 0.0070\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 150us/sample - loss: 0.0113 - val_loss: 0.0069\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 147us/sample - loss: 0.0104 - val_loss: 0.0068\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0098 - val_loss: 0.0069\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0102 - val_loss: 0.0069\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 122us/sample - loss: 0.0084 - val_loss: 0.0069\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 113us/sample - loss: 0.0090 - val_loss: 0.0067\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 123us/sample - loss: 0.0090 - val_loss: 0.0066\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0079 - val_loss: 0.0066\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 98us/sample - loss: 0.0080 - val_loss: 0.0067\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 107us/sample - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 105us/sample - loss: 0.0081 - val_loss: 0.0069\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 103us/sample - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 106us/sample - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 97us/sample - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 119us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 177us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 162us/sample - loss: 0.0069 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 114us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 96us/sample - loss: 0.0065 - val_loss: 0.0075\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 94us/sample - loss: 0.0068 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 86us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 93us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 88us/sample - loss: 0.0068 - val_loss: 0.0073\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 89us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 102us/sample - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 98us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0075\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 96us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 91us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 103us/sample - loss: 0.0064 - val_loss: 0.0075\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 99us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 98us/sample - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 93us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.002 - 0s 123us/sample - loss: 0.0064 - val_loss: 0.0075\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 152us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 135us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 150us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 3s - loss: 0.120 - 1s 1ms/sample - loss: 0.1143 - val_loss: 0.0188\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.115 - 0s 123us/sample - loss: 0.0650 - val_loss: 0.0256\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.073 - 0s 120us/sample - loss: 0.0543 - val_loss: 0.0274\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.048 - 0s 115us/sample - loss: 0.0443 - val_loss: 0.0226\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.026 - 0s 117us/sample - loss: 0.0351 - val_loss: 0.0190\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.041 - 0s 120us/sample - loss: 0.0316 - val_loss: 0.0162\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 125us/sample - loss: 0.0280 - val_loss: 0.0134\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 128us/sample - loss: 0.0245 - val_loss: 0.0129\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 111us/sample - loss: 0.0228 - val_loss: 0.0119\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 118us/sample - loss: 0.0209 - val_loss: 0.0103\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.034 - 0s 120us/sample - loss: 0.0188 - val_loss: 0.0100\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 111us/sample - loss: 0.0169 - val_loss: 0.0091\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 105us/sample - loss: 0.0166 - val_loss: 0.0086\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 112us/sample - loss: 0.0149 - val_loss: 0.0082\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 115us/sample - loss: 0.0132 - val_loss: 0.0082\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 112us/sample - loss: 0.0135 - val_loss: 0.0077\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 111us/sample - loss: 0.0117 - val_loss: 0.0075\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 111us/sample - loss: 0.0108 - val_loss: 0.0074\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 160us/sample - loss: 0.0109 - val_loss: 0.0073\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 135us/sample - loss: 0.0104 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 125us/sample - loss: 0.0094 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 120us/sample - loss: 0.0093 - val_loss: 0.0071\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 125us/sample - loss: 0.0088 - val_loss: 0.0071\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0086 - val_loss: 0.0071\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0081 - val_loss: 0.0071\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 135us/sample - loss: 0.0077 - val_loss: 0.0070\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 152us/sample - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 161us/sample - loss: 0.0077 - val_loss: 0.0070\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 99us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 100us/sample - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 84us/sample - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 90us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 87us/sample - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 93us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 85us/sample - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 84us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 94us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 104us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 99us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 104us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 105us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 91us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 102us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 97us/sample - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 90us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 104us/sample - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 96us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 94us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 99us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 54/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 94us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 55/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 97us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 56/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 117us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 57/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 58/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 59/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 60/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 1s - loss: 0.149 - 0s 827us/sample - loss: 0.1175 - val_loss: 0.0622\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.081 - 0s 120us/sample - loss: 0.0771 - val_loss: 0.0500\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.059 - 0s 130us/sample - loss: 0.0638 - val_loss: 0.0423\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.058 - 0s 113us/sample - loss: 0.0552 - val_loss: 0.0358\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.054 - 0s 111us/sample - loss: 0.0471 - val_loss: 0.0303\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.038 - 0s 105us/sample - loss: 0.0402 - val_loss: 0.0255\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.030 - 0s 98us/sample - loss: 0.0345 - val_loss: 0.0215\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 109us/sample - loss: 0.0296 - val_loss: 0.0181\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 105us/sample - loss: 0.0252 - val_loss: 0.0153\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 105us/sample - loss: 0.0218 - val_loss: 0.0129\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 97us/sample - loss: 0.0183 - val_loss: 0.0111\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 100us/sample - loss: 0.0160 - val_loss: 0.0097\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 106us/sample - loss: 0.0140 - val_loss: 0.0086\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 105us/sample - loss: 0.0125 - val_loss: 0.0078\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 115us/sample - loss: 0.0110 - val_loss: 0.0072\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 108us/sample - loss: 0.0103 - val_loss: 0.0068\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 108us/sample - loss: 0.0093 - val_loss: 0.0065\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 104us/sample - loss: 0.0085 - val_loss: 0.0063\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0081 - val_loss: 0.0063\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 102us/sample - loss: 0.0076 - val_loss: 0.0063\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0073 - val_loss: 0.0063\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 119us/sample - loss: 0.0071 - val_loss: 0.0063\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 118us/sample - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 112us/sample - loss: 0.0064 - val_loss: 0.0068\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 111us/sample - loss: 0.0063 - val_loss: 0.0070\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 108us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 108us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 98us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 105us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 110us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 98us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 113us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 102us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 84us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 86us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 82us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 93us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 94us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 108us/sample - loss: 0.0062 - val_loss: 0.0075\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.201 - 0s 1ms/sample - loss: 0.1446 - val_loss: 0.0176\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.091 - 0s 114us/sample - loss: 0.0724 - val_loss: 0.0229\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.074 - 0s 103us/sample - loss: 0.0561 - val_loss: 0.0218\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.048 - 0s 103us/sample - loss: 0.0458 - val_loss: 0.0193\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.039 - 0s 103us/sample - loss: 0.0428 - val_loss: 0.0183\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.052 - 0s 106us/sample - loss: 0.0367 - val_loss: 0.0157\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.051 - 0s 103us/sample - loss: 0.0327 - val_loss: 0.0146\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.025 - 0s 107us/sample - loss: 0.0268 - val_loss: 0.0136\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.027 - 0s 103us/sample - loss: 0.0238 - val_loss: 0.0123\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 113us/sample - loss: 0.0210 - val_loss: 0.0113\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 98us/sample - loss: 0.0197 - val_loss: 0.0103\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.025 - 0s 93us/sample - loss: 0.0181 - val_loss: 0.0094\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 98us/sample - loss: 0.0172 - val_loss: 0.0090\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 101us/sample - loss: 0.0159 - val_loss: 0.0089\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 110us/sample - loss: 0.0151 - val_loss: 0.0083\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 101us/sample - loss: 0.0131 - val_loss: 0.0078\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 108us/sample - loss: 0.0127 - val_loss: 0.0075\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 99us/sample - loss: 0.0115 - val_loss: 0.0073\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 100us/sample - loss: 0.0113 - val_loss: 0.0071\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 106us/sample - loss: 0.0105 - val_loss: 0.0069\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 93us/sample - loss: 0.0113 - val_loss: 0.0068\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 83us/sample - loss: 0.0102 - val_loss: 0.0068\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 113us/sample - loss: 0.0109 - val_loss: 0.0067\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 103us/sample - loss: 0.0093 - val_loss: 0.0067\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 91us/sample - loss: 0.0087 - val_loss: 0.0067\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 101us/sample - loss: 0.0098 - val_loss: 0.0067\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 106us/sample - loss: 0.0086 - val_loss: 0.0066\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0083 - val_loss: 0.0068\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 99us/sample - loss: 0.0081 - val_loss: 0.0069\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 101us/sample - loss: 0.0076 - val_loss: 0.0068\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 94us/sample - loss: 0.0075 - val_loss: 0.0069\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0077 - val_loss: 0.0071\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0075 - val_loss: 0.0071\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 114us/sample - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 108us/sample - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 113us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 107us/sample - loss: 0.0069 - val_loss: 0.0073\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 103us/sample - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 110us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 107us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 116us/sample - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 106us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 125us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 119us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 121us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0067 - val_loss: 0.0074\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 123us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 54/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 112us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 55/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 56/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 129us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 57/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 178us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.319 - 0s 1ms/sample - loss: 0.3010 - val_loss: 0.1259\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.339 - 0s 136us/sample - loss: 0.2085 - val_loss: 0.0775\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.188 - 0s 123us/sample - loss: 0.1609 - val_loss: 0.0768\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.150 - 0s 113us/sample - loss: 0.1397 - val_loss: 0.0664\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.103 - 0s 103us/sample - loss: 0.1061 - val_loss: 0.0531\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.064 - 0s 114us/sample - loss: 0.0770 - val_loss: 0.0347\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.074 - 0s 110us/sample - loss: 0.0696 - val_loss: 0.0212\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 108us/sample - loss: 0.0518 - val_loss: 0.0142\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.038 - 0s 107us/sample - loss: 0.0469 - val_loss: 0.0120\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.059 - 0s 103us/sample - loss: 0.0332 - val_loss: 0.0115\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 112us/sample - loss: 0.0347 - val_loss: 0.0098\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 119us/sample - loss: 0.0329 - val_loss: 0.0089\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 123us/sample - loss: 0.0242 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 113us/sample - loss: 0.0234 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 94us/sample - loss: 0.0223 - val_loss: 0.0076\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 115us/sample - loss: 0.0191 - val_loss: 0.0072\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 115us/sample - loss: 0.0184 - val_loss: 0.0072\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 127us/sample - loss: 0.0159 - val_loss: 0.0070\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 118us/sample - loss: 0.0169 - val_loss: 0.0068\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 93us/sample - loss: 0.0133 - val_loss: 0.0068\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 86us/sample - loss: 0.0146 - val_loss: 0.0067\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 89us/sample - loss: 0.0130 - val_loss: 0.0067\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 108us/sample - loss: 0.0120 - val_loss: 0.0067\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 108us/sample - loss: 0.0104 - val_loss: 0.0066\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 112us/sample - loss: 0.0112 - val_loss: 0.0066\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 106us/sample - loss: 0.0111 - val_loss: 0.0066\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 99us/sample - loss: 0.0098 - val_loss: 0.0067\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 100us/sample - loss: 0.0099 - val_loss: 0.0066\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 106us/sample - loss: 0.0090 - val_loss: 0.0066\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 102us/sample - loss: 0.0088 - val_loss: 0.0067\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 125us/sample - loss: 0.0092 - val_loss: 0.0067\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 121us/sample - loss: 0.0081 - val_loss: 0.0068\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 118us/sample - loss: 0.0082 - val_loss: 0.0067\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 143us/sample - loss: 0.0079 - val_loss: 0.0069\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0084 - val_loss: 0.0068\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 127us/sample - loss: 0.0079 - val_loss: 0.0067\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0073 - val_loss: 0.0069\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 133us/sample - loss: 0.0078 - val_loss: 0.0070\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 142us/sample - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 138us/sample - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 121us/sample - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 118us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 118us/sample - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 118us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 123us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 140us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 142us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 54/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 125us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 55/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 114us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 56/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 1s - loss: 0.455 - 0s 910us/sample - loss: 0.1949 - val_loss: 0.1031\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.087 - 0s 131us/sample - loss: 0.0790 - val_loss: 0.0422\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.087 - 0s 122us/sample - loss: 0.0481 - val_loss: 0.0244\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.027 - 0s 115us/sample - loss: 0.0359 - val_loss: 0.0174\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.055 - 0s 122us/sample - loss: 0.0356 - val_loss: 0.0146\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 112us/sample - loss: 0.0277 - val_loss: 0.0132\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.031 - 0s 113us/sample - loss: 0.0283 - val_loss: 0.0118\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 122us/sample - loss: 0.0239 - val_loss: 0.0108\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.026 - 0s 130us/sample - loss: 0.0219 - val_loss: 0.0102\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 117us/sample - loss: 0.0189 - val_loss: 0.0098\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 97us/sample - loss: 0.0161 - val_loss: 0.0091\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 101us/sample - loss: 0.0178 - val_loss: 0.0086\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 98us/sample - loss: 0.0152 - val_loss: 0.0081\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 103us/sample - loss: 0.0146 - val_loss: 0.0077\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 106us/sample - loss: 0.0133 - val_loss: 0.0075\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 102us/sample - loss: 0.0130 - val_loss: 0.0072\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 105us/sample - loss: 0.0125 - val_loss: 0.0070\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 106us/sample - loss: 0.0124 - val_loss: 0.0068\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 113us/sample - loss: 0.0119 - val_loss: 0.0067\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0101 - val_loss: 0.0066\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 113us/sample - loss: 0.0097 - val_loss: 0.0065\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 119us/sample - loss: 0.0100 - val_loss: 0.0064\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 98us/sample - loss: 0.0091 - val_loss: 0.0064\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 97us/sample - loss: 0.0084 - val_loss: 0.0064\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 94us/sample - loss: 0.0089 - val_loss: 0.0064\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 107us/sample - loss: 0.0083 - val_loss: 0.0064\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 98us/sample - loss: 0.0077 - val_loss: 0.0064\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 96us/sample - loss: 0.0079 - val_loss: 0.0064\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 89us/sample - loss: 0.0076 - val_loss: 0.0064\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 90us/sample - loss: 0.0077 - val_loss: 0.0064\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 99us/sample - loss: 0.0079 - val_loss: 0.0064\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 95us/sample - loss: 0.0074 - val_loss: 0.0065\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 98us/sample - loss: 0.0074 - val_loss: 0.0065\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 93us/sample - loss: 0.0071 - val_loss: 0.0066\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 96us/sample - loss: 0.0072 - val_loss: 0.0065\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 100us/sample - loss: 0.0073 - val_loss: 0.0066\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0070 - val_loss: 0.0066\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 95us/sample - loss: 0.0069 - val_loss: 0.0066\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0069 - val_loss: 0.0067\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 89us/sample - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 98us/sample - loss: 0.0066 - val_loss: 0.0068\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 98us/sample - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 96us/sample - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 110us/sample - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0063 - val_loss: 0.0069\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 121us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0062 - val_loss: 0.0070\n",
      "Epoch 54/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 55/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 108us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.191 - 0s 899us/sample - loss: 0.1591 - val_loss: 0.0487\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.142 - 0s 104us/sample - loss: 0.0847 - val_loss: 0.0541\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.075 - 0s 98us/sample - loss: 0.0678 - val_loss: 0.0434\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.056 - 0s 103us/sample - loss: 0.0507 - val_loss: 0.0317\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 96us/sample - loss: 0.0423 - val_loss: 0.0220\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.042 - 0s 101us/sample - loss: 0.0307 - val_loss: 0.0153\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 103us/sample - loss: 0.0255 - val_loss: 0.0113\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.030 - 0s 113us/sample - loss: 0.0230 - val_loss: 0.0096\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 106us/sample - loss: 0.0215 - val_loss: 0.0086\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 102us/sample - loss: 0.0161 - val_loss: 0.0077\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.025 - 0s 96us/sample - loss: 0.0165 - val_loss: 0.0071\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 101us/sample - loss: 0.0138 - val_loss: 0.0069\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 106us/sample - loss: 0.0169 - val_loss: 0.0068\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 102us/sample - loss: 0.0124 - val_loss: 0.0066\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.026 - 0s 101us/sample - loss: 0.0135 - val_loss: 0.0066\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 106us/sample - loss: 0.0113 - val_loss: 0.0064\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 107us/sample - loss: 0.0099 - val_loss: 0.0064\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 101us/sample - loss: 0.0106 - val_loss: 0.0063\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 106us/sample - loss: 0.0114 - val_loss: 0.0063\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 106us/sample - loss: 0.0105 - val_loss: 0.0063\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 107us/sample - loss: 0.0090 - val_loss: 0.0063\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 111us/sample - loss: 0.0088 - val_loss: 0.0063\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0084 - val_loss: 0.0063\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0080 - val_loss: 0.0064\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 123us/sample - loss: 0.0079 - val_loss: 0.0064\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0079 - val_loss: 0.0064\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0080 - val_loss: 0.0064\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 112us/sample - loss: 0.0084 - val_loss: 0.0064\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0072 - val_loss: 0.0065\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 97us/sample - loss: 0.0082 - val_loss: 0.0065\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 104us/sample - loss: 0.0077 - val_loss: 0.0065\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 98us/sample - loss: 0.0078 - val_loss: 0.0066\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 104us/sample - loss: 0.0073 - val_loss: 0.0067\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 86us/sample - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 96us/sample - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 93us/sample - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 93us/sample - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 92us/sample - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 93us/sample - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 93us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 94us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 98us/sample - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 97us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 103us/sample - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 99us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 102us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 96us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 100us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.048 - 0s 1ms/sample - loss: 0.0427 - val_loss: 0.0157\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.035 - 0s 106us/sample - loss: 0.0375 - val_loss: 0.0120\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.042 - 0s 104us/sample - loss: 0.0315 - val_loss: 0.0107\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.034 - 0s 102us/sample - loss: 0.0245 - val_loss: 0.0104\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.040 - 0s 112us/sample - loss: 0.0212 - val_loss: 0.0094\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.042 - 0s 104us/sample - loss: 0.0194 - val_loss: 0.0086\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 98us/sample - loss: 0.0167 - val_loss: 0.0080\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 123us/sample - loss: 0.0166 - val_loss: 0.0078\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 133us/sample - loss: 0.0141 - val_loss: 0.0073\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 116us/sample - loss: 0.0118 - val_loss: 0.0069\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.010 - 0s 181us/sample - loss: 0.0102 - val_loss: 0.0066\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 142us/sample - loss: 0.0098 - val_loss: 0.0066\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 155us/sample - loss: 0.0094 - val_loss: 0.0064\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 134us/sample - loss: 0.0092 - val_loss: 0.0064\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 142us/sample - loss: 0.0084 - val_loss: 0.0063\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 134us/sample - loss: 0.0086 - val_loss: 0.0063\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 118us/sample - loss: 0.0085 - val_loss: 0.0063\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 141us/sample - loss: 0.0083 - val_loss: 0.0063\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 162us/sample - loss: 0.0082 - val_loss: 0.0063\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 135us/sample - loss: 0.0073 - val_loss: 0.0063\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0075 - val_loss: 0.0063\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 132us/sample - loss: 0.0070 - val_loss: 0.0064\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 135us/sample - loss: 0.0072 - val_loss: 0.0064\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 121us/sample - loss: 0.0069 - val_loss: 0.0065\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 123us/sample - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 115us/sample - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0071 - val_loss: 0.0065\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 118us/sample - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 138us/sample - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 160us/sample - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 182us/sample - loss: 0.0062 - val_loss: 0.0070\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 202us/sample - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 152us/sample - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 132us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 147us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0063 - val_loss: 0.0070\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 125us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 133us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0063 - val_loss: 0.0070\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 140us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 133us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 140us/sample - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 147us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0062 - val_loss: 0.0070\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 98us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.232 - 0s 1ms/sample - loss: 0.2012 - val_loss: 0.0614\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.114 - 0s 133us/sample - loss: 0.1078 - val_loss: 0.0448\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.109 - 0s 125us/sample - loss: 0.0844 - val_loss: 0.0393\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.043 - 0s 120us/sample - loss: 0.0698 - val_loss: 0.0340\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.032 - 0s 118us/sample - loss: 0.0493 - val_loss: 0.0284\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 115us/sample - loss: 0.0388 - val_loss: 0.0247\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.036 - 0s 103us/sample - loss: 0.0372 - val_loss: 0.0212\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.035 - 0s 100us/sample - loss: 0.0308 - val_loss: 0.0188\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.038 - 0s 106us/sample - loss: 0.0265 - val_loss: 0.0160\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 110us/sample - loss: 0.0219 - val_loss: 0.0141\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 120us/sample - loss: 0.0206 - val_loss: 0.0119\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 142us/sample - loss: 0.0194 - val_loss: 0.0103\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 151us/sample - loss: 0.0160 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 155us/sample - loss: 0.0137 - val_loss: 0.0079\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 151us/sample - loss: 0.0116 - val_loss: 0.0072\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 121us/sample - loss: 0.0111 - val_loss: 0.0067\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 138us/sample - loss: 0.0094 - val_loss: 0.0064\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0101 - val_loss: 0.0063\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 113us/sample - loss: 0.0082 - val_loss: 0.0062\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0077 - val_loss: 0.0062\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 117us/sample - loss: 0.0076 - val_loss: 0.0063\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0072 - val_loss: 0.0064\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.002 - 0s 118us/sample - loss: 0.0071 - val_loss: 0.0065\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0068 - val_loss: 0.0066\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 108us/sample - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 106us/sample - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 114us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 98us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 120us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 150us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.006 - 0s 219us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 206us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 145us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 140us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 128us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 142us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 106us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 103us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 108us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 118us/sample - loss: 0.0062 - val_loss: 0.0075\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.498 - 0s 970us/sample - loss: 0.4602 - val_loss: 0.1297\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.463 - 0s 118us/sample - loss: 0.2309 - val_loss: 0.0535\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.131 - 0s 118us/sample - loss: 0.1314 - val_loss: 0.0263\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.070 - 0s 108us/sample - loss: 0.0678 - val_loss: 0.0164\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.045 - 0s 108us/sample - loss: 0.0501 - val_loss: 0.0117\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.041 - 0s 110us/sample - loss: 0.0377 - val_loss: 0.0097\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.055 - 0s 113us/sample - loss: 0.0278 - val_loss: 0.0088\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 125us/sample - loss: 0.0263 - val_loss: 0.0083\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.041 - 0s 108us/sample - loss: 0.0233 - val_loss: 0.0078\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 111us/sample - loss: 0.0202 - val_loss: 0.0074\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 116us/sample - loss: 0.0179 - val_loss: 0.0072\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 106us/sample - loss: 0.0155 - val_loss: 0.0070\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 106us/sample - loss: 0.0148 - val_loss: 0.0070\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.025 - 0s 103us/sample - loss: 0.0147 - val_loss: 0.0069\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0137 - val_loss: 0.0068\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 106us/sample - loss: 0.0127 - val_loss: 0.0068\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 100us/sample - loss: 0.0109 - val_loss: 0.0068\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 113us/sample - loss: 0.0113 - val_loss: 0.0068\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 103us/sample - loss: 0.0109 - val_loss: 0.0069\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 112us/sample - loss: 0.0108 - val_loss: 0.0068\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 120us/sample - loss: 0.0111 - val_loss: 0.0067\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 118us/sample - loss: 0.0109 - val_loss: 0.0067\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0103 - val_loss: 0.0067\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 101us/sample - loss: 0.0105 - val_loss: 0.0067\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 108us/sample - loss: 0.0105 - val_loss: 0.0068\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 94us/sample - loss: 0.0092 - val_loss: 0.0067\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 87us/sample - loss: 0.0094 - val_loss: 0.0067\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 102us/sample - loss: 0.0093 - val_loss: 0.0068\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 92us/sample - loss: 0.0092 - val_loss: 0.0067\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 96us/sample - loss: 0.0087 - val_loss: 0.0067\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 97us/sample - loss: 0.0083 - val_loss: 0.0068\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 101us/sample - loss: 0.0092 - val_loss: 0.0069\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 98us/sample - loss: 0.0084 - val_loss: 0.0068\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 102us/sample - loss: 0.0082 - val_loss: 0.0068\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0080 - val_loss: 0.0067\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0078 - val_loss: 0.0068\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0076 - val_loss: 0.0068\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0080 - val_loss: 0.0068\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 96us/sample - loss: 0.0080 - val_loss: 0.0068\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 99us/sample - loss: 0.0077 - val_loss: 0.0068\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 102us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 96us/sample - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 100us/sample - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 98us/sample - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0073 - val_loss: 0.0070\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 118us/sample - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 100us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 120us/sample - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 133us/sample - loss: 0.0071 - val_loss: 0.0069\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.080 - 0s 828us/sample - loss: 0.0588 - val_loss: 0.0160\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.030 - 0s 102us/sample - loss: 0.0396 - val_loss: 0.0120\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.037 - 0s 112us/sample - loss: 0.0339 - val_loss: 0.0112\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.026 - 0s 127us/sample - loss: 0.0255 - val_loss: 0.0090\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 129us/sample - loss: 0.0233 - val_loss: 0.0081\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 130us/sample - loss: 0.0199 - val_loss: 0.0074\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 140us/sample - loss: 0.0182 - val_loss: 0.0070\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 115us/sample - loss: 0.0150 - val_loss: 0.0066\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 125us/sample - loss: 0.0158 - val_loss: 0.0065\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 125us/sample - loss: 0.0135 - val_loss: 0.0064\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 115us/sample - loss: 0.0124 - val_loss: 0.0063\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 114us/sample - loss: 0.0109 - val_loss: 0.0062\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 115us/sample - loss: 0.0113 - val_loss: 0.0063\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 106us/sample - loss: 0.0110 - val_loss: 0.0064\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 106us/sample - loss: 0.0099 - val_loss: 0.0064\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 109us/sample - loss: 0.0089 - val_loss: 0.0065\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 103us/sample - loss: 0.0088 - val_loss: 0.0066\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 114us/sample - loss: 0.0084 - val_loss: 0.0068\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0086 - val_loss: 0.0066\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0075 - val_loss: 0.0066\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 109us/sample - loss: 0.0087 - val_loss: 0.0068\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 98us/sample - loss: 0.0080 - val_loss: 0.0067\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0075 - val_loss: 0.0066\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 117us/sample - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 135us/sample - loss: 0.0075 - val_loss: 0.0069\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 118us/sample - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 144us/sample - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 145us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 114us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 124us/sample - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 119us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 125us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 99us/sample - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 93us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 93us/sample - loss: 0.0062 - val_loss: 0.0071\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 89us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 99us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 95us/sample - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 92us/sample - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 107us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 3s - loss: 0.151 - 1s 1ms/sample - loss: 0.1733 - val_loss: 0.0264\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.165 - 0s 130us/sample - loss: 0.0971 - val_loss: 0.0164\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.098 - 0s 123us/sample - loss: 0.0797 - val_loss: 0.0140\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.041 - 0s 116us/sample - loss: 0.0573 - val_loss: 0.0122\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.060 - 0s 128us/sample - loss: 0.0575 - val_loss: 0.0106\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.034 - 0s 111us/sample - loss: 0.0410 - val_loss: 0.0098\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.039 - 0s 106us/sample - loss: 0.0361 - val_loss: 0.0093\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.032 - 0s 101us/sample - loss: 0.0285 - val_loss: 0.0094\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 106us/sample - loss: 0.0264 - val_loss: 0.0091\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.026 - 0s 118us/sample - loss: 0.0242 - val_loss: 0.0088\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 111us/sample - loss: 0.0204 - val_loss: 0.0088\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 115us/sample - loss: 0.0178 - val_loss: 0.0082\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 98us/sample - loss: 0.0168 - val_loss: 0.0080\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 103us/sample - loss: 0.0158 - val_loss: 0.0078\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 115us/sample - loss: 0.0142 - val_loss: 0.0077\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 134us/sample - loss: 0.0129 - val_loss: 0.0077\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 135us/sample - loss: 0.0131 - val_loss: 0.0075\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 128us/sample - loss: 0.0114 - val_loss: 0.0074\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 120us/sample - loss: 0.0111 - val_loss: 0.0074\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 111us/sample - loss: 0.0102 - val_loss: 0.0073\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 101us/sample - loss: 0.0098 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 108us/sample - loss: 0.0088 - val_loss: 0.0071\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0083 - val_loss: 0.0070\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0079 - val_loss: 0.0071\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 103us/sample - loss: 0.0075 - val_loss: 0.0071\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 123us/sample - loss: 0.0078 - val_loss: 0.0072\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 145us/sample - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 135us/sample - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0070 - val_loss: 0.0073\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 135us/sample - loss: 0.0065 - val_loss: 0.0075\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 147us/sample - loss: 0.0068 - val_loss: 0.0076\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 147us/sample - loss: 0.0066 - val_loss: 0.0078\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0067 - val_loss: 0.0076\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0069 - val_loss: 0.0075\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 145us/sample - loss: 0.0064 - val_loss: 0.0075\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 167us/sample - loss: 0.0062 - val_loss: 0.0076\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0068 - val_loss: 0.0075\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 118us/sample - loss: 0.0067 - val_loss: 0.0075\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0067 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 135us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 170us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 150us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 158us/sample - loss: 0.0066 - val_loss: 0.0075\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 137us/sample - loss: 0.0062 - val_loss: 0.0075\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 226us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 265us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 194us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 162us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 155us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 125us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 135us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 4s - loss: 0.059 - 0s 1ms/sample - loss: 0.0613 - val_loss: 0.0185\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.044 - 0s 140us/sample - loss: 0.0397 - val_loss: 0.0135\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 145us/sample - loss: 0.0354 - val_loss: 0.0112\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.032 - 0s 142us/sample - loss: 0.0286 - val_loss: 0.0098\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.026 - 0s 111us/sample - loss: 0.0231 - val_loss: 0.0090\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0186 - val_loss: 0.0084\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 158us/sample - loss: 0.0152 - val_loss: 0.0083\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 194us/sample - loss: 0.0134 - val_loss: 0.0083\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 155us/sample - loss: 0.0128 - val_loss: 0.0083\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 133us/sample - loss: 0.0107 - val_loss: 0.0082\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 148us/sample - loss: 0.0106 - val_loss: 0.0080\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 153us/sample - loss: 0.0109 - val_loss: 0.0078\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 145us/sample - loss: 0.0092 - val_loss: 0.0079\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 120us/sample - loss: 0.0089 - val_loss: 0.0078\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.008 - 0s 194us/sample - loss: 0.0091 - val_loss: 0.0076\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 127us/sample - loss: 0.0088 - val_loss: 0.0077\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 146us/sample - loss: 0.0083 - val_loss: 0.0077\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 174us/sample - loss: 0.0085 - val_loss: 0.0074\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 160us/sample - loss: 0.0086 - val_loss: 0.0074\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 149us/sample - loss: 0.0082 - val_loss: 0.0075\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 135us/sample - loss: 0.0080 - val_loss: 0.0076\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 106us/sample - loss: 0.0077 - val_loss: 0.0078\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 106us/sample - loss: 0.0075 - val_loss: 0.0076\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 139us/sample - loss: 0.0073 - val_loss: 0.0079\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0075 - val_loss: 0.0077\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 125us/sample - loss: 0.0069 - val_loss: 0.0077\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 116us/sample - loss: 0.0068 - val_loss: 0.0079\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 108us/sample - loss: 0.0068 - val_loss: 0.0079\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0071 - val_loss: 0.0075\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0070 - val_loss: 0.0077\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 107us/sample - loss: 0.0064 - val_loss: 0.0081\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 125us/sample - loss: 0.0066 - val_loss: 0.0079\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 136us/sample - loss: 0.0063 - val_loss: 0.0080\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 116us/sample - loss: 0.0069 - val_loss: 0.0078\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 118us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 90us/sample - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 94us/sample - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 99us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0065 - val_loss: 0.0082\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 98us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 97us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 129us/sample - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 125us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 127us/sample - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 102us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 86us/sample - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 121us/sample - loss: 0.0062 - val_loss: 0.0082\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.209 - 0s 937us/sample - loss: 0.1270 - val_loss: 0.0554\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.078 - 0s 123us/sample - loss: 0.0833 - val_loss: 0.0442\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.071 - 0s 121us/sample - loss: 0.0604 - val_loss: 0.0330\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.048 - 0s 115us/sample - loss: 0.0428 - val_loss: 0.0225\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.035 - 0s 96us/sample - loss: 0.0331 - val_loss: 0.0147\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.044 - 0s 91us/sample - loss: 0.0243 - val_loss: 0.0097\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 90us/sample - loss: 0.0198 - val_loss: 0.0078\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 105us/sample - loss: 0.0176 - val_loss: 0.0072\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.039 - 0s 115us/sample - loss: 0.0170 - val_loss: 0.0069\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 108us/sample - loss: 0.0168 - val_loss: 0.0069\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 120us/sample - loss: 0.0160 - val_loss: 0.0068\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 111us/sample - loss: 0.0140 - val_loss: 0.0067\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 108us/sample - loss: 0.0132 - val_loss: 0.0065\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 152us/sample - loss: 0.0127 - val_loss: 0.0064\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 118us/sample - loss: 0.0127 - val_loss: 0.0064\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 130us/sample - loss: 0.0108 - val_loss: 0.0063\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 98us/sample - loss: 0.0115 - val_loss: 0.0063\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 108us/sample - loss: 0.0108 - val_loss: 0.0063\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 106us/sample - loss: 0.0105 - val_loss: 0.0063\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 101us/sample - loss: 0.0086 - val_loss: 0.0063\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0094 - val_loss: 0.0063\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 109us/sample - loss: 0.0089 - val_loss: 0.0063\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 103us/sample - loss: 0.0102 - val_loss: 0.0063\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 125us/sample - loss: 0.0086 - val_loss: 0.0063\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0083 - val_loss: 0.0063\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 116us/sample - loss: 0.0081 - val_loss: 0.0064\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 114us/sample - loss: 0.0076 - val_loss: 0.0064\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 113us/sample - loss: 0.0081 - val_loss: 0.0065\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0075 - val_loss: 0.0065\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 101us/sample - loss: 0.0077 - val_loss: 0.0065\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0073 - val_loss: 0.0065\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0070 - val_loss: 0.0065\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 119us/sample - loss: 0.0070 - val_loss: 0.0066\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 113us/sample - loss: 0.0071 - val_loss: 0.0066\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 119us/sample - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 126us/sample - loss: 0.0069 - val_loss: 0.0067\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 104us/sample - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 104us/sample - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 106us/sample - loss: 0.0066 - val_loss: 0.0068\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 96us/sample - loss: 0.0063 - val_loss: 0.0069\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 93us/sample - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 120us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 140us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 108us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 112us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 138us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 112us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 98us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 100us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 91us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 108us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 54/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 99us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.116 - 0s 1ms/sample - loss: 0.1310 - val_loss: 0.0598\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.098 - 0s 120us/sample - loss: 0.0792 - val_loss: 0.0241\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.075 - 0s 102us/sample - loss: 0.0490 - val_loss: 0.0121\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.055 - 0s 106us/sample - loss: 0.0364 - val_loss: 0.0105\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.032 - 0s 105us/sample - loss: 0.0348 - val_loss: 0.0097\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.041 - 0s 111us/sample - loss: 0.0298 - val_loss: 0.0094\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.031 - 0s 103us/sample - loss: 0.0252 - val_loss: 0.0091\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 103us/sample - loss: 0.0234 - val_loss: 0.0090\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 115us/sample - loss: 0.0196 - val_loss: 0.0089\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 127us/sample - loss: 0.0185 - val_loss: 0.0087\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 123us/sample - loss: 0.0174 - val_loss: 0.0082\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 112us/sample - loss: 0.0153 - val_loss: 0.0081\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 110us/sample - loss: 0.0142 - val_loss: 0.0083\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 88us/sample - loss: 0.0125 - val_loss: 0.0081\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 90us/sample - loss: 0.0135 - val_loss: 0.0077\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 96us/sample - loss: 0.0111 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 114us/sample - loss: 0.0108 - val_loss: 0.0075\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0100 - val_loss: 0.0076\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 111us/sample - loss: 0.0097 - val_loss: 0.0076\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0097 - val_loss: 0.0075\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 113us/sample - loss: 0.0096 - val_loss: 0.0074\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 111us/sample - loss: 0.0092 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 109us/sample - loss: 0.0086 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 116us/sample - loss: 0.0088 - val_loss: 0.0072\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 108us/sample - loss: 0.0081 - val_loss: 0.0072\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 113us/sample - loss: 0.0080 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 108us/sample - loss: 0.0080 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 106us/sample - loss: 0.0081 - val_loss: 0.0072\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 116us/sample - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 111us/sample - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0077 - val_loss: 0.0070\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 124us/sample - loss: 0.0073 - val_loss: 0.0070\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 114us/sample - loss: 0.0073 - val_loss: 0.0070\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 118us/sample - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 121us/sample - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 104us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 118us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 113us/sample - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 98us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 87us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 112us/sample - loss: 0.0062 - val_loss: 0.0071\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 108us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 103us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 102us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 91us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 86us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 94us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 93us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 54/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 106us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 55/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 98us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 56/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 57/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 97us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 58/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 105us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 59/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 100us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 60/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 96us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 61/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 100us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 62/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 96us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 63/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 100us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 64/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.002 - 0s 111us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 65/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 121us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 66/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 67/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 68/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 69/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 119us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 70/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 121us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 71/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 134us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 72/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 135us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.195 - 0s 960us/sample - loss: 0.1408 - val_loss: 0.0525\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.110 - 0s 130us/sample - loss: 0.0647 - val_loss: 0.0223\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.055 - 0s 125us/sample - loss: 0.0423 - val_loss: 0.0141\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.043 - 0s 125us/sample - loss: 0.0331 - val_loss: 0.0124\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 123us/sample - loss: 0.0283 - val_loss: 0.0116\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 130us/sample - loss: 0.0250 - val_loss: 0.0100\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 130us/sample - loss: 0.0234 - val_loss: 0.0092\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 128us/sample - loss: 0.0226 - val_loss: 0.0086\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 124us/sample - loss: 0.0204 - val_loss: 0.0079\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 128us/sample - loss: 0.0179 - val_loss: 0.0075\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0175 - val_loss: 0.0072\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 130us/sample - loss: 0.0160 - val_loss: 0.0070\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 145us/sample - loss: 0.0142 - val_loss: 0.0069\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 135us/sample - loss: 0.0138 - val_loss: 0.0069\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 133us/sample - loss: 0.0126 - val_loss: 0.0068\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 130us/sample - loss: 0.0123 - val_loss: 0.0066\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 114us/sample - loss: 0.0108 - val_loss: 0.0065\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 95us/sample - loss: 0.0107 - val_loss: 0.0065\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 108us/sample - loss: 0.0109 - val_loss: 0.0064\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 121us/sample - loss: 0.0096 - val_loss: 0.0064\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0094 - val_loss: 0.0064\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0085 - val_loss: 0.0064\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 103us/sample - loss: 0.0088 - val_loss: 0.0064\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 106us/sample - loss: 0.0083 - val_loss: 0.0065\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0077 - val_loss: 0.0065\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.002 - 0s 96us/sample - loss: 0.0081 - val_loss: 0.0065\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 88us/sample - loss: 0.0074 - val_loss: 0.0067\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 91us/sample - loss: 0.0080 - val_loss: 0.0066\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 93us/sample - loss: 0.0077 - val_loss: 0.0066\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 98us/sample - loss: 0.0073 - val_loss: 0.0067\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 106us/sample - loss: 0.0076 - val_loss: 0.0068\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 102us/sample - loss: 0.0073 - val_loss: 0.0067\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 103us/sample - loss: 0.0072 - val_loss: 0.0067\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 106us/sample - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 101us/sample - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 92us/sample - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 96us/sample - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 101us/sample - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 99us/sample - loss: 0.0063 - val_loss: 0.0069\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 106us/sample - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 103us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 121us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0062 - val_loss: 0.0071\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 113us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.061 - 0s 911us/sample - loss: 0.0576 - val_loss: 0.0224\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.042 - 0s 115us/sample - loss: 0.0377 - val_loss: 0.0090\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.047 - 0s 138us/sample - loss: 0.0334 - val_loss: 0.0087\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.030 - 0s 118us/sample - loss: 0.0284 - val_loss: 0.0084\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 118us/sample - loss: 0.0241 - val_loss: 0.0084\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 120us/sample - loss: 0.0207 - val_loss: 0.0080\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 122us/sample - loss: 0.0183 - val_loss: 0.0075\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 125us/sample - loss: 0.0165 - val_loss: 0.0074\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 108us/sample - loss: 0.0143 - val_loss: 0.0072\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 104us/sample - loss: 0.0146 - val_loss: 0.0071\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 103us/sample - loss: 0.0137 - val_loss: 0.0068\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0134 - val_loss: 0.0066\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 115us/sample - loss: 0.0120 - val_loss: 0.0066\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 106us/sample - loss: 0.0108 - val_loss: 0.0065\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 103us/sample - loss: 0.0106 - val_loss: 0.0064\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 107us/sample - loss: 0.0110 - val_loss: 0.0064\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 110us/sample - loss: 0.0099 - val_loss: 0.0063\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0096 - val_loss: 0.0063\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 101us/sample - loss: 0.0095 - val_loss: 0.0063\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 96us/sample - loss: 0.0090 - val_loss: 0.0064\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0078 - val_loss: 0.0064\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0083 - val_loss: 0.0064\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 101us/sample - loss: 0.0080 - val_loss: 0.0064\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 105us/sample - loss: 0.0075 - val_loss: 0.0064\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 111us/sample - loss: 0.0079 - val_loss: 0.0064\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 100us/sample - loss: 0.0075 - val_loss: 0.0066\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 101us/sample - loss: 0.0075 - val_loss: 0.0066\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0069 - val_loss: 0.0066\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0072 - val_loss: 0.0067\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 91us/sample - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 84us/sample - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 94us/sample - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 93us/sample - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 98us/sample - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 90us/sample - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 98us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 88us/sample - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 99us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 106us/sample - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 98us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 105us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 96us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 108us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 1s - loss: 0.083 - 0s 809us/sample - loss: 0.0712 - val_loss: 0.0259\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.064 - 0s 98us/sample - loss: 0.0445 - val_loss: 0.0157\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 93us/sample - loss: 0.0356 - val_loss: 0.0116\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 102us/sample - loss: 0.0306 - val_loss: 0.0095\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 96us/sample - loss: 0.0268 - val_loss: 0.0083\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 103us/sample - loss: 0.0228 - val_loss: 0.0077\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 120us/sample - loss: 0.0189 - val_loss: 0.0072\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 125us/sample - loss: 0.0204 - val_loss: 0.0070\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 136us/sample - loss: 0.0163 - val_loss: 0.0066\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 115us/sample - loss: 0.0155 - val_loss: 0.0064\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 112us/sample - loss: 0.0129 - val_loss: 0.0063\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 106us/sample - loss: 0.0123 - val_loss: 0.0064\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 105us/sample - loss: 0.0112 - val_loss: 0.0063\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 103us/sample - loss: 0.0102 - val_loss: 0.0063\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 105us/sample - loss: 0.0089 - val_loss: 0.0063\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0098 - val_loss: 0.0063\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 106us/sample - loss: 0.0098 - val_loss: 0.0064\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0097 - val_loss: 0.0064\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0093 - val_loss: 0.0065\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0087 - val_loss: 0.0066\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0089 - val_loss: 0.0066\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 97us/sample - loss: 0.0084 - val_loss: 0.0067\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0076 - val_loss: 0.0067\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 102us/sample - loss: 0.0077 - val_loss: 0.0067\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 96us/sample - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 102us/sample - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 100us/sample - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 102us/sample - loss: 0.0073 - val_loss: 0.0069\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.002 - 0s 106us/sample - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 102us/sample - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 107us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 91us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 97us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 107us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 103us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 91us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.168 - 0s 1ms/sample - loss: 0.1948 - val_loss: 0.0406\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.135 - 0s 91us/sample - loss: 0.1174 - val_loss: 0.0233\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.077 - 0s 103us/sample - loss: 0.0912 - val_loss: 0.0210\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.062 - 0s 97us/sample - loss: 0.0687 - val_loss: 0.0197\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.036 - 0s 108us/sample - loss: 0.0577 - val_loss: 0.0188\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 117us/sample - loss: 0.0476 - val_loss: 0.0166\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 113us/sample - loss: 0.0417 - val_loss: 0.0143\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.030 - 0s 109us/sample - loss: 0.0348 - val_loss: 0.0132\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.034 - 0s 106us/sample - loss: 0.0319 - val_loss: 0.0130\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 123us/sample - loss: 0.0278 - val_loss: 0.0119\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 111us/sample - loss: 0.0239 - val_loss: 0.0108\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 93us/sample - loss: 0.0205 - val_loss: 0.0102\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.026 - 0s 85us/sample - loss: 0.0201 - val_loss: 0.0097\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 92us/sample - loss: 0.0184 - val_loss: 0.0092\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 93us/sample - loss: 0.0146 - val_loss: 0.0086\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 100us/sample - loss: 0.0143 - val_loss: 0.0081\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 90us/sample - loss: 0.0125 - val_loss: 0.0077\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 95us/sample - loss: 0.0113 - val_loss: 0.0074\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 101us/sample - loss: 0.0108 - val_loss: 0.0071\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 97us/sample - loss: 0.0096 - val_loss: 0.0069\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0087 - val_loss: 0.0068\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 101us/sample - loss: 0.0083 - val_loss: 0.0067\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 116us/sample - loss: 0.0077 - val_loss: 0.0067\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 106us/sample - loss: 0.0074 - val_loss: 0.0067\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 102us/sample - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 96us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 104us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 98us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 103us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 118us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0062 - val_loss: 0.0076\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 115us/sample - loss: 0.0063 - val_loss: 0.0076\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 110us/sample - loss: 0.0061 - val_loss: 0.0076\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0062 - val_loss: 0.0075\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 133us/sample - loss: 0.0061 - val_loss: 0.0075\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0061 - val_loss: 0.0075\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 110us/sample - loss: 0.0062 - val_loss: 0.0075\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0062 - val_loss: 0.0075\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 88us/sample - loss: 0.0062 - val_loss: 0.0075\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 102us/sample - loss: 0.0060 - val_loss: 0.0075\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0062 - val_loss: 0.0075\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.129 - 0s 791us/sample - loss: 0.1053 - val_loss: 0.0551\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.061 - 0s 98us/sample - loss: 0.0710 - val_loss: 0.0397\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.057 - 0s 122us/sample - loss: 0.0523 - val_loss: 0.0272\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.063 - 0s 115us/sample - loss: 0.0380 - val_loss: 0.0169\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 114us/sample - loss: 0.0274 - val_loss: 0.0102\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 104us/sample - loss: 0.0240 - val_loss: 0.0080\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 91us/sample - loss: 0.0187 - val_loss: 0.0077\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 98us/sample - loss: 0.0207 - val_loss: 0.0075\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.035 - 0s 101us/sample - loss: 0.0181 - val_loss: 0.0073\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 105us/sample - loss: 0.0156 - val_loss: 0.0072\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.030 - 0s 101us/sample - loss: 0.0159 - val_loss: 0.0071\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 98us/sample - loss: 0.0133 - val_loss: 0.0070\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 103us/sample - loss: 0.0138 - val_loss: 0.0071\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 101us/sample - loss: 0.0128 - val_loss: 0.0070\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0113 - val_loss: 0.0069\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 114us/sample - loss: 0.0111 - val_loss: 0.0068\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 137us/sample - loss: 0.0098 - val_loss: 0.0068\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 130us/sample - loss: 0.0100 - val_loss: 0.0068\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 128us/sample - loss: 0.0089 - val_loss: 0.0070\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0088 - val_loss: 0.0071\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 112us/sample - loss: 0.0093 - val_loss: 0.0070\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0085 - val_loss: 0.0070\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 100us/sample - loss: 0.0088 - val_loss: 0.0070\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 106us/sample - loss: 0.0093 - val_loss: 0.0069\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0081 - val_loss: 0.0070\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 115us/sample - loss: 0.0080 - val_loss: 0.0070\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 99us/sample - loss: 0.0076 - val_loss: 0.0072\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0077 - val_loss: 0.0070\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 111us/sample - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 121us/sample - loss: 0.0073 - val_loss: 0.0070\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 118us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 121us/sample - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 102us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 98us/sample - loss: 0.0065 - val_loss: 0.0075\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 110us/sample - loss: 0.0067 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 97us/sample - loss: 0.0070 - val_loss: 0.0073\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 98us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 114us/sample - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 96us/sample - loss: 0.0065 - val_loss: 0.0075\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 87us/sample - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 89us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 98us/sample - loss: 0.0065 - val_loss: 0.0075\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0065 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.074 - 0s 979us/sample - loss: 0.0808 - val_loss: 0.0613\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.067 - 0s 101us/sample - loss: 0.0507 - val_loss: 0.0393\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.027 - 0s 103us/sample - loss: 0.0430 - val_loss: 0.0306\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.045 - 0s 104us/sample - loss: 0.0353 - val_loss: 0.0223\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.031 - 0s 95us/sample - loss: 0.0274 - val_loss: 0.0185\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.025 - 0s 97us/sample - loss: 0.0249 - val_loss: 0.0158\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 101us/sample - loss: 0.0212 - val_loss: 0.0125\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 103us/sample - loss: 0.0190 - val_loss: 0.0113\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 111us/sample - loss: 0.0172 - val_loss: 0.0107\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 106us/sample - loss: 0.0171 - val_loss: 0.0100\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 118us/sample - loss: 0.0144 - val_loss: 0.0090\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 125us/sample - loss: 0.0130 - val_loss: 0.0085\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 130us/sample - loss: 0.0130 - val_loss: 0.0081\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 118us/sample - loss: 0.0111 - val_loss: 0.0079\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 93us/sample - loss: 0.0108 - val_loss: 0.0076\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 96us/sample - loss: 0.0110 - val_loss: 0.0074\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 97us/sample - loss: 0.0095 - val_loss: 0.0073\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 102us/sample - loss: 0.0090 - val_loss: 0.0073\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0088 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 97us/sample - loss: 0.0082 - val_loss: 0.0071\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 99us/sample - loss: 0.0080 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 104us/sample - loss: 0.0075 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 132us/sample - loss: 0.0079 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 99us/sample - loss: 0.0074 - val_loss: 0.0072\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 120us/sample - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0074 - val_loss: 0.0071\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 96us/sample - loss: 0.0075 - val_loss: 0.0071\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 101us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 101us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 108us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 104us/sample - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 98us/sample - loss: 0.0068 - val_loss: 0.0073\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 101us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 111us/sample - loss: 0.0068 - val_loss: 0.0073\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 118us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 118us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 127us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 94us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 91us/sample - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 93us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 86us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 97us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 97us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 97us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 54/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 104us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 55/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 98us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.200 - 0s 897us/sample - loss: 0.1335 - val_loss: 0.0602\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.165 - 0s 97us/sample - loss: 0.0826 - val_loss: 0.0450\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.068 - 0s 98us/sample - loss: 0.0656 - val_loss: 0.0294\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.042 - 0s 100us/sample - loss: 0.0390 - val_loss: 0.0152\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.033 - 0s 100us/sample - loss: 0.0298 - val_loss: 0.0094\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 101us/sample - loss: 0.0259 - val_loss: 0.0091\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 98us/sample - loss: 0.0235 - val_loss: 0.0083\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 106us/sample - loss: 0.0225 - val_loss: 0.0078\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 101us/sample - loss: 0.0207 - val_loss: 0.0077\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 101us/sample - loss: 0.0202 - val_loss: 0.0076\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 93us/sample - loss: 0.0165 - val_loss: 0.0071\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 93us/sample - loss: 0.0178 - val_loss: 0.0068\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 100us/sample - loss: 0.0159 - val_loss: 0.0068\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 106us/sample - loss: 0.0151 - val_loss: 0.0066\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 112us/sample - loss: 0.0135 - val_loss: 0.0065\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 103us/sample - loss: 0.0121 - val_loss: 0.0064\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 108us/sample - loss: 0.0112 - val_loss: 0.0064\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 103us/sample - loss: 0.0116 - val_loss: 0.0063\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 106us/sample - loss: 0.0102 - val_loss: 0.0063\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 125us/sample - loss: 0.0107 - val_loss: 0.0063\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 103us/sample - loss: 0.0101 - val_loss: 0.0064\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 108us/sample - loss: 0.0091 - val_loss: 0.0064\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0087 - val_loss: 0.0065\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 108us/sample - loss: 0.0085 - val_loss: 0.0065\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 114us/sample - loss: 0.0081 - val_loss: 0.0065\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 111us/sample - loss: 0.0081 - val_loss: 0.0066\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 107us/sample - loss: 0.0072 - val_loss: 0.0067\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 100us/sample - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0076 - val_loss: 0.0068\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 103us/sample - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 109us/sample - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 99us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 106us/sample - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 102us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 106us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 89us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 89us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 100us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.253 - 0s 819us/sample - loss: 0.2151 - val_loss: 0.1003\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.162 - 0s 101us/sample - loss: 0.1323 - val_loss: 0.0402\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.068 - 0s 102us/sample - loss: 0.0806 - val_loss: 0.0206\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.085 - 0s 108us/sample - loss: 0.0535 - val_loss: 0.0153\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.043 - 0s 99us/sample - loss: 0.0366 - val_loss: 0.0124\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.050 - 0s 100us/sample - loss: 0.0304 - val_loss: 0.0096\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 98us/sample - loss: 0.0245 - val_loss: 0.0085\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.027 - 0s 93us/sample - loss: 0.0229 - val_loss: 0.0081\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 100us/sample - loss: 0.0192 - val_loss: 0.0076\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 101us/sample - loss: 0.0191 - val_loss: 0.0071\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 103us/sample - loss: 0.0197 - val_loss: 0.0070\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 106us/sample - loss: 0.0154 - val_loss: 0.0067\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 103us/sample - loss: 0.0139 - val_loss: 0.0065\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 111us/sample - loss: 0.0142 - val_loss: 0.0064\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 100us/sample - loss: 0.0143 - val_loss: 0.0062\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 101us/sample - loss: 0.0134 - val_loss: 0.0062\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 106us/sample - loss: 0.0137 - val_loss: 0.0061\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 118us/sample - loss: 0.0123 - val_loss: 0.0061\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 111us/sample - loss: 0.0113 - val_loss: 0.0062\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 118us/sample - loss: 0.0114 - val_loss: 0.0062\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0100 - val_loss: 0.0063\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 120us/sample - loss: 0.0100 - val_loss: 0.0063\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 142us/sample - loss: 0.0103 - val_loss: 0.0062\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 155us/sample - loss: 0.0101 - val_loss: 0.0061\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 128us/sample - loss: 0.0096 - val_loss: 0.0062\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 122us/sample - loss: 0.0101 - val_loss: 0.0062\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0096 - val_loss: 0.0064\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 135us/sample - loss: 0.0089 - val_loss: 0.0065\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 132us/sample - loss: 0.0090 - val_loss: 0.0065\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 118us/sample - loss: 0.0085 - val_loss: 0.0065\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0088 - val_loss: 0.0065\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 118us/sample - loss: 0.0078 - val_loss: 0.0065\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 125us/sample - loss: 0.0078 - val_loss: 0.0067\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 128us/sample - loss: 0.0084 - val_loss: 0.0065\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 125us/sample - loss: 0.0084 - val_loss: 0.0066\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 120us/sample - loss: 0.0081 - val_loss: 0.0067\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 115us/sample - loss: 0.0080 - val_loss: 0.0068\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0075 - val_loss: 0.0068\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 125us/sample - loss: 0.0073 - val_loss: 0.0067\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0075 - val_loss: 0.0068\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 112us/sample - loss: 0.0077 - val_loss: 0.0068\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 116us/sample - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 101us/sample - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 135us/sample - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 104us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 115us/sample - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 118us/sample - loss: 0.0068 - val_loss: 0.0070\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.118 - 0s 956us/sample - loss: 0.0842 - val_loss: 0.0157\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.074 - 0s 99us/sample - loss: 0.0567 - val_loss: 0.0199\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.064 - 0s 103us/sample - loss: 0.0456 - val_loss: 0.0198\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 101us/sample - loss: 0.0403 - val_loss: 0.0172\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.037 - 0s 101us/sample - loss: 0.0337 - val_loss: 0.0155\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.041 - 0s 103us/sample - loss: 0.0346 - val_loss: 0.0136\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 99us/sample - loss: 0.0249 - val_loss: 0.0112\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.030 - 0s 107us/sample - loss: 0.0233 - val_loss: 0.0098\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.027 - 0s 114us/sample - loss: 0.0202 - val_loss: 0.0089\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.025 - 0s 106us/sample - loss: 0.0204 - val_loss: 0.0084\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 105us/sample - loss: 0.0175 - val_loss: 0.0082\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 111us/sample - loss: 0.0147 - val_loss: 0.0077\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 97us/sample - loss: 0.0135 - val_loss: 0.0075\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 103us/sample - loss: 0.0123 - val_loss: 0.0073\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 96us/sample - loss: 0.0106 - val_loss: 0.0071\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 120us/sample - loss: 0.0100 - val_loss: 0.0069\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 106us/sample - loss: 0.0103 - val_loss: 0.0068\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0090 - val_loss: 0.0067\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 101us/sample - loss: 0.0089 - val_loss: 0.0066\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0088 - val_loss: 0.0067\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 106us/sample - loss: 0.0084 - val_loss: 0.0067\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0075 - val_loss: 0.0068\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 106us/sample - loss: 0.0075 - val_loss: 0.0067\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0075 - val_loss: 0.0068\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 96us/sample - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 102us/sample - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 104us/sample - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 106us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 96us/sample - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 94us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 98us/sample - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 103us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 102us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 104us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 95us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 116us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 98us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 93us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.002 - 0s 96us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 95us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 89us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 98us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.002 - 0s 97us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 102us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.163 - 0s 823us/sample - loss: 0.1114 - val_loss: 0.0172\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.050 - 0s 108us/sample - loss: 0.0574 - val_loss: 0.0206\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.034 - 0s 113us/sample - loss: 0.0432 - val_loss: 0.0189\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.046 - 0s 108us/sample - loss: 0.0435 - val_loss: 0.0170\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 103us/sample - loss: 0.0373 - val_loss: 0.0149\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.031 - 0s 112us/sample - loss: 0.0288 - val_loss: 0.0125\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.033 - 0s 98us/sample - loss: 0.0266 - val_loss: 0.0118\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.032 - 0s 102us/sample - loss: 0.0232 - val_loss: 0.0106\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 98us/sample - loss: 0.0196 - val_loss: 0.0095\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 96us/sample - loss: 0.0187 - val_loss: 0.0087\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 105us/sample - loss: 0.0152 - val_loss: 0.0078\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 98us/sample - loss: 0.0150 - val_loss: 0.0074\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 96us/sample - loss: 0.0127 - val_loss: 0.0071\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 101us/sample - loss: 0.0114 - val_loss: 0.0068\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 91us/sample - loss: 0.0123 - val_loss: 0.0068\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 96us/sample - loss: 0.0114 - val_loss: 0.0068\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 117us/sample - loss: 0.0105 - val_loss: 0.0070\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 113us/sample - loss: 0.0097 - val_loss: 0.0069\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 123us/sample - loss: 0.0081 - val_loss: 0.0068\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0087 - val_loss: 0.0068\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 120us/sample - loss: 0.0084 - val_loss: 0.0069\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 104us/sample - loss: 0.0083 - val_loss: 0.0070\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 100us/sample - loss: 0.0081 - val_loss: 0.0071\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 101us/sample - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 103us/sample - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0073 - val_loss: 0.0069\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 114us/sample - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 104us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 102us/sample - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 108us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 95us/sample - loss: 0.0069 - val_loss: 0.0073\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 115us/sample - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 123us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 92us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 88us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 96us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 92us/sample - loss: 0.0067 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 110us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 93us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 96us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 93us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.121 - 0s 938us/sample - loss: 0.1012 - val_loss: 0.0689\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.075 - 0s 98us/sample - loss: 0.0791 - val_loss: 0.0544\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.066 - 0s 100us/sample - loss: 0.0603 - val_loss: 0.0439\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.047 - 0s 103us/sample - loss: 0.0508 - val_loss: 0.0328\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.057 - 0s 102us/sample - loss: 0.0348 - val_loss: 0.0194\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.035 - 0s 117us/sample - loss: 0.0265 - val_loss: 0.0143\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 119us/sample - loss: 0.0253 - val_loss: 0.0128\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 101us/sample - loss: 0.0201 - val_loss: 0.0117\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 98us/sample - loss: 0.0179 - val_loss: 0.0103\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 97us/sample - loss: 0.0156 - val_loss: 0.0091\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 96us/sample - loss: 0.0160 - val_loss: 0.0086\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 131us/sample - loss: 0.0126 - val_loss: 0.0081\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 126us/sample - loss: 0.0121 - val_loss: 0.0076\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 125us/sample - loss: 0.0113 - val_loss: 0.0074\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 111us/sample - loss: 0.0110 - val_loss: 0.0072\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 93us/sample - loss: 0.0112 - val_loss: 0.0071\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0107 - val_loss: 0.0069\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 117us/sample - loss: 0.0109 - val_loss: 0.0069\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 108us/sample - loss: 0.0086 - val_loss: 0.0068\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0093 - val_loss: 0.0068\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 125us/sample - loss: 0.0097 - val_loss: 0.0067\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0090 - val_loss: 0.0067\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 146us/sample - loss: 0.0085 - val_loss: 0.0067\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0077 - val_loss: 0.0067\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0077 - val_loss: 0.0068\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0082 - val_loss: 0.0067\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 115us/sample - loss: 0.0073 - val_loss: 0.0067\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 105us/sample - loss: 0.0075 - val_loss: 0.0068\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0077 - val_loss: 0.0068\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 102us/sample - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 106us/sample - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 104us/sample - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 98us/sample - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 104us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 93us/sample - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 100us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 106us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 104us/sample - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0068\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 103us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 96us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 98us/sample - loss: 0.0062 - val_loss: 0.0071\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 94us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 102us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 98us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 98us/sample - loss: 0.0061 - val_loss: 0.0071\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 91us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 95us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 95us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 96us/sample - loss: 0.0062 - val_loss: 0.0072\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.129 - 0s 955us/sample - loss: 0.0726 - val_loss: 0.0365\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.046 - 0s 118us/sample - loss: 0.0411 - val_loss: 0.0178\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.030 - 0s 108us/sample - loss: 0.0270 - val_loss: 0.0107\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 109us/sample - loss: 0.0246 - val_loss: 0.0094\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 112us/sample - loss: 0.0216 - val_loss: 0.0088\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.027 - 0s 107us/sample - loss: 0.0211 - val_loss: 0.0087\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 114us/sample - loss: 0.0187 - val_loss: 0.0084\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 102us/sample - loss: 0.0159 - val_loss: 0.0078\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 114us/sample - loss: 0.0154 - val_loss: 0.0075\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 103us/sample - loss: 0.0155 - val_loss: 0.0073\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 120us/sample - loss: 0.0138 - val_loss: 0.0071\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 126us/sample - loss: 0.0123 - val_loss: 0.0069\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 120us/sample - loss: 0.0132 - val_loss: 0.0067\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 149us/sample - loss: 0.0119 - val_loss: 0.0066\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 129us/sample - loss: 0.0102 - val_loss: 0.0066\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0099 - val_loss: 0.0066\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 101us/sample - loss: 0.0105 - val_loss: 0.0065\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 133us/sample - loss: 0.0097 - val_loss: 0.0065\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0094 - val_loss: 0.0065\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0092 - val_loss: 0.0065\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0083 - val_loss: 0.0065\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 106us/sample - loss: 0.0084 - val_loss: 0.0066\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 99us/sample - loss: 0.0077 - val_loss: 0.0067\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0078 - val_loss: 0.0067\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 146us/sample - loss: 0.0081 - val_loss: 0.0065\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 133us/sample - loss: 0.0075 - val_loss: 0.0066\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 106us/sample - loss: 0.0074 - val_loss: 0.0066\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 102us/sample - loss: 0.0073 - val_loss: 0.0066\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0075 - val_loss: 0.0067\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 102us/sample - loss: 0.0076 - val_loss: 0.0067\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0073 - val_loss: 0.0067\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 138us/sample - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 152us/sample - loss: 0.0063 - val_loss: 0.0068\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 91us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 101us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 100us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 102us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 106us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 118us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 133us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 131us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 135us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 85us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 91us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 3s - loss: 0.082 - 0s 1ms/sample - loss: 0.0995 - val_loss: 0.0304\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.116 - 0s 112us/sample - loss: 0.0790 - val_loss: 0.0278\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.065 - 0s 106us/sample - loss: 0.0601 - val_loss: 0.0264\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.051 - 0s 104us/sample - loss: 0.0503 - val_loss: 0.0228\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.036 - 0s 130us/sample - loss: 0.0418 - val_loss: 0.0197\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.035 - 0s 151us/sample - loss: 0.0381 - val_loss: 0.0168\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 153us/sample - loss: 0.0303 - val_loss: 0.0153\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.027 - 0s 135us/sample - loss: 0.0266 - val_loss: 0.0136\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.038 - 0s 130us/sample - loss: 0.0242 - val_loss: 0.0123\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.027 - 0s 103us/sample - loss: 0.0237 - val_loss: 0.0110\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 113us/sample - loss: 0.0182 - val_loss: 0.0097\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 101us/sample - loss: 0.0144 - val_loss: 0.0086\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 107us/sample - loss: 0.0141 - val_loss: 0.0078\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0119 - val_loss: 0.0073\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 106us/sample - loss: 0.0113 - val_loss: 0.0069\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 111us/sample - loss: 0.0097 - val_loss: 0.0068\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 110us/sample - loss: 0.0095 - val_loss: 0.0066\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0083 - val_loss: 0.0065\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0075 - val_loss: 0.0066\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 108us/sample - loss: 0.0076 - val_loss: 0.0067\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 96us/sample - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 86us/sample - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 98us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 92us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 93us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 93us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 93us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 95us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 96us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 103us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 98us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 92us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 95us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 91us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 80us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 87us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 101us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 91us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 91us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 97us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 120us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 123us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 112us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.564 - 0s 1ms/sample - loss: 0.3336 - val_loss: 0.0347\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.327 - 0s 110us/sample - loss: 0.1488 - val_loss: 0.0119\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.073 - 0s 115us/sample - loss: 0.0934 - val_loss: 0.0090\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.082 - 0s 103us/sample - loss: 0.0623 - val_loss: 0.0088\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.034 - 0s 110us/sample - loss: 0.0507 - val_loss: 0.0088\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.064 - 0s 112us/sample - loss: 0.0438 - val_loss: 0.0090\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 111us/sample - loss: 0.0388 - val_loss: 0.0092\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 115us/sample - loss: 0.0342 - val_loss: 0.0091\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 108us/sample - loss: 0.0297 - val_loss: 0.0089\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 103us/sample - loss: 0.0240 - val_loss: 0.0086\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 97us/sample - loss: 0.0223 - val_loss: 0.0084\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 98us/sample - loss: 0.0200 - val_loss: 0.0082\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 108us/sample - loss: 0.0171 - val_loss: 0.0079\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 111us/sample - loss: 0.0178 - val_loss: 0.0077\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.030 - 0s 103us/sample - loss: 0.0151 - val_loss: 0.0075\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 106us/sample - loss: 0.0145 - val_loss: 0.0074\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 106us/sample - loss: 0.0129 - val_loss: 0.0072\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 101us/sample - loss: 0.0120 - val_loss: 0.0071\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 98us/sample - loss: 0.0114 - val_loss: 0.0070\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 95us/sample - loss: 0.0118 - val_loss: 0.0069\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 96us/sample - loss: 0.0095 - val_loss: 0.0068\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 105us/sample - loss: 0.0099 - val_loss: 0.0068\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 92us/sample - loss: 0.0097 - val_loss: 0.0067\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 96us/sample - loss: 0.0091 - val_loss: 0.0067\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 92us/sample - loss: 0.0094 - val_loss: 0.0067\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 88us/sample - loss: 0.0087 - val_loss: 0.0067\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 88us/sample - loss: 0.0077 - val_loss: 0.0067\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 87us/sample - loss: 0.0078 - val_loss: 0.0067\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 105us/sample - loss: 0.0081 - val_loss: 0.0068\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 87us/sample - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 104us/sample - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 92us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 97us/sample - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 91us/sample - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 95us/sample - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 100us/sample - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 102us/sample - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 108us/sample - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 88us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 97us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 101us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 104us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 116us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 126us/sample - loss: 0.0068 - val_loss: 0.0073\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 115us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 107us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 103us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0064 - val_loss: 0.0075\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 54/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 108us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 3s - loss: 0.159 - 0s 1ms/sample - loss: 0.1700 - val_loss: 0.0326\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.058 - 0s 122us/sample - loss: 0.0799 - val_loss: 0.0213\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.088 - 0s 114us/sample - loss: 0.0563 - val_loss: 0.0158\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.081 - 0s 110us/sample - loss: 0.0447 - val_loss: 0.0140\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.037 - 0s 111us/sample - loss: 0.0356 - val_loss: 0.0128\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.025 - 0s 106us/sample - loss: 0.0321 - val_loss: 0.0125\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.027 - 0s 106us/sample - loss: 0.0263 - val_loss: 0.0111\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 109us/sample - loss: 0.0238 - val_loss: 0.0102\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 113us/sample - loss: 0.0223 - val_loss: 0.0096\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 104us/sample - loss: 0.0183 - val_loss: 0.0088\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 118us/sample - loss: 0.0172 - val_loss: 0.0085\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 120us/sample - loss: 0.0159 - val_loss: 0.0077\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 108us/sample - loss: 0.0146 - val_loss: 0.0076\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 113us/sample - loss: 0.0141 - val_loss: 0.0072\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 91us/sample - loss: 0.0130 - val_loss: 0.0071\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 96us/sample - loss: 0.0123 - val_loss: 0.0069\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 88us/sample - loss: 0.0114 - val_loss: 0.0067\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 98us/sample - loss: 0.0116 - val_loss: 0.0065\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 119us/sample - loss: 0.0107 - val_loss: 0.0065\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0095 - val_loss: 0.0064\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 107us/sample - loss: 0.0097 - val_loss: 0.0063\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 111us/sample - loss: 0.0083 - val_loss: 0.0063\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 111us/sample - loss: 0.0085 - val_loss: 0.0063\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 111us/sample - loss: 0.0087 - val_loss: 0.0064\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 118us/sample - loss: 0.0081 - val_loss: 0.0064\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 101us/sample - loss: 0.0082 - val_loss: 0.0064\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 105us/sample - loss: 0.0072 - val_loss: 0.0064\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 102us/sample - loss: 0.0079 - val_loss: 0.0064\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0075 - val_loss: 0.0065\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 115us/sample - loss: 0.0071 - val_loss: 0.0065\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 97us/sample - loss: 0.0074 - val_loss: 0.0065\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 118us/sample - loss: 0.0074 - val_loss: 0.0066\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 133us/sample - loss: 0.0070 - val_loss: 0.0066\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 119us/sample - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 129us/sample - loss: 0.0066 - val_loss: 0.0068\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 118us/sample - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 135us/sample - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 120us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 123us/sample - loss: 0.0063 - val_loss: 0.0070\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 125us/sample - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 112us/sample - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 121us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 118us/sample - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0063 - val_loss: 0.0070\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 102us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 110us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.718 - 0s 1ms/sample - loss: 0.4243 - val_loss: 0.1827\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.131 - 0s 95us/sample - loss: 0.1630 - val_loss: 0.0717\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.111 - 0s 98us/sample - loss: 0.0919 - val_loss: 0.0408\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.066 - 0s 98us/sample - loss: 0.0683 - val_loss: 0.0331\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.066 - 0s 94us/sample - loss: 0.0595 - val_loss: 0.0285\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.041 - 0s 114us/sample - loss: 0.0441 - val_loss: 0.0246\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.038 - 0s 96us/sample - loss: 0.0429 - val_loss: 0.0215\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 98us/sample - loss: 0.0406 - val_loss: 0.0192\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.036 - 0s 100us/sample - loss: 0.0370 - val_loss: 0.0177\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.042 - 0s 96us/sample - loss: 0.0301 - val_loss: 0.0160\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.034 - 0s 96us/sample - loss: 0.0281 - val_loss: 0.0146\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 108us/sample - loss: 0.0285 - val_loss: 0.0133\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.027 - 0s 99us/sample - loss: 0.0240 - val_loss: 0.0122\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 96us/sample - loss: 0.0211 - val_loss: 0.0112\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 99us/sample - loss: 0.0182 - val_loss: 0.0101\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 101us/sample - loss: 0.0176 - val_loss: 0.0092\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 107us/sample - loss: 0.0161 - val_loss: 0.0085\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 111us/sample - loss: 0.0130 - val_loss: 0.0079\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 107us/sample - loss: 0.0127 - val_loss: 0.0073\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 108us/sample - loss: 0.0109 - val_loss: 0.0070\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 97us/sample - loss: 0.0124 - val_loss: 0.0068\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 93us/sample - loss: 0.0111 - val_loss: 0.0067\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 109us/sample - loss: 0.0098 - val_loss: 0.0066\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0097 - val_loss: 0.0065\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 108us/sample - loss: 0.0088 - val_loss: 0.0065\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 109us/sample - loss: 0.0087 - val_loss: 0.0065\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 103us/sample - loss: 0.0098 - val_loss: 0.0066\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0089 - val_loss: 0.0066\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 104us/sample - loss: 0.0076 - val_loss: 0.0066\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 110us/sample - loss: 0.0084 - val_loss: 0.0066\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0090 - val_loss: 0.0067\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 108us/sample - loss: 0.0079 - val_loss: 0.0067\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 104us/sample - loss: 0.0084 - val_loss: 0.0068\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0075 - val_loss: 0.0069\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 106us/sample - loss: 0.0083 - val_loss: 0.0068\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 98us/sample - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 105us/sample - loss: 0.0072 - val_loss: 0.0068\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0075 - val_loss: 0.0069\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 121us/sample - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 106us/sample - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 100us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 100us/sample - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 91us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 92us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 96us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 54/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 94us/sample - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 55/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 100us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.380 - 0s 914us/sample - loss: 0.2212 - val_loss: 0.0165\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.115 - 0s 135us/sample - loss: 0.1467 - val_loss: 0.0120\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.112 - 0s 96us/sample - loss: 0.0870 - val_loss: 0.0113\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.057 - 0s 96us/sample - loss: 0.0730 - val_loss: 0.0100\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.036 - 0s 125us/sample - loss: 0.0482 - val_loss: 0.0096\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.036 - 0s 119us/sample - loss: 0.0386 - val_loss: 0.0093\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.035 - 0s 105us/sample - loss: 0.0331 - val_loss: 0.0088\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.032 - 0s 88us/sample - loss: 0.0297 - val_loss: 0.0087\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.026 - 0s 85us/sample - loss: 0.0282 - val_loss: 0.0082\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.030 - 0s 92us/sample - loss: 0.0224 - val_loss: 0.0078\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 101us/sample - loss: 0.0239 - val_loss: 0.0078\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 103us/sample - loss: 0.0185 - val_loss: 0.0075\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 101us/sample - loss: 0.0168 - val_loss: 0.0072\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 123us/sample - loss: 0.0161 - val_loss: 0.0071\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 123us/sample - loss: 0.0136 - val_loss: 0.0070\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 113us/sample - loss: 0.0134 - val_loss: 0.0069\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 113us/sample - loss: 0.0128 - val_loss: 0.0069\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 113us/sample - loss: 0.0135 - val_loss: 0.0069\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 112us/sample - loss: 0.0118 - val_loss: 0.0069\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 116us/sample - loss: 0.0100 - val_loss: 0.0069\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 110us/sample - loss: 0.0113 - val_loss: 0.0069\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 111us/sample - loss: 0.0103 - val_loss: 0.0069\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 118us/sample - loss: 0.0093 - val_loss: 0.0069\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 110us/sample - loss: 0.0095 - val_loss: 0.0070\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 103us/sample - loss: 0.0100 - val_loss: 0.0070\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 106us/sample - loss: 0.0083 - val_loss: 0.0071\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 103us/sample - loss: 0.0077 - val_loss: 0.0071\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 106us/sample - loss: 0.0077 - val_loss: 0.0071\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 104us/sample - loss: 0.0081 - val_loss: 0.0071\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0078 - val_loss: 0.0072\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 111us/sample - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.002 - 0s 106us/sample - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 118us/sample - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 97us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 90us/sample - loss: 0.0068 - val_loss: 0.0073\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 95us/sample - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 84us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 101us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0075\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 108us/sample - loss: 0.0066 - val_loss: 0.0075\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 108us/sample - loss: 0.0062 - val_loss: 0.0075\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 88us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 94us/sample - loss: 0.0064 - val_loss: 0.0073\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.790 - 0s 889us/sample - loss: 0.4638 - val_loss: 0.0302\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.485 - 0s 102us/sample - loss: 0.1949 - val_loss: 0.0473\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.233 - 0s 93us/sample - loss: 0.1467 - val_loss: 0.0487\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.073 - 0s 92us/sample - loss: 0.0910 - val_loss: 0.0425\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.077 - 0s 95us/sample - loss: 0.0803 - val_loss: 0.0373\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.077 - 0s 92us/sample - loss: 0.0784 - val_loss: 0.0329\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.041 - 0s 106us/sample - loss: 0.0632 - val_loss: 0.0294\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.049 - 0s 93us/sample - loss: 0.0453 - val_loss: 0.0245\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.072 - 0s 98us/sample - loss: 0.0459 - val_loss: 0.0186\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.057 - 0s 114us/sample - loss: 0.0361 - val_loss: 0.0148\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.031 - 0s 118us/sample - loss: 0.0298 - val_loss: 0.0116\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 122us/sample - loss: 0.0304 - val_loss: 0.0095\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 111us/sample - loss: 0.0235 - val_loss: 0.0085\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 105us/sample - loss: 0.0276 - val_loss: 0.0078\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 98us/sample - loss: 0.0190 - val_loss: 0.0074\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 105us/sample - loss: 0.0171 - val_loss: 0.0069\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.017 - 0s 101us/sample - loss: 0.0194 - val_loss: 0.0067\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 108us/sample - loss: 0.0196 - val_loss: 0.0067\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 94us/sample - loss: 0.0157 - val_loss: 0.0065\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 118us/sample - loss: 0.0157 - val_loss: 0.0064\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 108us/sample - loss: 0.0124 - val_loss: 0.0063\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 108us/sample - loss: 0.0119 - val_loss: 0.0063\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 103us/sample - loss: 0.0114 - val_loss: 0.0063\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 109us/sample - loss: 0.0111 - val_loss: 0.0062\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 104us/sample - loss: 0.0105 - val_loss: 0.0062\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 102us/sample - loss: 0.0107 - val_loss: 0.0063\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 98us/sample - loss: 0.0100 - val_loss: 0.0063\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 96us/sample - loss: 0.0092 - val_loss: 0.0063\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 108us/sample - loss: 0.0097 - val_loss: 0.0064\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 104us/sample - loss: 0.0089 - val_loss: 0.0064\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 105us/sample - loss: 0.0081 - val_loss: 0.0064\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 100us/sample - loss: 0.0078 - val_loss: 0.0065\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 108us/sample - loss: 0.0080 - val_loss: 0.0066\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 94us/sample - loss: 0.0074 - val_loss: 0.0066\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 84us/sample - loss: 0.0073 - val_loss: 0.0066\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 96us/sample - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 95us/sample - loss: 0.0078 - val_loss: 0.0068\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 95us/sample - loss: 0.0079 - val_loss: 0.0067\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 92us/sample - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 93us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 117us/sample - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 95us/sample - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 97us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 92us/sample - loss: 0.0077 - val_loss: 0.0070\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 93us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 98us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 103us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 123us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 54/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.137 - 0s 1ms/sample - loss: 0.1133 - val_loss: 0.0565\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.081 - 0s 121us/sample - loss: 0.0793 - val_loss: 0.0405\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.058 - 0s 107us/sample - loss: 0.0609 - val_loss: 0.0268\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.059 - 0s 100us/sample - loss: 0.0441 - val_loss: 0.0172\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.038 - 0s 102us/sample - loss: 0.0350 - val_loss: 0.0113\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.036 - 0s 98us/sample - loss: 0.0290 - val_loss: 0.0092\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.036 - 0s 103us/sample - loss: 0.0289 - val_loss: 0.0083\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.033 - 0s 102us/sample - loss: 0.0268 - val_loss: 0.0082\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.026 - 0s 131us/sample - loss: 0.0223 - val_loss: 0.0077\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 120us/sample - loss: 0.0201 - val_loss: 0.0075\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 113us/sample - loss: 0.0186 - val_loss: 0.0072\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 106us/sample - loss: 0.0160 - val_loss: 0.0070\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 93us/sample - loss: 0.0140 - val_loss: 0.0068\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 92us/sample - loss: 0.0150 - val_loss: 0.0068\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 100us/sample - loss: 0.0130 - val_loss: 0.0067\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 93us/sample - loss: 0.0123 - val_loss: 0.0066\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 92us/sample - loss: 0.0119 - val_loss: 0.0066\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 101us/sample - loss: 0.0110 - val_loss: 0.0066\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 84us/sample - loss: 0.0112 - val_loss: 0.0066\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 97us/sample - loss: 0.0091 - val_loss: 0.0066\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0089 - val_loss: 0.0066\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 100us/sample - loss: 0.0085 - val_loss: 0.0067\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0078 - val_loss: 0.0068\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 96us/sample - loss: 0.0084 - val_loss: 0.0067\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 119us/sample - loss: 0.0077 - val_loss: 0.0067\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 129us/sample - loss: 0.0075 - val_loss: 0.0068\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 111us/sample - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 115us/sample - loss: 0.0076 - val_loss: 0.0069\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 100us/sample - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 102us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 101us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 102us/sample - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 102us/sample - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 101us/sample - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 106us/sample - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 106us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 101us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 108us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 102us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 104us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 135us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 100us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 106us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 102us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 112us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 1.129 - 0s 962us/sample - loss: 0.8152 - val_loss: 0.3517\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.621 - 0s 97us/sample - loss: 0.4223 - val_loss: 0.1789\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.245 - 0s 106us/sample - loss: 0.2130 - val_loss: 0.1014\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.298 - 0s 93us/sample - loss: 0.1769 - val_loss: 0.0576\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.160 - 0s 101us/sample - loss: 0.1065 - val_loss: 0.0381\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.088 - 0s 111us/sample - loss: 0.0749 - val_loss: 0.0309\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.094 - 0s 103us/sample - loss: 0.0607 - val_loss: 0.0267\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.035 - 0s 108us/sample - loss: 0.0461 - val_loss: 0.0230\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.053 - 0s 97us/sample - loss: 0.0410 - val_loss: 0.0196\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 102us/sample - loss: 0.0335 - val_loss: 0.0164\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 102us/sample - loss: 0.0258 - val_loss: 0.0132\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 101us/sample - loss: 0.0243 - val_loss: 0.0109\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 96us/sample - loss: 0.0187 - val_loss: 0.0094\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 96us/sample - loss: 0.0185 - val_loss: 0.0085\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 103us/sample - loss: 0.0179 - val_loss: 0.0080\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 94us/sample - loss: 0.0164 - val_loss: 0.0076\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 98us/sample - loss: 0.0144 - val_loss: 0.0073\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 103us/sample - loss: 0.0142 - val_loss: 0.0071\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 118us/sample - loss: 0.0132 - val_loss: 0.0069\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 125us/sample - loss: 0.0120 - val_loss: 0.0068\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 139us/sample - loss: 0.0120 - val_loss: 0.0067\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0110 - val_loss: 0.0066\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0107 - val_loss: 0.0065\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 105us/sample - loss: 0.0106 - val_loss: 0.0064\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 106us/sample - loss: 0.0094 - val_loss: 0.0064\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0091 - val_loss: 0.0065\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 100us/sample - loss: 0.0097 - val_loss: 0.0064\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 99us/sample - loss: 0.0088 - val_loss: 0.0065\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 103us/sample - loss: 0.0089 - val_loss: 0.0065\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 100us/sample - loss: 0.0078 - val_loss: 0.0065\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 111us/sample - loss: 0.0080 - val_loss: 0.0064\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 96us/sample - loss: 0.0082 - val_loss: 0.0065\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0082 - val_loss: 0.0065\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 101us/sample - loss: 0.0074 - val_loss: 0.0066\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 106us/sample - loss: 0.0077 - val_loss: 0.0066\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 100us/sample - loss: 0.0075 - val_loss: 0.0066\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 112us/sample - loss: 0.0071 - val_loss: 0.0066\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 98us/sample - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 103us/sample - loss: 0.0072 - val_loss: 0.0067\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 89us/sample - loss: 0.0066 - val_loss: 0.0068\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 97us/sample - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 91us/sample - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 90us/sample - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 106us/sample - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0073 - val_loss: 0.0069\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 97us/sample - loss: 0.0076 - val_loss: 0.0068\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 99us/sample - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 95us/sample - loss: 0.0063 - val_loss: 0.0070\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 88us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 93us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 54/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 93us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.068 - 0s 929us/sample - loss: 0.0600 - val_loss: 0.0287\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.044 - 0s 110us/sample - loss: 0.0380 - val_loss: 0.0148\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.027 - 0s 116us/sample - loss: 0.0318 - val_loss: 0.0118\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.024 - 0s 103us/sample - loss: 0.0274 - val_loss: 0.0106\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.029 - 0s 113us/sample - loss: 0.0272 - val_loss: 0.0095\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 101us/sample - loss: 0.0232 - val_loss: 0.0092\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 87us/sample - loss: 0.0198 - val_loss: 0.0084\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 110us/sample - loss: 0.0180 - val_loss: 0.0079\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 112us/sample - loss: 0.0170 - val_loss: 0.0075\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 115us/sample - loss: 0.0151 - val_loss: 0.0070\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 108us/sample - loss: 0.0138 - val_loss: 0.0067\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 111us/sample - loss: 0.0131 - val_loss: 0.0065\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 125us/sample - loss: 0.0116 - val_loss: 0.0063\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0114 - val_loss: 0.0063\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0109 - val_loss: 0.0063\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 113us/sample - loss: 0.0110 - val_loss: 0.0062\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 102us/sample - loss: 0.0092 - val_loss: 0.0062\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 113us/sample - loss: 0.0097 - val_loss: 0.0062\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 100us/sample - loss: 0.0091 - val_loss: 0.0062\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 145us/sample - loss: 0.0086 - val_loss: 0.0062\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 101us/sample - loss: 0.0084 - val_loss: 0.0063\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 105us/sample - loss: 0.0085 - val_loss: 0.0063\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 110us/sample - loss: 0.0074 - val_loss: 0.0064\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0076 - val_loss: 0.0065\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 99us/sample - loss: 0.0075 - val_loss: 0.0065\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 99us/sample - loss: 0.0069 - val_loss: 0.0066\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 98us/sample - loss: 0.0074 - val_loss: 0.0067\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 105us/sample - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 113us/sample - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 92us/sample - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 86us/sample - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 95us/sample - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 108us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 88us/sample - loss: 0.0059 - val_loss: 0.0070\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 86us/sample - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 91us/sample - loss: 0.0063 - val_loss: 0.0070\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 108us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 96us/sample - loss: 0.0062 - val_loss: 0.0070\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 89us/sample - loss: 0.0064 - val_loss: 0.0071\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 90us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 106us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 95us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.002 - 0s 99us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 96us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.395 - 0s 963us/sample - loss: 0.2844 - val_loss: 0.1205\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.181 - 0s 117us/sample - loss: 0.1680 - val_loss: 0.0814\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.122 - 0s 101us/sample - loss: 0.1117 - val_loss: 0.0608\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.112 - 0s 112us/sample - loss: 0.0831 - val_loss: 0.0485\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.060 - 0s 109us/sample - loss: 0.0651 - val_loss: 0.0404\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.056 - 0s 106us/sample - loss: 0.0578 - val_loss: 0.0338\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.042 - 0s 112us/sample - loss: 0.0471 - val_loss: 0.0280\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.035 - 0s 117us/sample - loss: 0.0396 - val_loss: 0.0235\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.050 - 0s 101us/sample - loss: 0.0318 - val_loss: 0.0201\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.032 - 0s 114us/sample - loss: 0.0279 - val_loss: 0.0172\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 154us/sample - loss: 0.0233 - val_loss: 0.0149\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 123us/sample - loss: 0.0213 - val_loss: 0.0130\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 122us/sample - loss: 0.0172 - val_loss: 0.0113\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 115us/sample - loss: 0.0158 - val_loss: 0.0098\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0141 - val_loss: 0.0090\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 133us/sample - loss: 0.0138 - val_loss: 0.0082\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 115us/sample - loss: 0.0117 - val_loss: 0.0076\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 119us/sample - loss: 0.0110 - val_loss: 0.0073\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0099 - val_loss: 0.0071\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 104us/sample - loss: 0.0089 - val_loss: 0.0071\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 107us/sample - loss: 0.0094 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 103us/sample - loss: 0.0085 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0084 - val_loss: 0.0071\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 102us/sample - loss: 0.0085 - val_loss: 0.0072\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 84us/sample - loss: 0.0080 - val_loss: 0.0072\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 91us/sample - loss: 0.0076 - val_loss: 0.0074\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 105us/sample - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 96us/sample - loss: 0.0082 - val_loss: 0.0072\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0073 - val_loss: 0.0072\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 98us/sample - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 95us/sample - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 94us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 89us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 93us/sample - loss: 0.0069 - val_loss: 0.0073\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0069 - val_loss: 0.0073\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 104us/sample - loss: 0.0068 - val_loss: 0.0073\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 98us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 99us/sample - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 108us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 108us/sample - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 102us/sample - loss: 0.0067 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 86us/sample - loss: 0.0069 - val_loss: 0.0073\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 89us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0065 - val_loss: 0.0075\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 118us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0067 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 114us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 52/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0061 - val_loss: 0.0074\n",
      "Epoch 53/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0063 - val_loss: 0.0075\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.028 - 0s 1ms/sample - loss: 0.0587 - val_loss: 0.0139\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.042 - 0s 109us/sample - loss: 0.0364 - val_loss: 0.0132\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.032 - 0s 115us/sample - loss: 0.0345 - val_loss: 0.0121\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 109us/sample - loss: 0.0296 - val_loss: 0.0113\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.034 - 0s 113us/sample - loss: 0.0261 - val_loss: 0.0099\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 123us/sample - loss: 0.0214 - val_loss: 0.0091\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 111us/sample - loss: 0.0193 - val_loss: 0.0086\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.031 - 0s 110us/sample - loss: 0.0168 - val_loss: 0.0078\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 128us/sample - loss: 0.0141 - val_loss: 0.0072\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 93us/sample - loss: 0.0141 - val_loss: 0.0070\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 90us/sample - loss: 0.0123 - val_loss: 0.0068\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 93us/sample - loss: 0.0105 - val_loss: 0.0065\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 96us/sample - loss: 0.0097 - val_loss: 0.0063\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 98us/sample - loss: 0.0100 - val_loss: 0.0062\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 93us/sample - loss: 0.0089 - val_loss: 0.0061\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0095 - val_loss: 0.0061\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0090 - val_loss: 0.0061\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 98us/sample - loss: 0.0077 - val_loss: 0.0061\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 112us/sample - loss: 0.0078 - val_loss: 0.0061\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 102us/sample - loss: 0.0083 - val_loss: 0.0061\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0070 - val_loss: 0.0062\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 86us/sample - loss: 0.0072 - val_loss: 0.0062\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 87us/sample - loss: 0.0076 - val_loss: 0.0062\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 103us/sample - loss: 0.0067 - val_loss: 0.0063\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 100us/sample - loss: 0.0067 - val_loss: 0.0064\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 104us/sample - loss: 0.0067 - val_loss: 0.0064\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 94us/sample - loss: 0.0069 - val_loss: 0.0064\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 103us/sample - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 93us/sample - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0069 - val_loss: 0.0066\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0063 - val_loss: 0.0067\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 101us/sample - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 98us/sample - loss: 0.0063 - val_loss: 0.0069\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 99us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 95us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 106us/sample - loss: 0.0062 - val_loss: 0.0071\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 106us/sample - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 100us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 98us/sample - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 104us/sample - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 106us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 121us/sample - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 127us/sample - loss: 0.0061 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 3s - loss: 0.151 - 0s 1ms/sample - loss: 0.1642 - val_loss: 0.0367\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.081 - 0s 118us/sample - loss: 0.0968 - val_loss: 0.0429\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.096 - 0s 111us/sample - loss: 0.0674 - val_loss: 0.0320\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.051 - 0s 106us/sample - loss: 0.0440 - val_loss: 0.0231\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.036 - 0s 93us/sample - loss: 0.0378 - val_loss: 0.0164\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 103us/sample - loss: 0.0281 - val_loss: 0.0123\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.034 - 0s 92us/sample - loss: 0.0271 - val_loss: 0.0111\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.023 - 0s 96us/sample - loss: 0.0218 - val_loss: 0.0095\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.026 - 0s 98us/sample - loss: 0.0236 - val_loss: 0.0086\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 103us/sample - loss: 0.0201 - val_loss: 0.0078\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 103us/sample - loss: 0.0191 - val_loss: 0.0072\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 123us/sample - loss: 0.0160 - val_loss: 0.0072\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 142us/sample - loss: 0.0158 - val_loss: 0.0070\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 160us/sample - loss: 0.0154 - val_loss: 0.0068\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 95us/sample - loss: 0.0133 - val_loss: 0.0066\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 98us/sample - loss: 0.0131 - val_loss: 0.0065\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 89us/sample - loss: 0.0133 - val_loss: 0.0065\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 93us/sample - loss: 0.0115 - val_loss: 0.0065\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 97us/sample - loss: 0.0110 - val_loss: 0.0065\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 93us/sample - loss: 0.0097 - val_loss: 0.0066\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0102 - val_loss: 0.0066\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 95us/sample - loss: 0.0098 - val_loss: 0.0067\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 92us/sample - loss: 0.0091 - val_loss: 0.0068\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 96us/sample - loss: 0.0089 - val_loss: 0.0069\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 118us/sample - loss: 0.0084 - val_loss: 0.0068\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 109us/sample - loss: 0.0090 - val_loss: 0.0068\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 114us/sample - loss: 0.0079 - val_loss: 0.0069\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 116us/sample - loss: 0.0079 - val_loss: 0.0070\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0078 - val_loss: 0.0072\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 108us/sample - loss: 0.0075 - val_loss: 0.0071\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 112us/sample - loss: 0.0080 - val_loss: 0.0072\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 114us/sample - loss: 0.0070 - val_loss: 0.0073\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 96us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 113us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 99us/sample - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0070 - val_loss: 0.0073\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 108us/sample - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 104us/sample - loss: 0.0068 - val_loss: 0.0073\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 104us/sample - loss: 0.0070 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 103us/sample - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 91us/sample - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 97us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 94us/sample - loss: 0.0067 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 114us/sample - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 105us/sample - loss: 0.0067 - val_loss: 0.0074\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 3s - loss: 0.107 - 0s 1ms/sample - loss: 0.0854 - val_loss: 0.0572\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.071 - 0s 115us/sample - loss: 0.0627 - val_loss: 0.0468\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.052 - 0s 111us/sample - loss: 0.0480 - val_loss: 0.0348\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.038 - 0s 113us/sample - loss: 0.0379 - val_loss: 0.0262\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.034 - 0s 108us/sample - loss: 0.0331 - val_loss: 0.0205\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.035 - 0s 107us/sample - loss: 0.0280 - val_loss: 0.0160\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.031 - 0s 112us/sample - loss: 0.0223 - val_loss: 0.0126\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.026 - 0s 113us/sample - loss: 0.0213 - val_loss: 0.0107\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.022 - 0s 108us/sample - loss: 0.0191 - val_loss: 0.0095\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 110us/sample - loss: 0.0164 - val_loss: 0.0088\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 133us/sample - loss: 0.0156 - val_loss: 0.0080\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 108us/sample - loss: 0.0128 - val_loss: 0.0077\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 113us/sample - loss: 0.0128 - val_loss: 0.0074\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0115 - val_loss: 0.0070\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 121us/sample - loss: 0.0108 - val_loss: 0.0068\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 134us/sample - loss: 0.0113 - val_loss: 0.0066\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 120us/sample - loss: 0.0105 - val_loss: 0.0065\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 116us/sample - loss: 0.0093 - val_loss: 0.0064\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0089 - val_loss: 0.0064\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 119us/sample - loss: 0.0084 - val_loss: 0.0064\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 99us/sample - loss: 0.0088 - val_loss: 0.0064\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 101us/sample - loss: 0.0080 - val_loss: 0.0064\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 100us/sample - loss: 0.0080 - val_loss: 0.0065\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0074 - val_loss: 0.0066\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 103us/sample - loss: 0.0078 - val_loss: 0.0067\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 110us/sample - loss: 0.0072 - val_loss: 0.0068\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 100us/sample - loss: 0.0076 - val_loss: 0.0069\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 99us/sample - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 96us/sample - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 96us/sample - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 99us/sample - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 108us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 114us/sample - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 120us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 102us/sample - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 101us/sample - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 98us/sample - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0063 - val_loss: 0.0071\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 112us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 93us/sample - loss: 0.0064 - val_loss: 0.0072\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 135us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 88us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 97us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 98us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 3s - loss: 0.262 - 1s 1ms/sample - loss: 0.1758 - val_loss: 0.0534\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.071 - 0s 128us/sample - loss: 0.1074 - val_loss: 0.0250\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.068 - 0s 123us/sample - loss: 0.0668 - val_loss: 0.0178\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.072 - 0s 127us/sample - loss: 0.0570 - val_loss: 0.0154\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.039 - 0s 128us/sample - loss: 0.0413 - val_loss: 0.0132\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.037 - 0s 169us/sample - loss: 0.0365 - val_loss: 0.0120\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.035 - 0s 102us/sample - loss: 0.0329 - val_loss: 0.0105\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.037 - 0s 118us/sample - loss: 0.0270 - val_loss: 0.0097\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 111us/sample - loss: 0.0224 - val_loss: 0.0089\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 111us/sample - loss: 0.0213 - val_loss: 0.0084\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 117us/sample - loss: 0.0200 - val_loss: 0.0078\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 95us/sample - loss: 0.0179 - val_loss: 0.0074\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 98us/sample - loss: 0.0173 - val_loss: 0.0070\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 86us/sample - loss: 0.0144 - val_loss: 0.0068\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 96us/sample - loss: 0.0135 - val_loss: 0.0067\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.019 - 0s 102us/sample - loss: 0.0128 - val_loss: 0.0067\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.016 - 0s 136us/sample - loss: 0.0131 - val_loss: 0.0066\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 131us/sample - loss: 0.0119 - val_loss: 0.0066\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 104us/sample - loss: 0.0103 - val_loss: 0.0066\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.015 - 0s 95us/sample - loss: 0.0105 - val_loss: 0.0066\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 96us/sample - loss: 0.0097 - val_loss: 0.0066\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 82us/sample - loss: 0.0094 - val_loss: 0.0066\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 98us/sample - loss: 0.0091 - val_loss: 0.0066\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 108us/sample - loss: 0.0087 - val_loss: 0.0066\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 84us/sample - loss: 0.0088 - val_loss: 0.0066\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 96us/sample - loss: 0.0077 - val_loss: 0.0067\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 99us/sample - loss: 0.0083 - val_loss: 0.0067\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 96us/sample - loss: 0.0082 - val_loss: 0.0068\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 101us/sample - loss: 0.0081 - val_loss: 0.0068\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 104us/sample - loss: 0.0076 - val_loss: 0.0069\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 110us/sample - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0076 - val_loss: 0.0069\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 140us/sample - loss: 0.0073 - val_loss: 0.0069\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 123us/sample - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 103us/sample - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 110us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 108us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 105us/sample - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 106us/sample - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 116us/sample - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 106us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 102us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 104us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 106us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0063 - val_loss: 0.0072\n",
      "Train on 406 samples, validate on 174 samples\n",
      "Epoch 1/150\n",
      "406/406 [==============================] - ETA: 2s - loss: 0.087 - 0s 1ms/sample - loss: 0.0721 - val_loss: 0.0311\n",
      "Epoch 2/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.047 - 0s 103us/sample - loss: 0.0432 - val_loss: 0.0114\n",
      "Epoch 3/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.051 - 0s 105us/sample - loss: 0.0303 - val_loss: 0.0087\n",
      "Epoch 4/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 94us/sample - loss: 0.0279 - val_loss: 0.0088\n",
      "Epoch 5/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.020 - 0s 92us/sample - loss: 0.0222 - val_loss: 0.0082\n",
      "Epoch 6/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.014 - 0s 98us/sample - loss: 0.0225 - val_loss: 0.0080\n",
      "Epoch 7/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.028 - 0s 101us/sample - loss: 0.0190 - val_loss: 0.0077\n",
      "Epoch 8/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 92us/sample - loss: 0.0177 - val_loss: 0.0075\n",
      "Epoch 9/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.021 - 0s 100us/sample - loss: 0.0163 - val_loss: 0.0074\n",
      "Epoch 10/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 94us/sample - loss: 0.0156 - val_loss: 0.0072\n",
      "Epoch 11/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.018 - 0s 92us/sample - loss: 0.0121 - val_loss: 0.0072\n",
      "Epoch 12/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 99us/sample - loss: 0.0140 - val_loss: 0.0070\n",
      "Epoch 13/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 96us/sample - loss: 0.0116 - val_loss: 0.0069\n",
      "Epoch 14/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 101us/sample - loss: 0.0122 - val_loss: 0.0068\n",
      "Epoch 15/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 96us/sample - loss: 0.0103 - val_loss: 0.0068\n",
      "Epoch 16/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 96us/sample - loss: 0.0102 - val_loss: 0.0068\n",
      "Epoch 17/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 96us/sample - loss: 0.0098 - val_loss: 0.0068\n",
      "Epoch 18/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 118us/sample - loss: 0.0104 - val_loss: 0.0068\n",
      "Epoch 19/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.013 - 0s 118us/sample - loss: 0.0090 - val_loss: 0.0069\n",
      "Epoch 20/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0091 - val_loss: 0.0067\n",
      "Epoch 21/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0092 - val_loss: 0.0067\n",
      "Epoch 22/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0084 - val_loss: 0.0067\n",
      "Epoch 23/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 125us/sample - loss: 0.0085 - val_loss: 0.0068\n",
      "Epoch 24/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 104us/sample - loss: 0.0080 - val_loss: 0.0068\n",
      "Epoch 25/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.012 - 0s 115us/sample - loss: 0.0080 - val_loss: 0.0068\n",
      "Epoch 26/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.011 - 0s 120us/sample - loss: 0.0081 - val_loss: 0.0068\n",
      "Epoch 27/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 112us/sample - loss: 0.0075 - val_loss: 0.0069\n",
      "Epoch 28/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 108us/sample - loss: 0.0076 - val_loss: 0.0069\n",
      "Epoch 29/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 95us/sample - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 30/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 99us/sample - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 31/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 0s 201us/sample - loss: 0.0075 - val_loss: 0.0069\n",
      "Epoch 32/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 0s 253us/sample - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 33/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.007 - 0s 214us/sample - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 34/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 147us/sample - loss: 0.0071 - val_loss: 0.0070\n",
      "Epoch 35/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 158us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 36/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 187us/sample - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 37/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 145us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 38/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 123us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 39/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.009 - 0s 127us/sample - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 40/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.010 - 0s 123us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 41/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 42/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 142us/sample - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 43/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 44/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 114us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 45/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 46/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 121us/sample - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 47/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0065 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.008 - 0s 107us/sample - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.005 - 0s 99us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 51/150\n",
      "406/406 [==============================] - ETA: 0s - loss: 0.003 - 0s 120us/sample - loss: 0.0063 - val_loss: 0.0073\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(testFeature).reshape(testTarget.shape[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27805b3ba88>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(result)\n",
    "plt.plot(testTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2548331654518842"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2777890739174401"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTarget.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27182852078352016"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainTarget.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
