{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T07:30:20.151038Z",
     "start_time": "2020-09-02T07:29:18.486999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (2.6.2)\n",
      "Requirement already satisfied: jdcal in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (from openpyxl) (1.4.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (from openpyxl) (1.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jmp08\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\n",
      "      T_ID   GDAY_DS  HEADER_NO   P_ID  START_CK  BAT_ORDER_NO   PA   AB  RBI  \\\n",
      "0       HH  20160401          0  60404         0             3    1    1    0   \n",
      "1       HH  20160401          0  62700         1             9    2    2    0   \n",
      "2       HH  20160401          0  64086         1             7    6    4    0   \n",
      "3       HH  20160401          0  66740         1             5    6    6    0   \n",
      "4       HH  20160401          0  71347         1             2    6    6    1   \n",
      "...    ...       ...        ...    ...       ...           ...  ...  ...  ...   \n",
      "18679   WO  20161009          0  74215        91           374  402  341   80   \n",
      "18680   WO  20161009          0  78168       139           177  646  560   63   \n",
      "18681   WO  20161009          0  79130        15           251   80   66    7   \n",
      "18682   WO  20161009          0  79300        13           429  106   91    9   \n",
      "18683   WO  20161009          0  79365       122           965  454  411   70   \n",
      "\n",
      "       RUN  ...  BB  IB  HP  KK  GD  ERR  LOB  P_AB_CN  P_HIT_CN  GAME_COUNT  \n",
      "0        0  ...   0   0   0   0   0    0    1        0         0           1  \n",
      "1        1  ...   0   0   0   0   0    1    0        0         0           1  \n",
      "2        0  ...   1   0   0   3   0    0    1        2         0           1  \n",
      "3        0  ...   0   0   0   0   0    0    1        1         1           1  \n",
      "4        1  ...   0   0   0   2   0    0    0        1         0           1  \n",
      "...    ...  ...  ..  ..  ..  ..  ..  ...  ...      ...       ...         ...  \n",
      "18679   72  ...  46   3   9  50  16    6   77      121        35          92  \n",
      "18680  111  ...  69   2  10  58   6   15  113      118        36         140  \n",
      "18681   10  ...  11   0   1  24   1    1   18       20         3          40  \n",
      "18682   16  ...  10   0   0  16   3    0   29       25         4          81  \n",
      "18683   44  ...  27   0   7  93  18    7   71      132        33         127  \n",
      "\n",
      "[18684 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "%run batter_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:05:08.382310Z",
     "start_time": "2020-09-02T13:05:08.376351Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:93: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\jmp08\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objs as go\n",
    "import xgboost\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from statsmodels import tsa\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from plotly.offline import plot\n",
    "from plotly.offline import init_notebook_mode\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "feature, target = batter_data(\"WO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(484, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFeature = feature.loc[0:feature.shape[0]-20]\n",
    "trainTarget = target.loc[0:feature.shape[0]-20]\n",
    "\n",
    "testFeature = feature.loc[feature.shape[0]-20:].reset_index(drop = True)\n",
    "testTarget = target[feature.shape[0]-20:].reset_index(drop = True)\n",
    "trainFeature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "res = scaler.fit(trainFeature)\n",
    "res = scaler.transform(trainFeature)\n",
    "trainFeature = pd.DataFrame(res, columns = trainFeature.columns, index = list(trainFeature.index.values))\n",
    "res = scaler.transform(testFeature)\n",
    "testFeature = pd.DataFrame(res, columns = testFeature.columns, index = list(testFeature.index.values))\n",
    "\n",
    "res = scaler.fit(np.array(trainTarget).reshape(trainTarget.shape[0], 1))\n",
    "trainTarget = scaler.transform(np.array(trainTarget).reshape(trainTarget.shape[0], 1))\n",
    "testTarget = scaler.transform(np.array(testTarget).reshape(testTarget.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Input(shape = trainFeature.shape[1]))\n",
    "model.add(keras.layers.Dense(16, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(32, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(64, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(128, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(128, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(128, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(64, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(32, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(16, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(8, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(4, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(2, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(rate = 0.3))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.Dense(1, activation = None))\n",
    "model.compile(optimizer = \"Adam\", loss = \"mse\")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(min_delta = 0.0005, patience = 30, restore_best_weights = True)\n",
    "\n",
    "history = model.fit(trainFeature, trainTarget, epochs = 200, validation_split = 0.3, shuffle = True,\n",
    "          use_multiprocessing = True, callbacks = [early_stopping], batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_250\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1750 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1751 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1752 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1753 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1754 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1755 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1756 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.095 - ETA: 0s - loss: 0.080 - 1s 1ms/sample - loss: 0.0816 - val_loss: 0.0670\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - 0s 177us/sample - loss: 0.0681 - val_loss: 0.0558\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.056 - 0s 152us/sample - loss: 0.0569 - val_loss: 0.0463\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.054 - 0s 131us/sample - loss: 0.0475 - val_loss: 0.0383\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.040 - 0s 179us/sample - loss: 0.0395 - val_loss: 0.0315\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.033 - 0s 198us/sample - loss: 0.0327 - val_loss: 0.0260\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.029 - 0s 188us/sample - loss: 0.0272 - val_loss: 0.0214\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - 0s 186us/sample - loss: 0.0226 - val_loss: 0.0179\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.018 - 0s 167us/sample - loss: 0.0189 - val_loss: 0.0150\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.016 - 0s 153us/sample - loss: 0.0159 - val_loss: 0.0128\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 131us/sample - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0118 - val_loss: 0.0098\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 127us/sample - loss: 0.0103 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 0s 152us/sample - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 122us/sample - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 125us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 119us/sample - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 121us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.006 - 0s 171us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 103us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 116us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_251\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1757 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1758 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1759 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1760 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1761 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1762 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1763 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.063 - 1s 1ms/sample - loss: 0.0247 - val_loss: 0.0090\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 0s 155us/sample - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 134us/sample - loss: 0.0075 - val_loss: 0.0081\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.007 - 0s 157us/sample - loss: 0.0073 - val_loss: 0.0083\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.007 - 0s 160us/sample - loss: 0.0074 - val_loss: 0.0079\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 127us/sample - loss: 0.0070 - val_loss: 0.0082\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0069 - val_loss: 0.0081\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0069 - val_loss: 0.0082\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0070 - val_loss: 0.0085\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0069 - val_loss: 0.0094\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.007 - 0s 163us/sample - loss: 0.0070 - val_loss: 0.0087\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 151us/sample - loss: 0.0068 - val_loss: 0.0083\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 130us/sample - loss: 0.0069 - val_loss: 0.0083\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0079\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 114us/sample - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 132us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 132us/sample - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0065 - val_loss: 0.0093\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0064 - val_loss: 0.0090\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 117us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0065 - val_loss: 0.0079\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0081\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0065 - val_loss: 0.0081\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 132us/sample - loss: 0.0063 - val_loss: 0.0080\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 130us/sample - loss: 0.0063 - val_loss: 0.0089\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 124us/sample - loss: 0.0063 - val_loss: 0.0092\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0063 - val_loss: 0.0087\n",
      "Model: \"sequential_252\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1764 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1765 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1766 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1767 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1768 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1769 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1770 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 5s - loss: 0.095 - 1s 1ms/sample - loss: 0.0815 - val_loss: 0.0671\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.089 - 0s 119us/sample - loss: 0.0683 - val_loss: 0.0560\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.056 - 0s 126us/sample - loss: 0.0572 - val_loss: 0.0465\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.045 - 0s 132us/sample - loss: 0.0477 - val_loss: 0.0384\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.039 - 0s 152us/sample - loss: 0.0396 - val_loss: 0.0316\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.040 - 0s 132us/sample - loss: 0.0329 - val_loss: 0.0261\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.017 - 0s 128us/sample - loss: 0.0273 - val_loss: 0.0216\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.022 - 0s 126us/sample - loss: 0.0227 - val_loss: 0.0180\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.025 - 0s 130us/sample - loss: 0.0190 - val_loss: 0.0151\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.020 - ETA: 0s - loss: 0.015 - 0s 163us/sample - loss: 0.0160 - val_loss: 0.0128\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - 0s 157us/sample - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 128us/sample - loss: 0.0118 - val_loss: 0.0099\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 130us/sample - loss: 0.0104 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.009 - 0s 150us/sample - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 121us/sample - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.008 - 0s 152us/sample - loss: 0.0080 - val_loss: 0.0076\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 130us/sample - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 124us/sample - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 160us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 177us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 208us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 165us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0075\n",
      "Model: \"sequential_253\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1771 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1772 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1773 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1774 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1775 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1776 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1777 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.027 - 1s 1ms/sample - loss: 0.0111 - val_loss: 0.0098\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0083 - val_loss: 0.0095\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0073 - val_loss: 0.0106\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.007 - 0s 155us/sample - loss: 0.0071 - val_loss: 0.0098\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.007 - 0s 146us/sample - loss: 0.0071 - val_loss: 0.0101\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 130us/sample - loss: 0.0069 - val_loss: 0.0099\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 127us/sample - loss: 0.0067 - val_loss: 0.0096\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0065 - val_loss: 0.0118\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0070 - val_loss: 0.0113\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0065 - val_loss: 0.0092\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 117us/sample - loss: 0.0065 - val_loss: 0.0098\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 112us/sample - loss: 0.0066 - val_loss: 0.0109\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0063 - val_loss: 0.0096\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0064 - val_loss: 0.0089\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 125us/sample - loss: 0.0069 - val_loss: 0.0091\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 117us/sample - loss: 0.0064 - val_loss: 0.0092\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0063 - val_loss: 0.0098\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 120us/sample - loss: 0.0067 - val_loss: 0.0121\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0063 - val_loss: 0.0098\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0061 - val_loss: 0.0108\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 118us/sample - loss: 0.0062 - val_loss: 0.0092\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 116us/sample - loss: 0.0061 - val_loss: 0.0092\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 109us/sample - loss: 0.0062 - val_loss: 0.0114\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 142us/sample - loss: 0.0063 - val_loss: 0.0096\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0061 - val_loss: 0.0090\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 136us/sample - loss: 0.0060 - val_loss: 0.0089\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0061 - val_loss: 0.0093\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0060 - val_loss: 0.0092\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.005 - 0s 148us/sample - loss: 0.0060 - val_loss: 0.0093\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 127us/sample - loss: 0.0060 - val_loss: 0.0096\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 130us/sample - loss: 0.0060 - val_loss: 0.0092\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.006 - 0s 154us/sample - loss: 0.0062 - val_loss: 0.0090\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0061 - val_loss: 0.0095\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0058 - val_loss: 0.0091\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 146us/sample - loss: 0.0058 - val_loss: 0.0109\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 127us/sample - loss: 0.0062 - val_loss: 0.0091\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 125us/sample - loss: 0.0061 - val_loss: 0.0090\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0063 - val_loss: 0.0104\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 100us/sample - loss: 0.0059 - val_loss: 0.0091\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0058 - val_loss: 0.0088\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 103us/sample - loss: 0.0059 - val_loss: 0.0095\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 103us/sample - loss: 0.0058 - val_loss: 0.0094\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0058 - val_loss: 0.0094\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 118us/sample - loss: 0.0059 - val_loss: 0.0102\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0062 - val_loss: 0.0100\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.005 - 0s 168us/sample - loss: 0.0059 - val_loss: 0.0088\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 155us/sample - loss: 0.0058 - val_loss: 0.0101\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.005 - 0s 167us/sample - loss: 0.0056 - val_loss: 0.0094\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0056 - val_loss: 0.0092\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0057 - val_loss: 0.0098\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 125us/sample - loss: 0.0056 - val_loss: 0.0095\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 113us/sample - loss: 0.0059 - val_loss: 0.0100\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 125us/sample - loss: 0.0056 - val_loss: 0.0094\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 132us/sample - loss: 0.0057 - val_loss: 0.0096\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0056 - val_loss: 0.0089\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0058 - val_loss: 0.0106\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 118us/sample - loss: 0.0056 - val_loss: 0.0105\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 120us/sample - loss: 0.0056 - val_loss: 0.0101\n",
      "Epoch 59/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 165us/sample - loss: 0.0056 - val_loss: 0.0113\n",
      "Epoch 60/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.005 - 0s 169us/sample - loss: 0.0055 - val_loss: 0.0098\n",
      "Epoch 61/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 177us/sample - loss: 0.0056 - val_loss: 0.0106\n",
      "Epoch 62/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 155us/sample - loss: 0.0057 - val_loss: 0.0088\n",
      "Epoch 63/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 157us/sample - loss: 0.0055 - val_loss: 0.0099\n",
      "Epoch 64/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.005 - 0s 167us/sample - loss: 0.0054 - val_loss: 0.0093\n",
      "Epoch 65/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.005 - 0s 152us/sample - loss: 0.0055 - val_loss: 0.0087\n",
      "Epoch 66/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 145us/sample - loss: 0.0055 - val_loss: 0.0098\n",
      "Epoch 67/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 120us/sample - loss: 0.0057 - val_loss: 0.0105\n",
      "Epoch 68/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0054 - val_loss: 0.0093\n",
      "Epoch 69/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0053 - val_loss: 0.0101\n",
      "Epoch 70/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 146us/sample - loss: 0.0054 - val_loss: 0.0100\n",
      "Epoch 71/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 163us/sample - loss: 0.0054 - val_loss: 0.0101\n",
      "Epoch 72/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.005 - 0s 157us/sample - loss: 0.0054 - val_loss: 0.0093\n",
      "Epoch 73/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 114us/sample - loss: 0.0052 - val_loss: 0.0108\n",
      "Epoch 74/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0053 - val_loss: 0.0113\n",
      "Epoch 75/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0053 - val_loss: 0.0097\n",
      "Epoch 76/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 121us/sample - loss: 0.0052 - val_loss: 0.0099\n",
      "Epoch 77/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.005 - 0s 179us/sample - loss: 0.0052 - val_loss: 0.0100\n",
      "Epoch 78/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 169us/sample - loss: 0.0051 - val_loss: 0.0105\n",
      "Epoch 79/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.005 - 0s 163us/sample - loss: 0.0053 - val_loss: 0.0099\n",
      "Epoch 80/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.005 - 0s 179us/sample - loss: 0.0055 - val_loss: 0.0112\n",
      "Epoch 81/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 171us/sample - loss: 0.0052 - val_loss: 0.0109\n",
      "Epoch 82/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0050 - val_loss: 0.0100\n",
      "Epoch 83/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.004 - 0s 175us/sample - loss: 0.0051 - val_loss: 0.0101\n",
      "Epoch 84/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 167us/sample - loss: 0.0050 - val_loss: 0.0098\n",
      "Epoch 85/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 165us/sample - loss: 0.0051 - val_loss: 0.0116\n",
      "Epoch 86/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 165us/sample - loss: 0.0051 - val_loss: 0.0104\n",
      "Epoch 87/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.005 - 0s 143us/sample - loss: 0.0053 - val_loss: 0.0107\n",
      "Epoch 88/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 134us/sample - loss: 0.0049 - val_loss: 0.0114\n",
      "Epoch 89/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.004 - 0s 173us/sample - loss: 0.0048 - val_loss: 0.0094\n",
      "Epoch 90/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.004 - 0s 163us/sample - loss: 0.0048 - val_loss: 0.0104\n",
      "Epoch 91/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 132us/sample - loss: 0.0050 - val_loss: 0.0117\n",
      "Epoch 92/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.004 - 0s 159us/sample - loss: 0.0048 - val_loss: 0.0107\n",
      "Epoch 93/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.005 - 0s 148us/sample - loss: 0.0049 - val_loss: 0.0098\n",
      "Epoch 94/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 110us/sample - loss: 0.0048 - val_loss: 0.0121\n",
      "Epoch 95/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 106us/sample - loss: 0.0054 - val_loss: 0.0098\n",
      "Model: \"sequential_254\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1778 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1779 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1780 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1781 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1782 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1783 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1784 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.081 - 1s 1ms/sample - loss: 0.0809 - val_loss: 0.0670\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.073 - 0s 123us/sample - loss: 0.0680 - val_loss: 0.0558\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.055 - 0s 124us/sample - loss: 0.0569 - val_loss: 0.0462\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.047 - 0s 155us/sample - loss: 0.0474 - val_loss: 0.0381\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.039 - 0s 165us/sample - loss: 0.0394 - val_loss: 0.0315\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.044 - 0s 130us/sample - loss: 0.0327 - val_loss: 0.0259\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.027 - 0s 124us/sample - loss: 0.0271 - val_loss: 0.0214\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.026 - 0s 120us/sample - loss: 0.0225 - val_loss: 0.0178\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.025 - 0s 122us/sample - loss: 0.0188 - val_loss: 0.0150\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.014 - 0s 122us/sample - loss: 0.0158 - val_loss: 0.0127\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.014 - 0s 123us/sample - loss: 0.0135 - val_loss: 0.0110\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 124us/sample - loss: 0.0117 - val_loss: 0.0098\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0103 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 111us/sample - loss: 0.0093 - val_loss: 0.0082\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0084 - val_loss: 0.0078\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 113us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 111us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 107us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 114us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 183us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 121us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 119us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 121us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 171us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 165us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 119us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 114us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 103us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_255\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1785 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1786 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1787 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1788 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1789 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1790 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1791 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.044 - 1s 1ms/sample - loss: 0.0210 - val_loss: 0.0144\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 120us/sample - loss: 0.0077 - val_loss: 0.0084\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 115us/sample - loss: 0.0070 - val_loss: 0.0089\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0069 - val_loss: 0.0090\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 136us/sample - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 121us/sample - loss: 0.0069 - val_loss: 0.0088\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0067 - val_loss: 0.0088\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 122us/sample - loss: 0.0068 - val_loss: 0.0086\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0068 - val_loss: 0.0088\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0069 - val_loss: 0.0100\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 120us/sample - loss: 0.0067 - val_loss: 0.0088\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0093\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0091\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0065 - val_loss: 0.0092\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 105us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 101us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 105us/sample - loss: 0.0064 - val_loss: 0.0098\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0065 - val_loss: 0.0100\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0067 - val_loss: 0.0118\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 106us/sample - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 112us/sample - loss: 0.0064 - val_loss: 0.0092\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 105us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0063 - val_loss: 0.0107\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0064 - val_loss: 0.0096\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 103us/sample - loss: 0.0063 - val_loss: 0.0088\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0062 - val_loss: 0.0091\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0065 - val_loss: 0.0096\n",
      "Model: \"sequential_256\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1792 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1793 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1794 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1795 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1796 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1797 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1798 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.067 - 1s 1ms/sample - loss: 0.0808 - val_loss: 0.0670\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.093 - 0s 120us/sample - loss: 0.0680 - val_loss: 0.0558\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.061 - 0s 124us/sample - loss: 0.0569 - val_loss: 0.0462\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.058 - 0s 124us/sample - loss: 0.0474 - val_loss: 0.0381\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.038 - 0s 126us/sample - loss: 0.0394 - val_loss: 0.0315\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.039 - 0s 132us/sample - loss: 0.0327 - val_loss: 0.0259\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.031 - 0s 124us/sample - loss: 0.0271 - val_loss: 0.0214\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.025 - 0s 124us/sample - loss: 0.0225 - val_loss: 0.0177\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - 0s 124us/sample - loss: 0.0187 - val_loss: 0.0149\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.025 - 0s 130us/sample - loss: 0.0158 - val_loss: 0.0127\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.013 - 0s 147us/sample - loss: 0.0135 - val_loss: 0.0110\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.016 - 0s 128us/sample - loss: 0.0117 - val_loss: 0.0098\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0103 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 130us/sample - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 131us/sample - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.007 - 0s 155us/sample - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.007 - 0s 153us/sample - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 136us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 125us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 121us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 100us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 101us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 108us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 108us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 100us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 106us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 110us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 112us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_257\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1799 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1800 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1801 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1802 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1803 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1804 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1805 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 5s - loss: 0.043 - 1s 1ms/sample - loss: 0.0156 - val_loss: 0.0110\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 134us/sample - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0076 - val_loss: 0.0108\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0072 - val_loss: 0.0083\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0069 - val_loss: 0.0099\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0069 - val_loss: 0.0099\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 106us/sample - loss: 0.0071 - val_loss: 0.0092\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 111us/sample - loss: 0.0069 - val_loss: 0.0086\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0065 - val_loss: 0.0085\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 122us/sample - loss: 0.0065 - val_loss: 0.0103\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 142us/sample - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0064 - val_loss: 0.0090\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0064 - val_loss: 0.0102\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0066 - val_loss: 0.0091\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 127us/sample - loss: 0.0066 - val_loss: 0.0090\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 124us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 129us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0064 - val_loss: 0.0092\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0065 - val_loss: 0.0100\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 132us/sample - loss: 0.0065 - val_loss: 0.0095\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 127us/sample - loss: 0.0063 - val_loss: 0.0097\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 116us/sample - loss: 0.0064 - val_loss: 0.0091\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 128us/sample - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 138us/sample - loss: 0.0063 - val_loss: 0.0089\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 123us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 149us/sample - loss: 0.0063 - val_loss: 0.0088\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 130us/sample - loss: 0.0062 - val_loss: 0.0094\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0064 - val_loss: 0.0089\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0063 - val_loss: 0.0093\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 127us/sample - loss: 0.0062 - val_loss: 0.0087\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0062 - val_loss: 0.0106\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 126us/sample - loss: 0.0061 - val_loss: 0.0086\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0061 - val_loss: 0.0089\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0062 - val_loss: 0.0088\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0061 - val_loss: 0.0086\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0061 - val_loss: 0.0097\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0063 - val_loss: 0.0091\n",
      "Epoch 59/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 107us/sample - loss: 0.0062 - val_loss: 0.0092\n",
      "Epoch 60/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 106us/sample - loss: 0.0063 - val_loss: 0.0094\n",
      "Epoch 61/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0062 - val_loss: 0.0089\n",
      "Epoch 62/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 119us/sample - loss: 0.0061 - val_loss: 0.0092\n",
      "Model: \"sequential_258\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1806 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1807 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1808 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1809 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1810 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1811 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1812 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.024 - 1s 1ms/sample - loss: 0.0125 - val_loss: 0.0096\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0074 - val_loss: 0.0103\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 124us/sample - loss: 0.0071 - val_loss: 0.0099\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 118us/sample - loss: 0.0070 - val_loss: 0.0106\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 121us/sample - loss: 0.0070 - val_loss: 0.0117\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0068 - val_loss: 0.0103\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0067 - val_loss: 0.0101\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0096\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0102\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0065 - val_loss: 0.0094\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 107us/sample - loss: 0.0067 - val_loss: 0.0105\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 112us/sample - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0064 - val_loss: 0.0094\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0064 - val_loss: 0.0093\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 112us/sample - loss: 0.0064 - val_loss: 0.0093\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 110us/sample - loss: 0.0064 - val_loss: 0.0098\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0063 - val_loss: 0.0091\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 112us/sample - loss: 0.0062 - val_loss: 0.0100\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 109us/sample - loss: 0.0063 - val_loss: 0.0105\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 109us/sample - loss: 0.0063 - val_loss: 0.0090\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0064 - val_loss: 0.0090\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 110us/sample - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 121us/sample - loss: 0.0063 - val_loss: 0.0090\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0060 - val_loss: 0.0091\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0060 - val_loss: 0.0093\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 159us/sample - loss: 0.0060 - val_loss: 0.0103\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0061 - val_loss: 0.0089\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0059 - val_loss: 0.0094\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0059 - val_loss: 0.0093\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 152us/sample - loss: 0.0059 - val_loss: 0.0095\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 152us/sample - loss: 0.0059 - val_loss: 0.0096\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0059 - val_loss: 0.0103\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 140us/sample - loss: 0.0059 - val_loss: 0.0090\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0057 - val_loss: 0.0100\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.005 - 0s 151us/sample - loss: 0.0058 - val_loss: 0.0100\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.005 - 0s 151us/sample - loss: 0.0057 - val_loss: 0.0100\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 155us/sample - loss: 0.0058 - val_loss: 0.0104\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 154us/sample - loss: 0.0056 - val_loss: 0.0093\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 148us/sample - loss: 0.0055 - val_loss: 0.0115\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 126us/sample - loss: 0.0058 - val_loss: 0.0100\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0055 - val_loss: 0.0090\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0056 - val_loss: 0.0094\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0059 - val_loss: 0.0101\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0057 - val_loss: 0.0107\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0056 - val_loss: 0.0105\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0053 - val_loss: 0.0085\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 121us/sample - loss: 0.0055 - val_loss: 0.0091\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0053 - val_loss: 0.0097\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.005 - 0s 144us/sample - loss: 0.0054 - val_loss: 0.0100\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0053 - val_loss: 0.0095\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 124us/sample - loss: 0.0052 - val_loss: 0.0095\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 144us/sample - loss: 0.0052 - val_loss: 0.0097\n",
      "Model: \"sequential_259\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1813 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1814 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1815 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1816 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1817 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1818 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1819 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.082 - 1s 1ms/sample - loss: 0.0810 - val_loss: 0.0671\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.070 - 0s 129us/sample - loss: 0.0682 - val_loss: 0.0560\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.077 - 0s 128us/sample - loss: 0.0571 - val_loss: 0.0463\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.040 - 0s 126us/sample - loss: 0.0475 - val_loss: 0.0383\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.043 - 0s 117us/sample - loss: 0.0396 - val_loss: 0.0315\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.041 - 0s 109us/sample - loss: 0.0328 - val_loss: 0.0260\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.021 - 0s 107us/sample - loss: 0.0272 - val_loss: 0.0215\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.029 - 0s 109us/sample - loss: 0.0226 - val_loss: 0.0179\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.020 - 0s 113us/sample - loss: 0.0189 - val_loss: 0.0150\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 113us/sample - loss: 0.0159 - val_loss: 0.0127\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.014 - 0s 103us/sample - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.016 - 0s 109us/sample - loss: 0.0117 - val_loss: 0.0098\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 113us/sample - loss: 0.0103 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 117us/sample - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 124us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 127us/sample - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 109us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 129us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 123us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 142us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 136us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 158us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 134us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 118us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 108us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 101us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 103us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_260\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1820 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1821 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1822 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1823 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1824 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1825 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1826 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.118 - ETA: 0s - loss: 0.080 - 1s 1ms/sample - loss: 0.0807 - val_loss: 0.0623\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.053 - 0s 132us/sample - loss: 0.0300 - val_loss: 0.0111\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.007 - 0s 153us/sample - loss: 0.0078 - val_loss: 0.0085\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 127us/sample - loss: 0.0069 - val_loss: 0.0084\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0068 - val_loss: 0.0086\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0082\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0065 - val_loss: 0.0080\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0078\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0079\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 113us/sample - loss: 0.0065 - val_loss: 0.0077\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 107us/sample - loss: 0.0064 - val_loss: 0.0078\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 107us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 108us/sample - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0064 - val_loss: 0.0087\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0063 - val_loss: 0.0078\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 105us/sample - loss: 0.0063 - val_loss: 0.0080\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0062 - val_loss: 0.0079\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0063 - val_loss: 0.0077\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0065 - val_loss: 0.0077\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 121us/sample - loss: 0.0063 - val_loss: 0.0079\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0062 - val_loss: 0.0077\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 117us/sample - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0061 - val_loss: 0.0084\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 128us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0061 - val_loss: 0.0080\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0061 - val_loss: 0.0085\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 128us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 121us/sample - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0061 - val_loss: 0.0080\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0061 - val_loss: 0.0078\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0061 - val_loss: 0.0081\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 118us/sample - loss: 0.0060 - val_loss: 0.0082\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 143us/sample - loss: 0.0060 - val_loss: 0.0087\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 104us/sample - loss: 0.0061 - val_loss: 0.0085\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 106us/sample - loss: 0.0061 - val_loss: 0.0081\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 105us/sample - loss: 0.0059 - val_loss: 0.0081\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0059 - val_loss: 0.0083\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0059 - val_loss: 0.0082\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 111us/sample - loss: 0.0060 - val_loss: 0.0080\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0059 - val_loss: 0.0082\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0059 - val_loss: 0.0079\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0060 - val_loss: 0.0080\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0059 - val_loss: 0.0082\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 118us/sample - loss: 0.0059 - val_loss: 0.0082\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 104us/sample - loss: 0.0057 - val_loss: 0.0079\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 105us/sample - loss: 0.0060 - val_loss: 0.0078\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 107us/sample - loss: 0.0058 - val_loss: 0.0085\n",
      "Model: \"sequential_261\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1827 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1828 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1829 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1830 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1831 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1832 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1833 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.095 - 1s 1ms/sample - loss: 0.0817 - val_loss: 0.0671\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.091 - 0s 133us/sample - loss: 0.0682 - val_loss: 0.0560\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.052 - 0s 128us/sample - loss: 0.0571 - val_loss: 0.0465\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.049 - 0s 134us/sample - loss: 0.0477 - val_loss: 0.0384\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.040 - 0s 161us/sample - loss: 0.0397 - val_loss: 0.0317\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.030 - ETA: 0s - loss: 0.033 - 0s 161us/sample - loss: 0.0329 - val_loss: 0.0262\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.027 - 0s 150us/sample - loss: 0.0274 - val_loss: 0.0216\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.023 - 0s 171us/sample - loss: 0.0228 - val_loss: 0.0180\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - 0s 128us/sample - loss: 0.0190 - val_loss: 0.0151\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.020 - 0s 132us/sample - loss: 0.0160 - val_loss: 0.0129\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 130us/sample - loss: 0.0136 - val_loss: 0.0112\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 0s 148us/sample - loss: 0.0118 - val_loss: 0.0099\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 132us/sample - loss: 0.0104 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 119us/sample - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 125us/sample - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 131us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 153us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 153us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 132us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 140us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 112us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 112us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 106us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 175us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 123us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 118us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 114us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_262\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1834 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1835 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1836 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1837 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1838 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1839 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1840 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.029 - 1s 1ms/sample - loss: 0.0134 - val_loss: 0.0116\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 117us/sample - loss: 0.0083 - val_loss: 0.0110\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 117us/sample - loss: 0.0076 - val_loss: 0.0102\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0071 - val_loss: 0.0097\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0071 - val_loss: 0.0094\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 111us/sample - loss: 0.0068 - val_loss: 0.0099\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0067 - val_loss: 0.0105\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0065 - val_loss: 0.0098\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0065 - val_loss: 0.0107\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 106us/sample - loss: 0.0067 - val_loss: 0.0101\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 102us/sample - loss: 0.0065 - val_loss: 0.0097\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 130us/sample - loss: 0.0065 - val_loss: 0.0090\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 132us/sample - loss: 0.0064 - val_loss: 0.0092\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0063 - val_loss: 0.0102\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0064 - val_loss: 0.0097\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0063 - val_loss: 0.0107\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0063 - val_loss: 0.0092\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0061 - val_loss: 0.0101\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 121us/sample - loss: 0.0059 - val_loss: 0.0092\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 119us/sample - loss: 0.0060 - val_loss: 0.0092\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0061 - val_loss: 0.0093\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0061 - val_loss: 0.0096\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 120us/sample - loss: 0.0059 - val_loss: 0.0094\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0061 - val_loss: 0.0093\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 118us/sample - loss: 0.0060 - val_loss: 0.0097\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0060 - val_loss: 0.0101\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0057 - val_loss: 0.0104\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0058 - val_loss: 0.0105\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0057 - val_loss: 0.0096\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 129us/sample - loss: 0.0058 - val_loss: 0.0101\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0058 - val_loss: 0.0103\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0056 - val_loss: 0.0100\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0057 - val_loss: 0.0109\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0057 - val_loss: 0.0105\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 103us/sample - loss: 0.0055 - val_loss: 0.0111\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0056 - val_loss: 0.0099\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 111us/sample - loss: 0.0059 - val_loss: 0.0094\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0059 - val_loss: 0.0113\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 105us/sample - loss: 0.0056 - val_loss: 0.0105\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0056 - val_loss: 0.0099\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 115us/sample - loss: 0.0054 - val_loss: 0.0098\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0055 - val_loss: 0.0103\n",
      "Model: \"sequential_263\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1841 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1842 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1843 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1844 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1845 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1846 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1847 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.075 - 1s 1ms/sample - loss: 0.0809 - val_loss: 0.0670\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.072 - 0s 126us/sample - loss: 0.0681 - val_loss: 0.0559\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.053 - 0s 126us/sample - loss: 0.0571 - val_loss: 0.0464\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.061 - 0s 124us/sample - loss: 0.0476 - val_loss: 0.0383\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.031 - 0s 128us/sample - loss: 0.0395 - val_loss: 0.0315\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.045 - 0s 128us/sample - loss: 0.0328 - val_loss: 0.0259\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.033 - 0s 130us/sample - loss: 0.0272 - val_loss: 0.0215\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.018 - 0s 128us/sample - loss: 0.0226 - val_loss: 0.0178\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - 0s 118us/sample - loss: 0.0188 - val_loss: 0.0150\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 106us/sample - loss: 0.0159 - val_loss: 0.0128\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 108us/sample - loss: 0.0135 - val_loss: 0.0111\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 113us/sample - loss: 0.0117 - val_loss: 0.0098\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 118us/sample - loss: 0.0103 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 122us/sample - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 120us/sample - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 122us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 114us/sample - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 123us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 108us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 153us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 121us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 114us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 127us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_264\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1848 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1849 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1850 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1851 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1852 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1853 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1854 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.055 - 2s 4ms/sample - loss: 0.0330 - val_loss: 0.0080\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 135us/sample - loss: 0.0081 - val_loss: 0.0078\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 126us/sample - loss: 0.0073 - val_loss: 0.0083\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.007 - 0s 144us/sample - loss: 0.0072 - val_loss: 0.0083\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0071 - val_loss: 0.0079\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.007 - 0s 144us/sample - loss: 0.0070 - val_loss: 0.0078\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0068 - val_loss: 0.0080\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0069 - val_loss: 0.0080\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 132us/sample - loss: 0.0067 - val_loss: 0.0084\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 127us/sample - loss: 0.0068 - val_loss: 0.0079\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0068 - val_loss: 0.0080\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 128us/sample - loss: 0.0067 - val_loss: 0.0079\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0066 - val_loss: 0.0082\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0066 - val_loss: 0.0084\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0065 - val_loss: 0.0085\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 131us/sample - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0064 - val_loss: 0.0078\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 127us/sample - loss: 0.0065 - val_loss: 0.0079\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 110us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 112us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 131us/sample - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0064 - val_loss: 0.0081\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 173us/sample - loss: 0.0066 - val_loss: 0.0081\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 177us/sample - loss: 0.0064 - val_loss: 0.0081\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0066 - val_loss: 0.0081\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.005 - 0s 175us/sample - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.006 - 0s 171us/sample - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 124us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 186us/sample - loss: 0.0062 - val_loss: 0.0080\n",
      "Model: \"sequential_265\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1855 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1856 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1857 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1858 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1859 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1860 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1861 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.008 - 1s 1ms/sample - loss: 0.0089 - val_loss: 0.0119\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 132us/sample - loss: 0.0080 - val_loss: 0.0093\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0074 - val_loss: 0.0106\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 135us/sample - loss: 0.0075 - val_loss: 0.0088\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.007 - 0s 162us/sample - loss: 0.0072 - val_loss: 0.0082\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.007 - 0s 152us/sample - loss: 0.0070 - val_loss: 0.0086\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0070 - val_loss: 0.0088\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0069 - val_loss: 0.0083\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 130us/sample - loss: 0.0067 - val_loss: 0.0082\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0084\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 122us/sample - loss: 0.0065 - val_loss: 0.0099\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0067 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0079\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 128us/sample - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0065 - val_loss: 0.0079\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0069 - val_loss: 0.0080\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0078\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 112us/sample - loss: 0.0063 - val_loss: 0.0093\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0082\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0063 - val_loss: 0.0086\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 118us/sample - loss: 0.0062 - val_loss: 0.0087\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 127us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0062 - val_loss: 0.0095\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0061 - val_loss: 0.0094\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0065 - val_loss: 0.0081\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0061 - val_loss: 0.0087\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0061 - val_loss: 0.0081\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0061 - val_loss: 0.0084\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 134us/sample - loss: 0.0061 - val_loss: 0.0081\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 125us/sample - loss: 0.0059 - val_loss: 0.0087\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 132us/sample - loss: 0.0061 - val_loss: 0.0084\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 124us/sample - loss: 0.0060 - val_loss: 0.0087\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0061 - val_loss: 0.0088\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 130us/sample - loss: 0.0059 - val_loss: 0.0094\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0059 - val_loss: 0.0091\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 124us/sample - loss: 0.0058 - val_loss: 0.0093\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0059 - val_loss: 0.0088\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 114us/sample - loss: 0.0058 - val_loss: 0.0087\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 112us/sample - loss: 0.0061 - val_loss: 0.0086\n",
      "Model: \"sequential_266\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1862 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1863 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1864 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1865 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1866 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1867 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1868 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.079 - 0s 981us/sample - loss: 0.0584 - val_loss: 0.0270\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.037 - 0s 115us/sample - loss: 0.0156 - val_loss: 0.0098\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.007 - 0s 148us/sample - loss: 0.0080 - val_loss: 0.0084\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0074 - val_loss: 0.0085\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0072 - val_loss: 0.0086\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 126us/sample - loss: 0.0070 - val_loss: 0.0085\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 132us/sample - loss: 0.0070 - val_loss: 0.0084\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0070 - val_loss: 0.0092\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 134us/sample - loss: 0.0069 - val_loss: 0.0081\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 128us/sample - loss: 0.0069 - val_loss: 0.0081\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 134us/sample - loss: 0.0069 - val_loss: 0.0096\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 128us/sample - loss: 0.0067 - val_loss: 0.0084\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 130us/sample - loss: 0.0067 - val_loss: 0.0086\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0067 - val_loss: 0.0090\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0089\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 134us/sample - loss: 0.0066 - val_loss: 0.0089\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0065 - val_loss: 0.0082\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0065 - val_loss: 0.0093\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0067 - val_loss: 0.0096\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0083\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0067 - val_loss: 0.0084\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 118us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0065 - val_loss: 0.0088\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0065 - val_loss: 0.0088\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 134us/sample - loss: 0.0065 - val_loss: 0.0085\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0064 - val_loss: 0.0092\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0067 - val_loss: 0.0081\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 131us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 214us/sample - loss: 0.0065 - val_loss: 0.0088\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 177us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 169us/sample - loss: 0.0064 - val_loss: 0.0087\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0065 - val_loss: 0.0088\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 183us/sample - loss: 0.0064 - val_loss: 0.0089\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 177us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 190us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 181us/sample - loss: 0.0063 - val_loss: 0.0092\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 171us/sample - loss: 0.0063 - val_loss: 0.0090\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 169us/sample - loss: 0.0063 - val_loss: 0.0088\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 169us/sample - loss: 0.0063 - val_loss: 0.0093\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 154us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 168us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.006 - 0s 177us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 119us/sample - loss: 0.0063 - val_loss: 0.0092\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0063 - val_loss: 0.0089\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 121us/sample - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0063 - val_loss: 0.0090\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 169us/sample - loss: 0.0063 - val_loss: 0.0086\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0062 - val_loss: 0.0092\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0063 - val_loss: 0.0090\n",
      "Model: \"sequential_267\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1869 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1870 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1871 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1872 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1873 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1874 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1875 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.053 - 0s 1ms/sample - loss: 0.0179 - val_loss: 0.0157\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 122us/sample - loss: 0.0079 - val_loss: 0.0136\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0074 - val_loss: 0.0127\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 121us/sample - loss: 0.0072 - val_loss: 0.0111\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0071 - val_loss: 0.0095\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0069 - val_loss: 0.0097\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0069 - val_loss: 0.0099\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0068 - val_loss: 0.0107\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 121us/sample - loss: 0.0067 - val_loss: 0.0087\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0068 - val_loss: 0.0082\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0067 - val_loss: 0.0099\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 129us/sample - loss: 0.0066 - val_loss: 0.0092\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0094\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 132us/sample - loss: 0.0065 - val_loss: 0.0088\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 151us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0065 - val_loss: 0.0094\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0065 - val_loss: 0.0095\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 134us/sample - loss: 0.0065 - val_loss: 0.0092\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 154us/sample - loss: 0.0064 - val_loss: 0.0090\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 166us/sample - loss: 0.0064 - val_loss: 0.0098\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0065 - val_loss: 0.0094\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0093\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 128us/sample - loss: 0.0063 - val_loss: 0.0086\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 130us/sample - loss: 0.0064 - val_loss: 0.0081\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 125us/sample - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 125us/sample - loss: 0.0063 - val_loss: 0.0093\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 119us/sample - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 171us/sample - loss: 0.0062 - val_loss: 0.0078\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0063 - val_loss: 0.0089\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0062 - val_loss: 0.0100\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 133us/sample - loss: 0.0061 - val_loss: 0.0087\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0061 - val_loss: 0.0079\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 181us/sample - loss: 0.0063 - val_loss: 0.0086\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 131us/sample - loss: 0.0060 - val_loss: 0.0081\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 127us/sample - loss: 0.0061 - val_loss: 0.0089\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0061 - val_loss: 0.0106\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0061 - val_loss: 0.0085\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0060 - val_loss: 0.0081\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.005 - 0s 147us/sample - loss: 0.0060 - val_loss: 0.0089\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 108us/sample - loss: 0.0060 - val_loss: 0.0081\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0060 - val_loss: 0.0083\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0060 - val_loss: 0.0085\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 146us/sample - loss: 0.0059 - val_loss: 0.0082\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0059 - val_loss: 0.0081\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0060 - val_loss: 0.0089\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0060 - val_loss: 0.0100\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0059 - val_loss: 0.0088\n",
      "Epoch 59/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 126us/sample - loss: 0.0058 - val_loss: 0.0079\n",
      "Epoch 60/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 134us/sample - loss: 0.0060 - val_loss: 0.0082\n",
      "Epoch 61/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 151us/sample - loss: 0.0061 - val_loss: 0.0088\n",
      "Epoch 62/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 63/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0058 - val_loss: 0.0082\n",
      "Epoch 64/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0058 - val_loss: 0.0080\n",
      "Epoch 65/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0058 - val_loss: 0.0083\n",
      "Epoch 66/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 134us/sample - loss: 0.0059 - val_loss: 0.0091\n",
      "Model: \"sequential_268\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1876 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1877 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1878 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1879 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1880 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1881 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1882 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.024 - 1s 1ms/sample - loss: 0.0118 - val_loss: 0.0099\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 130us/sample - loss: 0.0078 - val_loss: 0.0095\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0076 - val_loss: 0.0101\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 131us/sample - loss: 0.0076 - val_loss: 0.0091\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.007 - 0s 159us/sample - loss: 0.0071 - val_loss: 0.0089\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0068 - val_loss: 0.0083\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0070 - val_loss: 0.0085\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.007 - 0s 167us/sample - loss: 0.0069 - val_loss: 0.0080\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 131us/sample - loss: 0.0067 - val_loss: 0.0088\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0066 - val_loss: 0.0095\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0065 - val_loss: 0.0090\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0068 - val_loss: 0.0106\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0066 - val_loss: 0.0101\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.007 - 0s 157us/sample - loss: 0.0071 - val_loss: 0.0083\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0064 - val_loss: 0.0093\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0065 - val_loss: 0.0081\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0063 - val_loss: 0.0092\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0063 - val_loss: 0.0101\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 171us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.006 - 0s 247us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 181us/sample - loss: 0.0066 - val_loss: 0.0086\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 162us/sample - loss: 0.0065 - val_loss: 0.0082\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0061 - val_loss: 0.0086\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0062 - val_loss: 0.0095\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0061 - val_loss: 0.0086\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0061 - val_loss: 0.0101\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0061 - val_loss: 0.0091\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 132us/sample - loss: 0.0061 - val_loss: 0.0091\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0060 - val_loss: 0.0084\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 143us/sample - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0061 - val_loss: 0.0089\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0063 - val_loss: 0.0103\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 160us/sample - loss: 0.0063 - val_loss: 0.0086\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 171us/sample - loss: 0.0061 - val_loss: 0.0090\n",
      "Model: \"sequential_269\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1883 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1884 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1885 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1886 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1887 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1888 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1889 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.139 - 1s 1ms/sample - loss: 0.0892 - val_loss: 0.0673\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.078 - 0s 113us/sample - loss: 0.0686 - val_loss: 0.0564\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.066 - 0s 113us/sample - loss: 0.0577 - val_loss: 0.0470\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.056 - 0s 117us/sample - loss: 0.0483 - val_loss: 0.0389\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.038 - 0s 113us/sample - loss: 0.0403 - val_loss: 0.0322\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.044 - 0s 122us/sample - loss: 0.0335 - val_loss: 0.0266\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.022 - 0s 113us/sample - loss: 0.0278 - val_loss: 0.0220\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.027 - 0s 123us/sample - loss: 0.0232 - val_loss: 0.0183\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.022 - 0s 118us/sample - loss: 0.0193 - val_loss: 0.0154\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.014 - 0s 128us/sample - loss: 0.0163 - val_loss: 0.0131\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - 0s 111us/sample - loss: 0.0139 - val_loss: 0.0113\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 129us/sample - loss: 0.0119 - val_loss: 0.0100\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 135us/sample - loss: 0.0105 - val_loss: 0.0090\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.009 - 0s 163us/sample - loss: 0.0094 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.008 - 0s 171us/sample - loss: 0.0086 - val_loss: 0.0079\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 0s 171us/sample - loss: 0.0080 - val_loss: 0.0076\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.007 - 0s 223us/sample - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.007 - 0s 152us/sample - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 130us/sample - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 153us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 116us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 127us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 158us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 169us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 173us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_270\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1890 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1891 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1892 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1893 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1894 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1895 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1896 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.068 - 1s 1ms/sample - loss: 0.0362 - val_loss: 0.0120\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 112us/sample - loss: 0.0095 - val_loss: 0.0095\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 107us/sample - loss: 0.0077 - val_loss: 0.0112\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 130us/sample - loss: 0.0073 - val_loss: 0.0104\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 132us/sample - loss: 0.0072 - val_loss: 0.0101\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.007 - 0s 144us/sample - loss: 0.0071 - val_loss: 0.0105\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 131us/sample - loss: 0.0070 - val_loss: 0.0098\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.007 - 0s 146us/sample - loss: 0.0069 - val_loss: 0.0101\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 129us/sample - loss: 0.0069 - val_loss: 0.0095\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 127us/sample - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 134us/sample - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 132us/sample - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0067 - val_loss: 0.0106\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0067 - val_loss: 0.0097\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 165us/sample - loss: 0.0067 - val_loss: 0.0094\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 153us/sample - loss: 0.0067 - val_loss: 0.0089\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 192us/sample - loss: 0.0068 - val_loss: 0.0100\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 129us/sample - loss: 0.0066 - val_loss: 0.0097\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0068 - val_loss: 0.0087\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 120us/sample - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0092\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 116us/sample - loss: 0.0066 - val_loss: 0.0090\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.001 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0086\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0086\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 112us/sample - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0065 - val_loss: 0.0081\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 116us/sample - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 114us/sample - loss: 0.0066 - val_loss: 0.0083\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0066 - val_loss: 0.0104\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0064 - val_loss: 0.0087\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0064 - val_loss: 0.0096\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 124us/sample - loss: 0.0065 - val_loss: 0.0090\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 132us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0065 - val_loss: 0.0081\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0083\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 124us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0065 - val_loss: 0.0093\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0064 - val_loss: 0.0099\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 128us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 105us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0062 - val_loss: 0.0101\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 110us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 132us/sample - loss: 0.0063 - val_loss: 0.0079\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0064 - val_loss: 0.0081\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 107us/sample - loss: 0.0065 - val_loss: 0.0080\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0062 - val_loss: 0.0095\n",
      "Epoch 59/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 115us/sample - loss: 0.0062 - val_loss: 0.0088\n",
      "Epoch 60/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 61/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 107us/sample - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 62/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 63/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 64/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 65/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 66/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0062 - val_loss: 0.0092\n",
      "Epoch 67/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 68/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 124us/sample - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 69/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 70/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 128us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 71/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 121us/sample - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 72/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0062 - val_loss: 0.0087\n",
      "Epoch 73/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0062 - val_loss: 0.0091\n",
      "Epoch 74/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 132us/sample - loss: 0.0063 - val_loss: 0.0096\n",
      "Epoch 75/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 134us/sample - loss: 0.0064 - val_loss: 0.0091\n",
      "Epoch 76/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0062 - val_loss: 0.0090\n",
      "Epoch 77/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0064 - val_loss: 0.0095\n",
      "Epoch 78/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0061 - val_loss: 0.0090\n",
      "Epoch 79/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 80/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 107us/sample - loss: 0.0060 - val_loss: 0.0083\n",
      "Epoch 81/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 128us/sample - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 82/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 126us/sample - loss: 0.0061 - val_loss: 0.0088\n",
      "Epoch 83/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 124us/sample - loss: 0.0062 - val_loss: 0.0091\n",
      "Epoch 84/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 124us/sample - loss: 0.0060 - val_loss: 0.0086\n",
      "Epoch 85/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 134us/sample - loss: 0.0062 - val_loss: 0.0087\n",
      "Model: \"sequential_271\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1897 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1898 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1899 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1900 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1901 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1902 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1903 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.091 - 1s 1ms/sample - loss: 0.0799 - val_loss: 0.0632\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.086 - 0s 128us/sample - loss: 0.0578 - val_loss: 0.0352\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.044 - 0s 117us/sample - loss: 0.0208 - val_loss: 0.0118\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0082 - val_loss: 0.0086\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 116us/sample - loss: 0.0075 - val_loss: 0.0092\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0073 - val_loss: 0.0093\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0071 - val_loss: 0.0086\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0069 - val_loss: 0.0083\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0069 - val_loss: 0.0092\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0067 - val_loss: 0.0086\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0092\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0067 - val_loss: 0.0087\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 114us/sample - loss: 0.0066 - val_loss: 0.0086\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0082\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0091\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 147us/sample - loss: 0.0065 - val_loss: 0.0082\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0065 - val_loss: 0.0094\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 145us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 124us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0082\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0065 - val_loss: 0.0085\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 110us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 121us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0065 - val_loss: 0.0080\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 111us/sample - loss: 0.0065 - val_loss: 0.0096\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0064 - val_loss: 0.0081\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0065 - val_loss: 0.0081\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 111us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 107us/sample - loss: 0.0064 - val_loss: 0.0096\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 107us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0087\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 114us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 107us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 126us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0064 - val_loss: 0.0081\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 116us/sample - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 125us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 140us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 125us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 120us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 128us/sample - loss: 0.0065 - val_loss: 0.0082\n",
      "Epoch 59/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 60/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 61/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 62/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0063 - val_loss: 0.0088\n",
      "Epoch 63/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 128us/sample - loss: 0.0064 - val_loss: 0.0081\n",
      "Epoch 64/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0063 - val_loss: 0.0094\n",
      "Epoch 65/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 66/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 175us/sample - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 67/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0064 - val_loss: 0.0096\n",
      "Epoch 68/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 69/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0063 - val_loss: 0.0080\n",
      "Epoch 70/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 71/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 72/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 128us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 73/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0063 - val_loss: 0.0088\n",
      "Epoch 74/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 119us/sample - loss: 0.0063 - val_loss: 0.0092\n",
      "Epoch 75/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 118us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 76/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0063 - val_loss: 0.0086\n",
      "Epoch 77/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 117us/sample - loss: 0.0063 - val_loss: 0.0090\n",
      "Epoch 78/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 143us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 79/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0063 - val_loss: 0.0095\n",
      "Epoch 80/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 165us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 81/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0062 - val_loss: 0.0094\n",
      "Epoch 82/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 143us/sample - loss: 0.0063 - val_loss: 0.0090\n",
      "Epoch 83/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 136us/sample - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 84/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 85/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 86/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 123us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 87/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 129us/sample - loss: 0.0063 - val_loss: 0.0089\n",
      "Epoch 88/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 89/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0062 - val_loss: 0.0089\n",
      "Epoch 90/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0062 - val_loss: 0.0086\n",
      "Epoch 91/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 129us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 92/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 145us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 93/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0063 - val_loss: 0.0088\n",
      "Epoch 94/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0062 - val_loss: 0.0094\n",
      "Epoch 95/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 118us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 96/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 119us/sample - loss: 0.0062 - val_loss: 0.0093\n",
      "Model: \"sequential_272\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1904 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1905 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1906 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1907 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1908 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1909 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1910 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.127 - 1s 1ms/sample - loss: 0.0404 - val_loss: 0.0130\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 128us/sample - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0071 - val_loss: 0.0079\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 119us/sample - loss: 0.0069 - val_loss: 0.0082\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 122us/sample - loss: 0.0068 - val_loss: 0.0083\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0096\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0084\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 106us/sample - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 109us/sample - loss: 0.0065 - val_loss: 0.0090\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 108us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 110us/sample - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 105us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0090\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 116us/sample - loss: 0.0065 - val_loss: 0.0082\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 110us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0063 - val_loss: 0.0090\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 119us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 122us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 124us/sample - loss: 0.0064 - val_loss: 0.0078\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 121us/sample - loss: 0.0062 - val_loss: 0.0093\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 123us/sample - loss: 0.0063 - val_loss: 0.0080\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0062 - val_loss: 0.0087\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0062 - val_loss: 0.0089\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 169us/sample - loss: 0.0064 - val_loss: 0.0087\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 151us/sample - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 130us/sample - loss: 0.0062 - val_loss: 0.0100\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 111us/sample - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 105us/sample - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 110us/sample - loss: 0.0062 - val_loss: 0.0078\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 125us/sample - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0060 - val_loss: 0.0076\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 143us/sample - loss: 0.0061 - val_loss: 0.0078\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 130us/sample - loss: 0.0060 - val_loss: 0.0080\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0060 - val_loss: 0.0084\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0060 - val_loss: 0.0085\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0060 - val_loss: 0.0090\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 131us/sample - loss: 0.0061 - val_loss: 0.0095\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0060 - val_loss: 0.0082\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0060 - val_loss: 0.0080\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0059 - val_loss: 0.0086\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 152us/sample - loss: 0.0059 - val_loss: 0.0078\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0059 - val_loss: 0.0077\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0060 - val_loss: 0.0085\n",
      "Epoch 59/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 152us/sample - loss: 0.0059 - val_loss: 0.0075\n",
      "Epoch 60/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0060 - val_loss: 0.0076\n",
      "Epoch 61/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 153us/sample - loss: 0.0062 - val_loss: 0.0076\n",
      "Epoch 62/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0059 - val_loss: 0.0080\n",
      "Epoch 63/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 155us/sample - loss: 0.0058 - val_loss: 0.0078\n",
      "Epoch 64/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 152us/sample - loss: 0.0058 - val_loss: 0.0074\n",
      "Epoch 65/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.005 - 0s 146us/sample - loss: 0.0058 - val_loss: 0.0077\n",
      "Epoch 66/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 154us/sample - loss: 0.0058 - val_loss: 0.0082\n",
      "Epoch 67/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0060 - val_loss: 0.0091\n",
      "Epoch 68/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 144us/sample - loss: 0.0059 - val_loss: 0.0094\n",
      "Epoch 69/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 114us/sample - loss: 0.0060 - val_loss: 0.0080\n",
      "Epoch 70/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 105us/sample - loss: 0.0058 - val_loss: 0.0076\n",
      "Epoch 71/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 105us/sample - loss: 0.0059 - val_loss: 0.0077\n",
      "Epoch 72/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 103us/sample - loss: 0.0057 - val_loss: 0.0079\n",
      "Epoch 73/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0059 - val_loss: 0.0077\n",
      "Epoch 74/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0058 - val_loss: 0.0074\n",
      "Epoch 75/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0062 - val_loss: 0.0074\n",
      "Epoch 76/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0082\n",
      "Epoch 77/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 114us/sample - loss: 0.0061 - val_loss: 0.0089\n",
      "Epoch 78/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0061 - val_loss: 0.0086\n",
      "Epoch 79/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 108us/sample - loss: 0.0060 - val_loss: 0.0083\n",
      "Epoch 80/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0059 - val_loss: 0.0074\n",
      "Epoch 81/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0059 - val_loss: 0.0080\n",
      "Epoch 82/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 131us/sample - loss: 0.0058 - val_loss: 0.0076\n",
      "Epoch 83/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 152us/sample - loss: 0.0058 - val_loss: 0.0087\n",
      "Epoch 84/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0060 - val_loss: 0.0077\n",
      "Epoch 85/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0059 - val_loss: 0.0087\n",
      "Epoch 86/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 125us/sample - loss: 0.0058 - val_loss: 0.0081\n",
      "Epoch 87/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 124us/sample - loss: 0.0061 - val_loss: 0.0078\n",
      "Epoch 88/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 117us/sample - loss: 0.0059 - val_loss: 0.0083\n",
      "Epoch 89/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0058 - val_loss: 0.0081\n",
      "Epoch 90/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 202us/sample - loss: 0.0059 - val_loss: 0.0081\n",
      "Epoch 91/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0057 - val_loss: 0.0078\n",
      "Epoch 92/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0056 - val_loss: 0.0082\n",
      "Epoch 93/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 129us/sample - loss: 0.0057 - val_loss: 0.0083\n",
      "Epoch 94/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 130us/sample - loss: 0.0056 - val_loss: 0.0078\n",
      "Epoch 95/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 128us/sample - loss: 0.0056 - val_loss: 0.0086\n",
      "Epoch 96/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0056 - val_loss: 0.0084\n",
      "Epoch 97/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 122us/sample - loss: 0.0056 - val_loss: 0.0084\n",
      "Epoch 98/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0056 - val_loss: 0.0084\n",
      "Epoch 99/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 110us/sample - loss: 0.0056 - val_loss: 0.0089\n",
      "Epoch 100/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 118us/sample - loss: 0.0055 - val_loss: 0.0085\n",
      "Epoch 101/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0056 - val_loss: 0.0089\n",
      "Epoch 102/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 107us/sample - loss: 0.0055 - val_loss: 0.0097\n",
      "Epoch 103/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 126us/sample - loss: 0.0056 - val_loss: 0.0089\n",
      "Epoch 104/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 112us/sample - loss: 0.0058 - val_loss: 0.0089\n",
      "Model: \"sequential_273\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1911 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1912 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1913 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1914 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1915 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1916 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1917 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 5s - loss: 0.085 - 1s 1ms/sample - loss: 0.0626 - val_loss: 0.0352\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.042 - 0s 140us/sample - loss: 0.0198 - val_loss: 0.0100\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0082 - val_loss: 0.0083\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 109us/sample - loss: 0.0074 - val_loss: 0.0087\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0071 - val_loss: 0.0085\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0068 - val_loss: 0.0084\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0068 - val_loss: 0.0082\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 117us/sample - loss: 0.0067 - val_loss: 0.0085\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0067 - val_loss: 0.0080\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 147us/sample - loss: 0.0066 - val_loss: 0.0092\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 153us/sample - loss: 0.0067 - val_loss: 0.0084\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0068 - val_loss: 0.0084\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0081\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0086\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0096\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 154us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 168us/sample - loss: 0.0065 - val_loss: 0.0082\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 126us/sample - loss: 0.0065 - val_loss: 0.0102\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 129us/sample - loss: 0.0067 - val_loss: 0.0095\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 181us/sample - loss: 0.0064 - val_loss: 0.0090\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 151us/sample - loss: 0.0065 - val_loss: 0.0092\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0065 - val_loss: 0.0099\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0067 - val_loss: 0.0083\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0065 - val_loss: 0.0092\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0091\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0091\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0063 - val_loss: 0.0090\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 122us/sample - loss: 0.0064 - val_loss: 0.0089\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 119us/sample - loss: 0.0064 - val_loss: 0.0087\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 124us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Model: \"sequential_274\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1918 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1919 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1920 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1921 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1922 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1923 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1924 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.083 - 1s 1ms/sample - loss: 0.0691 - val_loss: 0.0349\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.037 - 0s 125us/sample - loss: 0.0196 - val_loss: 0.0093\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0072 - val_loss: 0.0087\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 119us/sample - loss: 0.0070 - val_loss: 0.0091\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0069 - val_loss: 0.0085\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 110us/sample - loss: 0.0069 - val_loss: 0.0082\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 119us/sample - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0068 - val_loss: 0.0090\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 116us/sample - loss: 0.0067 - val_loss: 0.0092\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0067 - val_loss: 0.0092\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0067 - val_loss: 0.0093\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0103\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0067 - val_loss: 0.0099\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 105us/sample - loss: 0.0068 - val_loss: 0.0082\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 119us/sample - loss: 0.0066 - val_loss: 0.0086\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0095\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 106us/sample - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0067 - val_loss: 0.0082\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 121us/sample - loss: 0.0066 - val_loss: 0.0083\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 132us/sample - loss: 0.0065 - val_loss: 0.0088\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 125us/sample - loss: 0.0065 - val_loss: 0.0085\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0065 - val_loss: 0.0085\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0089\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 132us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0064 - val_loss: 0.0092\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0065 - val_loss: 0.0094\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 158us/sample - loss: 0.0066 - val_loss: 0.0090\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 132us/sample - loss: 0.0065 - val_loss: 0.0093\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 156us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0065 - val_loss: 0.0083\n",
      "Model: \"sequential_275\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1925 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1926 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1927 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1928 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1929 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1930 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1931 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.096 - 1s 1ms/sample - loss: 0.0751 - val_loss: 0.0550\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.051 - 0s 152us/sample - loss: 0.0510 - val_loss: 0.0359\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.034 - ETA: 0s - loss: 0.035 - 0s 177us/sample - loss: 0.0332 - val_loss: 0.0220\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.018 - 0s 153us/sample - loss: 0.0189 - val_loss: 0.0109\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.009 - 0s 161us/sample - loss: 0.0093 - val_loss: 0.0074\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 156us/sample - loss: 0.0068 - val_loss: 0.0073\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0075\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 118us/sample - loss: 0.0066 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 108us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0067 - val_loss: 0.0075\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0066 - val_loss: 0.0075\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 151us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 165us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 127us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 132us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_276\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1932 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1933 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1934 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1935 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1936 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1937 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1938 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.071 - 1s 1ms/sample - loss: 0.0809 - val_loss: 0.0670\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.065 - 0s 121us/sample - loss: 0.0681 - val_loss: 0.0558\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.062 - 0s 126us/sample - loss: 0.0571 - val_loss: 0.0463\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.057 - 0s 136us/sample - loss: 0.0476 - val_loss: 0.0383\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.044 - 0s 130us/sample - loss: 0.0396 - val_loss: 0.0316\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.041 - ETA: 0s - loss: 0.032 - 0s 157us/sample - loss: 0.0328 - val_loss: 0.0259\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.023 - 0s 132us/sample - loss: 0.0271 - val_loss: 0.0214\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.023 - 0s 131us/sample - loss: 0.0225 - val_loss: 0.0178\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 132us/sample - loss: 0.0187 - val_loss: 0.0149\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.019 - 0s 128us/sample - loss: 0.0158 - val_loss: 0.0127\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.018 - 0s 126us/sample - loss: 0.0135 - val_loss: 0.0110\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 126us/sample - loss: 0.0116 - val_loss: 0.0098\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 130us/sample - loss: 0.0103 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 132us/sample - loss: 0.0092 - val_loss: 0.0082\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 131us/sample - loss: 0.0084 - val_loss: 0.0078\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0078 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 124us/sample - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.007 - 0s 148us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 131us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 149us/sample - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 132us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 131us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 129us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 112us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.007 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 132us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_277\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1939 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1940 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1941 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1942 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1943 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1944 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1945 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 5s - loss: 0.108 - 1s 1ms/sample - loss: 0.0837 - val_loss: 0.0671\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.071 - 0s 124us/sample - loss: 0.0683 - val_loss: 0.0560\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.058 - 0s 166us/sample - loss: 0.0573 - val_loss: 0.0465\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.047 - 0s 150us/sample - loss: 0.0479 - val_loss: 0.0385\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.054 - 0s 128us/sample - loss: 0.0398 - val_loss: 0.0317\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.036 - 0s 126us/sample - loss: 0.0330 - val_loss: 0.0262\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.038 - 0s 132us/sample - loss: 0.0274 - val_loss: 0.0216\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.028 - 0s 110us/sample - loss: 0.0228 - val_loss: 0.0181\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - 0s 111us/sample - loss: 0.0191 - val_loss: 0.0151\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.016 - 0s 122us/sample - loss: 0.0160 - val_loss: 0.0129\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.013 - 0s 150us/sample - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 132us/sample - loss: 0.0118 - val_loss: 0.0099\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - 0s 125us/sample - loss: 0.0104 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 128us/sample - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.008 - 0s 163us/sample - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 0s 163us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.007 - 0s 163us/sample - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.007 - 0s 151us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.007 - 0s 150us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 166us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 154us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 147us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 125us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_278\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1946 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1947 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1948 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1949 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1950 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1951 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1952 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.092 - 1s 1ms/sample - loss: 0.0615 - val_loss: 0.0275\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.028 - 0s 124us/sample - loss: 0.0170 - val_loss: 0.0116\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0090 - val_loss: 0.0094\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 132us/sample - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0079 - val_loss: 0.0098\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0076 - val_loss: 0.0094\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 109us/sample - loss: 0.0074 - val_loss: 0.0091\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 117us/sample - loss: 0.0072 - val_loss: 0.0096\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 128us/sample - loss: 0.0070 - val_loss: 0.0093\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 132us/sample - loss: 0.0070 - val_loss: 0.0092\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0069 - val_loss: 0.0094\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0069 - val_loss: 0.0086\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0067 - val_loss: 0.0095\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 162us/sample - loss: 0.0067 - val_loss: 0.0094\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0067 - val_loss: 0.0089\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0067 - val_loss: 0.0086\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0067 - val_loss: 0.0082\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 118us/sample - loss: 0.0068 - val_loss: 0.0085\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 128us/sample - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0067 - val_loss: 0.0092\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 118us/sample - loss: 0.0066 - val_loss: 0.0084\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0084\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0086\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0097\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0065 - val_loss: 0.0081\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 108us/sample - loss: 0.0065 - val_loss: 0.0099\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0067 - val_loss: 0.0085\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0069 - val_loss: 0.0093\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0090\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 106us/sample - loss: 0.0065 - val_loss: 0.0082\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 110us/sample - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 105us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 129us/sample - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0065 - val_loss: 0.0080\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 153us/sample - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 122us/sample - loss: 0.0064 - val_loss: 0.0098\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0067 - val_loss: 0.0102\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0065 - val_loss: 0.0080\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0084\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0064 - val_loss: 0.0081\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 127us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0065 - val_loss: 0.0081\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0064 - val_loss: 0.0077\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 134us/sample - loss: 0.0065 - val_loss: 0.0077\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 147us/sample - loss: 0.0068 - val_loss: 0.0077\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 132us/sample - loss: 0.0066 - val_loss: 0.0093\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0064 - val_loss: 0.0089\n",
      "Epoch 59/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 60/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0064 - val_loss: 0.0079\n",
      "Epoch 61/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 164us/sample - loss: 0.0064 - val_loss: 0.0077\n",
      "Epoch 62/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 63/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 64/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 185us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 65/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 66/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 129us/sample - loss: 0.0063 - val_loss: 0.0079\n",
      "Epoch 67/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 68/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0064 - val_loss: 0.0094\n",
      "Epoch 69/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0064 - val_loss: 0.0089\n",
      "Epoch 70/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.006 - 0s 204us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 71/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 72/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 73/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 74/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0063 - val_loss: 0.0090\n",
      "Epoch 75/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 76/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0063 - val_loss: 0.0076\n",
      "Epoch 77/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 168us/sample - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 78/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 79/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 147us/sample - loss: 0.0063 - val_loss: 0.0078\n",
      "Epoch 80/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 81/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0064 - val_loss: 0.0078\n",
      "Epoch 82/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 114us/sample - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 83/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0062 - val_loss: 0.0078\n",
      "Epoch 84/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 160us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 85/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0064 - val_loss: 0.0095\n",
      "Epoch 86/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 87/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0061 - val_loss: 0.0077\n",
      "Epoch 88/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 114us/sample - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 89/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 112us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 90/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 114us/sample - loss: 0.0062 - val_loss: 0.0077\n",
      "Epoch 91/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 92/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 117us/sample - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 93/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 94/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 95/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 122us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 96/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 97/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 98/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 99/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0062 - val_loss: 0.0088\n",
      "Epoch 100/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 101/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 165us/sample - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 102/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 153us/sample - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 103/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 104/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 105/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0061 - val_loss: 0.0081\n",
      "Epoch 106/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 173us/sample - loss: 0.0062 - val_loss: 0.0093\n",
      "Model: \"sequential_279\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1953 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1954 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1955 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1956 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1957 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1958 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1959 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.105 - 1s 2ms/sample - loss: 0.0819 - val_loss: 0.0671\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.069 - 0s 165us/sample - loss: 0.0683 - val_loss: 0.0560\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.047 - 0s 135us/sample - loss: 0.0572 - val_loss: 0.0465\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.048 - 0s 154us/sample - loss: 0.0477 - val_loss: 0.0384\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.036 - 0s 142us/sample - loss: 0.0396 - val_loss: 0.0316\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.032 - 0s 156us/sample - loss: 0.0329 - val_loss: 0.0261\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.027 - 0s 158us/sample - loss: 0.0273 - val_loss: 0.0216\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.022 - 0s 155us/sample - loss: 0.0227 - val_loss: 0.0179\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.018 - 0s 150us/sample - loss: 0.0189 - val_loss: 0.0150\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 128us/sample - loss: 0.0159 - val_loss: 0.0128\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.016 - 0s 115us/sample - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 110us/sample - loss: 0.0117 - val_loss: 0.0098\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0103 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 109us/sample - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 117us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 115us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 108us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 106us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 132us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 132us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 145us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 142us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 123us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 138us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_280\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1960 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1961 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1962 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1963 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1964 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1965 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1966 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.059 - 1s 1ms/sample - loss: 0.0249 - val_loss: 0.0155\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 130us/sample - loss: 0.0098 - val_loss: 0.0094\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 110us/sample - loss: 0.0076 - val_loss: 0.0084\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0070 - val_loss: 0.0091\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0069 - val_loss: 0.0094\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0068 - val_loss: 0.0085\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0071 - val_loss: 0.0083\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 106us/sample - loss: 0.0067 - val_loss: 0.0096\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0069 - val_loss: 0.0105\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 110us/sample - loss: 0.0067 - val_loss: 0.0087\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 104us/sample - loss: 0.0066 - val_loss: 0.0091\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 118us/sample - loss: 0.0065 - val_loss: 0.0100\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 111us/sample - loss: 0.0070 - val_loss: 0.0103\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0068 - val_loss: 0.0088\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0065 - val_loss: 0.0088\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 151us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0063 - val_loss: 0.0090\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 117us/sample - loss: 0.0063 - val_loss: 0.0095\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 117us/sample - loss: 0.0064 - val_loss: 0.0093\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 128us/sample - loss: 0.0063 - val_loss: 0.0095\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0063 - val_loss: 0.0091\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 120us/sample - loss: 0.0063 - val_loss: 0.0091\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 127us/sample - loss: 0.0062 - val_loss: 0.0088\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0063 - val_loss: 0.0089\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 125us/sample - loss: 0.0064 - val_loss: 0.0078\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0062 - val_loss: 0.0088\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0062 - val_loss: 0.0087\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 106us/sample - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 103us/sample - loss: 0.0061 - val_loss: 0.0089\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 107us/sample - loss: 0.0061 - val_loss: 0.0092\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0061 - val_loss: 0.0085\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 101us/sample - loss: 0.0063 - val_loss: 0.0097\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0061 - val_loss: 0.0084\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0063 - val_loss: 0.0094\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0062 - val_loss: 0.0096\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0062 - val_loss: 0.0096\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 116us/sample - loss: 0.0061 - val_loss: 0.0085\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 103us/sample - loss: 0.0060 - val_loss: 0.0083\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0061 - val_loss: 0.0078\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0060 - val_loss: 0.0081\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0060 - val_loss: 0.0083\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0060 - val_loss: 0.0092\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0061 - val_loss: 0.0094\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0060 - val_loss: 0.0088\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0059 - val_loss: 0.0081\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 125us/sample - loss: 0.0059 - val_loss: 0.0087\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 125us/sample - loss: 0.0059 - val_loss: 0.0091\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 122us/sample - loss: 0.0059 - val_loss: 0.0095\n",
      "Epoch 59/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 127us/sample - loss: 0.0059 - val_loss: 0.0095\n",
      "Epoch 60/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 118us/sample - loss: 0.0059 - val_loss: 0.0082\n",
      "Epoch 61/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0059 - val_loss: 0.0082\n",
      "Epoch 62/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0059 - val_loss: 0.0077\n",
      "Epoch 63/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 64/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0058 - val_loss: 0.0103\n",
      "Epoch 65/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 107us/sample - loss: 0.0059 - val_loss: 0.0117\n",
      "Epoch 66/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0062 - val_loss: 0.0088\n",
      "Epoch 67/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 105us/sample - loss: 0.0058 - val_loss: 0.0089\n",
      "Epoch 68/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 103us/sample - loss: 0.0059 - val_loss: 0.0085\n",
      "Epoch 69/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0058 - val_loss: 0.0085\n",
      "Epoch 70/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 101us/sample - loss: 0.0057 - val_loss: 0.0088\n",
      "Epoch 71/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 105us/sample - loss: 0.0058 - val_loss: 0.0087\n",
      "Epoch 72/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 111us/sample - loss: 0.0057 - val_loss: 0.0093\n",
      "Epoch 73/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0057 - val_loss: 0.0080\n",
      "Epoch 74/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 106us/sample - loss: 0.0057 - val_loss: 0.0092\n",
      "Epoch 75/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0057 - val_loss: 0.0091\n",
      "Epoch 76/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0058 - val_loss: 0.0085\n",
      "Epoch 77/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 118us/sample - loss: 0.0056 - val_loss: 0.0093\n",
      "Epoch 78/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 112us/sample - loss: 0.0056 - val_loss: 0.0080\n",
      "Epoch 79/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0059 - val_loss: 0.0083\n",
      "Epoch 80/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 105us/sample - loss: 0.0057 - val_loss: 0.0079\n",
      "Epoch 81/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0056 - val_loss: 0.0090\n",
      "Epoch 82/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0056 - val_loss: 0.0090\n",
      "Epoch 83/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0055 - val_loss: 0.0091\n",
      "Epoch 84/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 126us/sample - loss: 0.0056 - val_loss: 0.0078\n",
      "Epoch 85/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0056 - val_loss: 0.0083\n",
      "Epoch 86/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 118us/sample - loss: 0.0057 - val_loss: 0.0093\n",
      "Epoch 87/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 126us/sample - loss: 0.0056 - val_loss: 0.0096\n",
      "Epoch 88/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0055 - val_loss: 0.0097\n",
      "Epoch 89/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0054 - val_loss: 0.0089\n",
      "Epoch 90/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0055 - val_loss: 0.0091\n",
      "Epoch 91/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0054 - val_loss: 0.0088\n",
      "Epoch 92/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.005 - 0s 161us/sample - loss: 0.0055 - val_loss: 0.0092\n",
      "Model: \"sequential_281\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1967 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1968 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1969 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1970 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1971 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1972 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1973 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.095 - ETA: 0s - loss: 0.082 - 1s 2ms/sample - loss: 0.0822 - val_loss: 0.0671\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 153us/sample - loss: 0.0683 - val_loss: 0.0560\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.057 - 0s 155us/sample - loss: 0.0572 - val_loss: 0.0465\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.058 - ETA: 0s - loss: 0.047 - 0s 206us/sample - loss: 0.0478 - val_loss: 0.0385\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.039 - 0s 177us/sample - loss: 0.0396 - val_loss: 0.0317\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.033 - 0s 175us/sample - loss: 0.0328 - val_loss: 0.0260\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.027 - 0s 202us/sample - loss: 0.0273 - val_loss: 0.0216\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.027 - ETA: 0s - loss: 0.022 - 0s 179us/sample - loss: 0.0227 - val_loss: 0.0179\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.019 - 0s 156us/sample - loss: 0.0190 - val_loss: 0.0151\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.016 - 0s 148us/sample - loss: 0.0160 - val_loss: 0.0129\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 130us/sample - loss: 0.0137 - val_loss: 0.0112\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - 0s 114us/sample - loss: 0.0118 - val_loss: 0.0099\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 114us/sample - loss: 0.0104 - val_loss: 0.0090\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 116us/sample - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 123us/sample - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 116us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 119us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 118us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 127us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 125us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 149us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 147us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 134us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 116us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 108us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0075\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_282\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1974 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1975 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1976 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1977 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1978 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1979 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1980 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.093 - 1s 1ms/sample - loss: 0.0810 - val_loss: 0.0671\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.068 - 0s 150us/sample - loss: 0.0682 - val_loss: 0.0559\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.062 - 0s 128us/sample - loss: 0.0572 - val_loss: 0.0464\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.047 - 0s 148us/sample - loss: 0.0476 - val_loss: 0.0384\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.039 - 0s 157us/sample - loss: 0.0396 - val_loss: 0.0316\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.033 - 0s 169us/sample - loss: 0.0329 - val_loss: 0.0261\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.026 - ETA: 0s - loss: 0.027 - 0s 150us/sample - loss: 0.0273 - val_loss: 0.0215\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.021 - 0s 107us/sample - loss: 0.0226 - val_loss: 0.0179\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.021 - 0s 113us/sample - loss: 0.0190 - val_loss: 0.0151\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - 0s 111us/sample - loss: 0.0160 - val_loss: 0.0128\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 124us/sample - loss: 0.0136 - val_loss: 0.0112\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 131us/sample - loss: 0.0119 - val_loss: 0.0099\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 128us/sample - loss: 0.0104 - val_loss: 0.0090\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 115us/sample - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 128us/sample - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.008 - 0s 157us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.007 - 0s 153us/sample - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 0s 161us/sample - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.007 - 0s 166us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 132us/sample - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 121us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 133us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 149us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 147us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 165us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 131us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 131us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 154us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 185us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_283\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1981 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1982 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1983 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1984 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1985 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1986 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1987 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.097 - 1s 1ms/sample - loss: 0.0813 - val_loss: 0.0671\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.060 - ETA: 0s - loss: 0.068 - 0s 152us/sample - loss: 0.0681 - val_loss: 0.0559\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.058 - 0s 165us/sample - loss: 0.0571 - val_loss: 0.0463\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.050 - 0s 132us/sample - loss: 0.0476 - val_loss: 0.0383\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.039 - 0s 152us/sample - loss: 0.0396 - val_loss: 0.0316\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.038 - 0s 130us/sample - loss: 0.0328 - val_loss: 0.0260\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.027 - 0s 132us/sample - loss: 0.0272 - val_loss: 0.0215\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.023 - 0s 128us/sample - loss: 0.0227 - val_loss: 0.0179\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.027 - 0s 135us/sample - loss: 0.0190 - val_loss: 0.0151\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.016 - 0s 163us/sample - loss: 0.0160 - val_loss: 0.0128\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 130us/sample - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - 0s 155us/sample - loss: 0.0117 - val_loss: 0.0098\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.010 - 0s 161us/sample - loss: 0.0103 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.009 - 0s 152us/sample - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 133us/sample - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 115us/sample - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.007 - 0s 142us/sample - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.007 - 0s 146us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 122us/sample - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 124us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 118us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 121us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 121us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 151us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 153us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 237us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_284\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1988 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1989 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1990 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1991 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1992 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1993 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_1994 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.108 - ETA: 1s - loss: 0.086 - ETA: 0s - loss: 0.070 - 1s 2ms/sample - loss: 0.0709 - val_loss: 0.0464\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.030 - 0s 150us/sample - loss: 0.0306 - val_loss: 0.0086\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0085 - val_loss: 0.0087\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0079 - val_loss: 0.0089\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 135us/sample - loss: 0.0073 - val_loss: 0.0082\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 118us/sample - loss: 0.0072 - val_loss: 0.0086\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0072 - val_loss: 0.0079\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 125us/sample - loss: 0.0070 - val_loss: 0.0082\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 128us/sample - loss: 0.0069 - val_loss: 0.0087\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0069 - val_loss: 0.0083\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0068 - val_loss: 0.0083\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0067 - val_loss: 0.0085\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0067 - val_loss: 0.0087\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 108us/sample - loss: 0.0069 - val_loss: 0.0082\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0067 - val_loss: 0.0086\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 102us/sample - loss: 0.0067 - val_loss: 0.0082\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 108us/sample - loss: 0.0067 - val_loss: 0.0080\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0067 - val_loss: 0.0083\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0097\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 117us/sample - loss: 0.0067 - val_loss: 0.0088\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 104us/sample - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0092\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0091\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0089\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 128us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 129us/sample - loss: 0.0066 - val_loss: 0.0084\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0096\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 125us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 122us/sample - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0065 - val_loss: 0.0097\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 121us/sample - loss: 0.0068 - val_loss: 0.0086\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 133us/sample - loss: 0.0065 - val_loss: 0.0090\n",
      "Model: \"sequential_285\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1995 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_1996 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_1997 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1998 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1999 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2000 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2001 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.049 - 1s 1ms/sample - loss: 0.0241 - val_loss: 0.0130\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 119us/sample - loss: 0.0093 - val_loss: 0.0125\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 123us/sample - loss: 0.0078 - val_loss: 0.0099\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0073 - val_loss: 0.0098\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 114us/sample - loss: 0.0071 - val_loss: 0.0119\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0071 - val_loss: 0.0103\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0068 - val_loss: 0.0102\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 138us/sample - loss: 0.0067 - val_loss: 0.0093\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 132us/sample - loss: 0.0067 - val_loss: 0.0086\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0096\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 127us/sample - loss: 0.0066 - val_loss: 0.0086\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 202us/sample - loss: 0.0065 - val_loss: 0.0093\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0065 - val_loss: 0.0098\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0065 - val_loss: 0.0095\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 127us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 142us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0064 - val_loss: 0.0081\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 104us/sample - loss: 0.0065 - val_loss: 0.0095\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0065 - val_loss: 0.0105\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 142us/sample - loss: 0.0064 - val_loss: 0.0103\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 104us/sample - loss: 0.0063 - val_loss: 0.0093\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 110us/sample - loss: 0.0062 - val_loss: 0.0087\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0064 - val_loss: 0.0091\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0062 - val_loss: 0.0098\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0063 - val_loss: 0.0091\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 106us/sample - loss: 0.0063 - val_loss: 0.0093\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 127us/sample - loss: 0.0063 - val_loss: 0.0097\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 125us/sample - loss: 0.0061 - val_loss: 0.0090\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 132us/sample - loss: 0.0061 - val_loss: 0.0098\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0061 - val_loss: 0.0087\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0061 - val_loss: 0.0091\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 132us/sample - loss: 0.0060 - val_loss: 0.0087\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0060 - val_loss: 0.0092\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 145us/sample - loss: 0.0060 - val_loss: 0.0082\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0060 - val_loss: 0.0097\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 117us/sample - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0058 - val_loss: 0.0078\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 125us/sample - loss: 0.0059 - val_loss: 0.0093\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 126us/sample - loss: 0.0060 - val_loss: 0.0082\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 140us/sample - loss: 0.0061 - val_loss: 0.0079\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0063 - val_loss: 0.0091\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0061 - val_loss: 0.0098\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 118us/sample - loss: 0.0059 - val_loss: 0.0087\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 126us/sample - loss: 0.0057 - val_loss: 0.0087\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0059 - val_loss: 0.0085\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 169us/sample - loss: 0.0059 - val_loss: 0.0091\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 118us/sample - loss: 0.0058 - val_loss: 0.0085\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0057 - val_loss: 0.0090\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0057 - val_loss: 0.0082\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 115us/sample - loss: 0.0058 - val_loss: 0.0099\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 119us/sample - loss: 0.0058 - val_loss: 0.0105\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 148us/sample - loss: 0.0058 - val_loss: 0.0083\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0057 - val_loss: 0.0088\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 136us/sample - loss: 0.0056 - val_loss: 0.0086\n",
      "Epoch 59/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.005 - 0s 151us/sample - loss: 0.0056 - val_loss: 0.0090\n",
      "Epoch 60/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 148us/sample - loss: 0.0057 - val_loss: 0.0088\n",
      "Epoch 61/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 152us/sample - loss: 0.0058 - val_loss: 0.0100\n",
      "Epoch 62/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.005 - 0s 163us/sample - loss: 0.0058 - val_loss: 0.0090\n",
      "Epoch 63/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.005 - 0s 191us/sample - loss: 0.0056 - val_loss: 0.0082\n",
      "Epoch 64/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.005 - 0s 161us/sample - loss: 0.0056 - val_loss: 0.0079\n",
      "Epoch 65/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.005 - 0s 159us/sample - loss: 0.0058 - val_loss: 0.0083\n",
      "Epoch 66/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.005 - 0s 155us/sample - loss: 0.0055 - val_loss: 0.0086\n",
      "Epoch 67/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 140us/sample - loss: 0.0056 - val_loss: 0.0092\n",
      "Epoch 68/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 144us/sample - loss: 0.0054 - val_loss: 0.0096\n",
      "Epoch 69/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 136us/sample - loss: 0.0054 - val_loss: 0.0077\n",
      "Epoch 70/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 155us/sample - loss: 0.0055 - val_loss: 0.0082\n",
      "Epoch 71/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0054 - val_loss: 0.0089\n",
      "Epoch 72/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 128us/sample - loss: 0.0055 - val_loss: 0.0103\n",
      "Epoch 73/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 134us/sample - loss: 0.0057 - val_loss: 0.0101\n",
      "Epoch 74/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0054 - val_loss: 0.0076\n",
      "Epoch 75/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0055 - val_loss: 0.0077\n",
      "Epoch 76/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0054 - val_loss: 0.0084\n",
      "Epoch 77/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.005 - 0s 142us/sample - loss: 0.0053 - val_loss: 0.0086\n",
      "Epoch 78/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.005 - 0s 184us/sample - loss: 0.0054 - val_loss: 0.0080\n",
      "Epoch 79/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 144us/sample - loss: 0.0052 - val_loss: 0.0085\n",
      "Epoch 80/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 125us/sample - loss: 0.0053 - val_loss: 0.0082\n",
      "Epoch 81/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 132us/sample - loss: 0.0054 - val_loss: 0.0081\n",
      "Epoch 82/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 123us/sample - loss: 0.0052 - val_loss: 0.0085\n",
      "Epoch 83/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 119us/sample - loss: 0.0054 - val_loss: 0.0092\n",
      "Epoch 84/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0051 - val_loss: 0.0082\n",
      "Epoch 85/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 134us/sample - loss: 0.0052 - val_loss: 0.0100\n",
      "Epoch 86/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 153us/sample - loss: 0.0052 - val_loss: 0.0098\n",
      "Epoch 87/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0051 - val_loss: 0.0082\n",
      "Epoch 88/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 142us/sample - loss: 0.0052 - val_loss: 0.0079\n",
      "Epoch 89/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0051 - val_loss: 0.0083\n",
      "Epoch 90/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 144us/sample - loss: 0.0050 - val_loss: 0.0078\n",
      "Epoch 91/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.005 - 0s 155us/sample - loss: 0.0052 - val_loss: 0.0083\n",
      "Epoch 92/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 130us/sample - loss: 0.0049 - val_loss: 0.0093\n",
      "Epoch 93/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 150us/sample - loss: 0.0051 - val_loss: 0.0089\n",
      "Epoch 94/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 152us/sample - loss: 0.0051 - val_loss: 0.0085\n",
      "Epoch 95/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.005 - 0s 146us/sample - loss: 0.0050 - val_loss: 0.0085\n",
      "Epoch 96/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.004 - 0s 145us/sample - loss: 0.0049 - val_loss: 0.0098\n",
      "Epoch 97/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 146us/sample - loss: 0.0051 - val_loss: 0.0103\n",
      "Epoch 98/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0050 - val_loss: 0.0094\n",
      "Epoch 99/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0050 - val_loss: 0.0080\n",
      "Epoch 100/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 105us/sample - loss: 0.0048 - val_loss: 0.0092\n",
      "Epoch 101/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0049 - val_loss: 0.0082\n",
      "Epoch 102/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0052 - val_loss: 0.0079\n",
      "Epoch 103/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 104us/sample - loss: 0.0050 - val_loss: 0.0093\n",
      "Epoch 104/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 112us/sample - loss: 0.0052 - val_loss: 0.0083\n",
      "Model: \"sequential_286\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2002 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_2003 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2004 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2005 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2006 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2007 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2008 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.106 - 1s 1ms/sample - loss: 0.0712 - val_loss: 0.0350\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.033 - 0s 122us/sample - loss: 0.0182 - val_loss: 0.0084\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.007 - 0s 169us/sample - loss: 0.0069 - val_loss: 0.0079\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0069 - val_loss: 0.0081\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 131us/sample - loss: 0.0068 - val_loss: 0.0080\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0068 - val_loss: 0.0081\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0067 - val_loss: 0.0079\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0067 - val_loss: 0.0087\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0067 - val_loss: 0.0083\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0067 - val_loss: 0.0083\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0079\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 105us/sample - loss: 0.0067 - val_loss: 0.0086\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0067 - val_loss: 0.0094\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 113us/sample - loss: 0.0068 - val_loss: 0.0082\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 106us/sample - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0081\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 116us/sample - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 119us/sample - loss: 0.0065 - val_loss: 0.0079\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0065 - val_loss: 0.0081\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0099\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 123us/sample - loss: 0.0065 - val_loss: 0.0092\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 131us/sample - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0065 - val_loss: 0.0082\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 121us/sample - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 127us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 220us/sample - loss: 0.0064 - val_loss: 0.0090\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0064 - val_loss: 0.0094\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0065 - val_loss: 0.0083\n",
      "Model: \"sequential_287\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2009 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_2010 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2011 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2012 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2013 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2014 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2015 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.222 - 1s 1ms/sample - loss: 0.1072 - val_loss: 0.0678\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.072 - 0s 124us/sample - loss: 0.0694 - val_loss: 0.0575\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.071 - 0s 128us/sample - loss: 0.0590 - val_loss: 0.0483\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.067 - 0s 125us/sample - loss: 0.0497 - val_loss: 0.0403\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.040 - 0s 138us/sample - loss: 0.0417 - val_loss: 0.0336\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.029 - 0s 126us/sample - loss: 0.0349 - val_loss: 0.0278\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.022 - 0s 134us/sample - loss: 0.0291 - val_loss: 0.0231\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.022 - 0s 122us/sample - loss: 0.0243 - val_loss: 0.0193\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.022 - 0s 126us/sample - loss: 0.0204 - val_loss: 0.0163\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.016 - 0s 128us/sample - loss: 0.0172 - val_loss: 0.0138\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - 0s 131us/sample - loss: 0.0146 - val_loss: 0.0119\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 123us/sample - loss: 0.0126 - val_loss: 0.0104\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0110 - val_loss: 0.0094\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 121us/sample - loss: 0.0098 - val_loss: 0.0086\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 110us/sample - loss: 0.0083 - val_loss: 0.0077\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0078 - val_loss: 0.0075\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 105us/sample - loss: 0.0074 - val_loss: 0.0073\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 105us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 116us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 118us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 119us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 121us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 105us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 125us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 171us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_288\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2016 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_2017 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2018 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2019 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2020 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2021 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2022 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 5s - loss: 0.103 - 1s 1ms/sample - loss: 0.0830 - val_loss: 0.0671\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.070 - 0s 179us/sample - loss: 0.0683 - val_loss: 0.0560\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.058 - 0s 130us/sample - loss: 0.0572 - val_loss: 0.0465\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.059 - 0s 110us/sample - loss: 0.0477 - val_loss: 0.0384\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.035 - 0s 115us/sample - loss: 0.0396 - val_loss: 0.0316\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.045 - 0s 118us/sample - loss: 0.0328 - val_loss: 0.0261\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.040 - 0s 110us/sample - loss: 0.0272 - val_loss: 0.0216\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.036 - 0s 112us/sample - loss: 0.0226 - val_loss: 0.0179\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.025 - 0s 126us/sample - loss: 0.0189 - val_loss: 0.0150\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.017 - 0s 113us/sample - loss: 0.0159 - val_loss: 0.0128\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.021 - 0s 111us/sample - loss: 0.0135 - val_loss: 0.0111\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - 0s 119us/sample - loss: 0.0117 - val_loss: 0.0098\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 110us/sample - loss: 0.0103 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0093 - val_loss: 0.0082\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 110us/sample - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 105us/sample - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 107us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 136us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 134us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 108us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 127us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 136us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_289\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2023 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_2024 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2025 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2026 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2027 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2028 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2029 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.074 - 1s 1ms/sample - loss: 0.0563 - val_loss: 0.0190\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.021 - 0s 113us/sample - loss: 0.0120 - val_loss: 0.0122\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 109us/sample - loss: 0.0085 - val_loss: 0.0099\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 114us/sample - loss: 0.0077 - val_loss: 0.0089\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 109us/sample - loss: 0.0072 - val_loss: 0.0090\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 104us/sample - loss: 0.0070 - val_loss: 0.0096\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0069 - val_loss: 0.0099\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 109us/sample - loss: 0.0071 - val_loss: 0.0093\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0068 - val_loss: 0.0089\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 110us/sample - loss: 0.0067 - val_loss: 0.0086\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0067 - val_loss: 0.0088\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 108us/sample - loss: 0.0066 - val_loss: 0.0086\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0065 - val_loss: 0.0092\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0065 - val_loss: 0.0090\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 201us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0065 - val_loss: 0.0085\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0065 - val_loss: 0.0090\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 142us/sample - loss: 0.0066 - val_loss: 0.0090\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0064 - val_loss: 0.0090\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0064 - val_loss: 0.0091\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0067 - val_loss: 0.0091\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 156us/sample - loss: 0.0065 - val_loss: 0.0088\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 171us/sample - loss: 0.0064 - val_loss: 0.0092\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0065 - val_loss: 0.0093\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 154us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 143us/sample - loss: 0.0064 - val_loss: 0.0092\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 126us/sample - loss: 0.0064 - val_loss: 0.0095\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0064 - val_loss: 0.0092\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0063 - val_loss: 0.0080\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 115us/sample - loss: 0.0063 - val_loss: 0.0090\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 132us/sample - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 127us/sample - loss: 0.0063 - val_loss: 0.0089\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0063 - val_loss: 0.0092\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 122us/sample - loss: 0.0062 - val_loss: 0.0087\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0062 - val_loss: 0.0088\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0062 - val_loss: 0.0089\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0062 - val_loss: 0.0088\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0062 - val_loss: 0.0097\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0064 - val_loss: 0.0093\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0062 - val_loss: 0.0086\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0061 - val_loss: 0.0085\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 153us/sample - loss: 0.0061 - val_loss: 0.0086\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 131us/sample - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 132us/sample - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 129us/sample - loss: 0.0061 - val_loss: 0.0089\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0061 - val_loss: 0.0084\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0061 - val_loss: 0.0086\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0060 - val_loss: 0.0092\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0061 - val_loss: 0.0087\n",
      "Epoch 59/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 60/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 108us/sample - loss: 0.0060 - val_loss: 0.0081\n",
      "Epoch 61/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 119us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Model: \"sequential_290\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2030 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_2031 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2032 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2033 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2034 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2035 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2036 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.101 - 1s 1ms/sample - loss: 0.0810 - val_loss: 0.0671\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.092 - 0s 128us/sample - loss: 0.0682 - val_loss: 0.0559\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.057 - 0s 154us/sample - loss: 0.0572 - val_loss: 0.0465\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.050 - 0s 124us/sample - loss: 0.0477 - val_loss: 0.0384\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.044 - 0s 124us/sample - loss: 0.0397 - val_loss: 0.0316\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.034 - 0s 124us/sample - loss: 0.0329 - val_loss: 0.0261\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.030 - 0s 124us/sample - loss: 0.0274 - val_loss: 0.0216\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.031 - 0s 136us/sample - loss: 0.0227 - val_loss: 0.0179\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.019 - 0s 157us/sample - loss: 0.0190 - val_loss: 0.0151\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.016 - 0s 157us/sample - loss: 0.0161 - val_loss: 0.0128\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.013 - 0s 157us/sample - loss: 0.0137 - val_loss: 0.0112\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.011 - 0s 155us/sample - loss: 0.0119 - val_loss: 0.0099\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 134us/sample - loss: 0.0104 - val_loss: 0.0090\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 138us/sample - loss: 0.0094 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - 0s 134us/sample - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 127us/sample - loss: 0.0080 - val_loss: 0.0076\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.007 - 0s 237us/sample - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.007 - 0s 159us/sample - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.007 - 0s 155us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 151us/sample - loss: 0.0069 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 153us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 142us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 117us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 104us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 121us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 165us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_291\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2037 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_2038 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2039 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2040 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2041 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2042 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2043 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.084 - 1s 1ms/sample - loss: 0.0581 - val_loss: 0.0259\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.030 - 0s 132us/sample - loss: 0.0136 - val_loss: 0.0105\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.007 - 0s 161us/sample - loss: 0.0075 - val_loss: 0.0084\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.007 - 0s 148us/sample - loss: 0.0070 - val_loss: 0.0089\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 132us/sample - loss: 0.0069 - val_loss: 0.0085\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0069 - val_loss: 0.0085\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 130us/sample - loss: 0.0068 - val_loss: 0.0084\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0068 - val_loss: 0.0087\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0067 - val_loss: 0.0086\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0067 - val_loss: 0.0088\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0067 - val_loss: 0.0087\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 123us/sample - loss: 0.0067 - val_loss: 0.0090\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0065 - val_loss: 0.0088\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 132us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 106us/sample - loss: 0.0064 - val_loss: 0.0087\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 103us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 110us/sample - loss: 0.0063 - val_loss: 0.0093\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0064 - val_loss: 0.0087\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0063 - val_loss: 0.0089\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 104us/sample - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 105us/sample - loss: 0.0062 - val_loss: 0.0087\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0062 - val_loss: 0.0086\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 175us/sample - loss: 0.0062 - val_loss: 0.0089\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 160us/sample - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0063 - val_loss: 0.0094\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 136us/sample - loss: 0.0062 - val_loss: 0.0088\n",
      "Model: \"sequential_292\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2044 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_2045 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2046 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2047 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2048 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2049 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2050 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.090 - 1s 1ms/sample - loss: 0.0813 - val_loss: 0.0671\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 153us/sample - loss: 0.0683 - val_loss: 0.0560\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.057 - 0s 153us/sample - loss: 0.0572 - val_loss: 0.0465\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.043 - ETA: 0s - loss: 0.048 - 0s 177us/sample - loss: 0.0477 - val_loss: 0.0384\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.040 - ETA: 0s - loss: 0.040 - 0s 163us/sample - loss: 0.0396 - val_loss: 0.0316\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.024 - 0s 143us/sample - loss: 0.0328 - val_loss: 0.0260\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.027 - 0s 165us/sample - loss: 0.0271 - val_loss: 0.0214\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.022 - 0s 160us/sample - loss: 0.0226 - val_loss: 0.0178\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.018 - 0s 162us/sample - loss: 0.0189 - val_loss: 0.0150\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.024 - ETA: 0s - loss: 0.015 - 0s 165us/sample - loss: 0.0159 - val_loss: 0.0128\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.013 - 0s 177us/sample - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 0s 155us/sample - loss: 0.0118 - val_loss: 0.0098\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 126us/sample - loss: 0.0104 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 135us/sample - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.008 - 0s 173us/sample - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.007 - 0s 145us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 110us/sample - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 113us/sample - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 109us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 111us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 116us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 121us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 112us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 112us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 112us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 114us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 169us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 132us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - 0s 119us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 121us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 127us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_293\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2051 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_2052 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2053 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2054 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2055 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2056 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2057 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 5s - loss: 0.084 - 1s 1ms/sample - loss: 0.0301 - val_loss: 0.0226\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 126us/sample - loss: 0.0083 - val_loss: 0.0103\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0071 - val_loss: 0.0115\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 121us/sample - loss: 0.0071 - val_loss: 0.0095\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0070 - val_loss: 0.0102\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0069 - val_loss: 0.0102\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0068 - val_loss: 0.0102\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0067 - val_loss: 0.0101\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0067 - val_loss: 0.0107\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 138us/sample - loss: 0.0066 - val_loss: 0.0105\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 124us/sample - loss: 0.0067 - val_loss: 0.0108\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 103us/sample - loss: 0.0067 - val_loss: 0.0097\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 117us/sample - loss: 0.0065 - val_loss: 0.0088\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 198us/sample - loss: 0.0065 - val_loss: 0.0090\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0065 - val_loss: 0.0100\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0064 - val_loss: 0.0093\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 145us/sample - loss: 0.0064 - val_loss: 0.0090\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0064 - val_loss: 0.0096\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0063 - val_loss: 0.0088\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0063 - val_loss: 0.0089\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0063 - val_loss: 0.0100\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 123us/sample - loss: 0.0062 - val_loss: 0.0091\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0062 - val_loss: 0.0095\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0063 - val_loss: 0.0096\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0063 - val_loss: 0.0092\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0062 - val_loss: 0.0090\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 128us/sample - loss: 0.0062 - val_loss: 0.0086\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 121us/sample - loss: 0.0061 - val_loss: 0.0084\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 126us/sample - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 124us/sample - loss: 0.0060 - val_loss: 0.0094\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0062 - val_loss: 0.0091\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0060 - val_loss: 0.0089\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0060 - val_loss: 0.0095\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0061 - val_loss: 0.0084\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0060 - val_loss: 0.0088\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 105us/sample - loss: 0.0060 - val_loss: 0.0094\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 99us/sample - loss: 0.0061 - val_loss: 0.0087\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 103us/sample - loss: 0.0060 - val_loss: 0.0084\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 103us/sample - loss: 0.0060 - val_loss: 0.0082\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0060 - val_loss: 0.0078\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 108us/sample - loss: 0.0059 - val_loss: 0.0094\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 103us/sample - loss: 0.0059 - val_loss: 0.0087\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 104us/sample - loss: 0.0058 - val_loss: 0.0085\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 107us/sample - loss: 0.0058 - val_loss: 0.0087\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 97us/sample - loss: 0.0059 - val_loss: 0.0084\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 105us/sample - loss: 0.0059 - val_loss: 0.0086\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 102us/sample - loss: 0.0059 - val_loss: 0.0086\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 107us/sample - loss: 0.0059 - val_loss: 0.0096\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0060 - val_loss: 0.0083\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0058 - val_loss: 0.0091\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 122us/sample - loss: 0.0059 - val_loss: 0.0090\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 115us/sample - loss: 0.0059 - val_loss: 0.0089\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 116us/sample - loss: 0.0058 - val_loss: 0.0082\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 132us/sample - loss: 0.0056 - val_loss: 0.0084\n",
      "Epoch 59/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 155us/sample - loss: 0.0057 - val_loss: 0.0099\n",
      "Epoch 60/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 155us/sample - loss: 0.0058 - val_loss: 0.0084\n",
      "Epoch 61/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 185us/sample - loss: 0.0057 - val_loss: 0.0088\n",
      "Epoch 62/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 173us/sample - loss: 0.0057 - val_loss: 0.0081\n",
      "Epoch 63/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 161us/sample - loss: 0.0057 - val_loss: 0.0090\n",
      "Epoch 64/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 159us/sample - loss: 0.0057 - val_loss: 0.0083\n",
      "Epoch 65/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 155us/sample - loss: 0.0058 - val_loss: 0.0084\n",
      "Epoch 66/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 148us/sample - loss: 0.0057 - val_loss: 0.0083\n",
      "Epoch 67/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 161us/sample - loss: 0.0055 - val_loss: 0.0095\n",
      "Epoch 68/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0055 - val_loss: 0.0096\n",
      "Epoch 69/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 148us/sample - loss: 0.0057 - val_loss: 0.0087\n",
      "Epoch 70/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 136us/sample - loss: 0.0058 - val_loss: 0.0081\n",
      "Epoch 71/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.005 - 0s 167us/sample - loss: 0.0059 - val_loss: 0.0088\n",
      "Epoch 72/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0059 - val_loss: 0.0107\n",
      "Epoch 73/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 130us/sample - loss: 0.0056 - val_loss: 0.0098\n",
      "Epoch 74/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 132us/sample - loss: 0.0057 - val_loss: 0.0089\n",
      "Model: \"sequential_294\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2058 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_2059 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2060 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2061 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2062 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2063 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2064 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 5s - loss: 0.079 - ETA: 0s - loss: 0.080 - 1s 1ms/sample - loss: 0.0806 - val_loss: 0.0649\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.063 - 0s 218us/sample - loss: 0.0587 - val_loss: 0.0322\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.016 - 0s 149us/sample - loss: 0.0166 - val_loss: 0.0152\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0079 - val_loss: 0.0093\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 0s 154us/sample - loss: 0.0074 - val_loss: 0.0093\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.007 - 0s 152us/sample - loss: 0.0072 - val_loss: 0.0095\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 136us/sample - loss: 0.0071 - val_loss: 0.0094\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 160us/sample - loss: 0.0070 - val_loss: 0.0104\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 173us/sample - loss: 0.0069 - val_loss: 0.0090\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0070 - val_loss: 0.0089\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0069 - val_loss: 0.0103\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.007 - 0s 161us/sample - loss: 0.0069 - val_loss: 0.0099\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.007 - 0s 179us/sample - loss: 0.0068 - val_loss: 0.0102\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0068 - val_loss: 0.0087\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.006 - 0s 165us/sample - loss: 0.0067 - val_loss: 0.0093\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0067 - val_loss: 0.0093\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 171us/sample - loss: 0.0067 - val_loss: 0.0094\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 174us/sample - loss: 0.0067 - val_loss: 0.0101\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.007 - 0s 159us/sample - loss: 0.0066 - val_loss: 0.0094\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 132us/sample - loss: 0.0067 - val_loss: 0.0090\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0066 - val_loss: 0.0089\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0106\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0092\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 132us/sample - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 130us/sample - loss: 0.0067 - val_loss: 0.0101\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0066 - val_loss: 0.0091\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 124us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0065 - val_loss: 0.0093\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 120us/sample - loss: 0.0065 - val_loss: 0.0094\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 117us/sample - loss: 0.0065 - val_loss: 0.0097\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 132us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0065 - val_loss: 0.0097\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 171us/sample - loss: 0.0065 - val_loss: 0.0090\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0064 - val_loss: 0.0100\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0065 - val_loss: 0.0100\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0065 - val_loss: 0.0090\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0065 - val_loss: 0.0088\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0063 - val_loss: 0.0092\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0063 - val_loss: 0.0088\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - 0s 128us/sample - loss: 0.0064 - val_loss: 0.0093\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 119us/sample - loss: 0.0064 - val_loss: 0.0095\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 132us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Model: \"sequential_295\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2065 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_2066 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2067 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2068 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2069 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2070 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2071 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.084 - 1s 1ms/sample - loss: 0.0306 - val_loss: 0.0149\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0086 - val_loss: 0.0088\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0075 - val_loss: 0.0091\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0071 - val_loss: 0.0096\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0070 - val_loss: 0.0098\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0069 - val_loss: 0.0091\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0068 - val_loss: 0.0090\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0067 - val_loss: 0.0092\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 110us/sample - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 112us/sample - loss: 0.0067 - val_loss: 0.0099\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0089\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 106us/sample - loss: 0.0066 - val_loss: 0.0092\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0066 - val_loss: 0.0095\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0069 - val_loss: 0.0085\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0065 - val_loss: 0.0094\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0065 - val_loss: 0.0090\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0065 - val_loss: 0.0090\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0064 - val_loss: 0.0089\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0064 - val_loss: 0.0098\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0065 - val_loss: 0.0101\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 125us/sample - loss: 0.0065 - val_loss: 0.0096\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0065 - val_loss: 0.0094\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 130us/sample - loss: 0.0064 - val_loss: 0.0093\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 126us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0063 - val_loss: 0.0095\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0064 - val_loss: 0.0111\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0105\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0065 - val_loss: 0.0088\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 111us/sample - loss: 0.0064 - val_loss: 0.0101\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0064 - val_loss: 0.0089\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0063 - val_loss: 0.0091\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 114us/sample - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0063 - val_loss: 0.0088\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0063 - val_loss: 0.0086\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 115us/sample - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0062 - val_loss: 0.0097\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 120us/sample - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0062 - val_loss: 0.0088\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 122us/sample - loss: 0.0061 - val_loss: 0.0084\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 121us/sample - loss: 0.0063 - val_loss: 0.0099\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0062 - val_loss: 0.0092\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0061 - val_loss: 0.0084\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 153us/sample - loss: 0.0061 - val_loss: 0.0094\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0060 - val_loss: 0.0096\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 115us/sample - loss: 0.0061 - val_loss: 0.0106\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0061 - val_loss: 0.0095\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0062 - val_loss: 0.0096\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0060 - val_loss: 0.0087\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 126us/sample - loss: 0.0060 - val_loss: 0.0092\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 127us/sample - loss: 0.0060 - val_loss: 0.0086\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0060 - val_loss: 0.0092\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0060 - val_loss: 0.0086\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0060 - val_loss: 0.0080\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0060 - val_loss: 0.0092\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 165us/sample - loss: 0.0060 - val_loss: 0.0097\n",
      "Epoch 59/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0059 - val_loss: 0.0094\n",
      "Epoch 60/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0060 - val_loss: 0.0090\n",
      "Epoch 61/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0059 - val_loss: 0.0093\n",
      "Epoch 62/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 107us/sample - loss: 0.0059 - val_loss: 0.0086\n",
      "Epoch 63/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 64/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 107us/sample - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 65/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 108us/sample - loss: 0.0061 - val_loss: 0.0085\n",
      "Epoch 66/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0058 - val_loss: 0.0094\n",
      "Epoch 67/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 109us/sample - loss: 0.0058 - val_loss: 0.0095\n",
      "Epoch 68/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 114us/sample - loss: 0.0058 - val_loss: 0.0091\n",
      "Epoch 69/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 111us/sample - loss: 0.0058 - val_loss: 0.0091\n",
      "Epoch 70/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0057 - val_loss: 0.0088\n",
      "Epoch 71/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 109us/sample - loss: 0.0061 - val_loss: 0.0087\n",
      "Epoch 72/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0057 - val_loss: 0.0103\n",
      "Epoch 73/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 110us/sample - loss: 0.0060 - val_loss: 0.0104\n",
      "Epoch 74/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 111us/sample - loss: 0.0056 - val_loss: 0.0093\n",
      "Epoch 75/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 109us/sample - loss: 0.0057 - val_loss: 0.0100\n",
      "Epoch 76/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0056 - val_loss: 0.0111\n",
      "Epoch 77/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 119us/sample - loss: 0.0056 - val_loss: 0.0096\n",
      "Epoch 78/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0056 - val_loss: 0.0101\n",
      "Epoch 79/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0057 - val_loss: 0.0097\n",
      "Epoch 80/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0057 - val_loss: 0.0094\n",
      "Epoch 81/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 132us/sample - loss: 0.0057 - val_loss: 0.0090\n",
      "Epoch 82/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 128us/sample - loss: 0.0057 - val_loss: 0.0099\n",
      "Epoch 83/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.005 - 0s 148us/sample - loss: 0.0058 - val_loss: 0.0098\n",
      "Epoch 84/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 157us/sample - loss: 0.0057 - val_loss: 0.0087\n",
      "Epoch 85/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.005 - 0s 142us/sample - loss: 0.0055 - val_loss: 0.0086\n",
      "Epoch 86/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 128us/sample - loss: 0.0056 - val_loss: 0.0092\n",
      "Model: \"sequential_296\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2072 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_2073 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2074 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2075 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2076 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2077 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2078 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 4s - loss: 0.095 - 1s 1ms/sample - loss: 0.0817 - val_loss: 0.0671\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.067 - 0s 130us/sample - loss: 0.0682 - val_loss: 0.0560\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.064 - 0s 128us/sample - loss: 0.0572 - val_loss: 0.0464\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.059 - 0s 132us/sample - loss: 0.0476 - val_loss: 0.0383\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.047 - ETA: 0s - loss: 0.039 - 0s 159us/sample - loss: 0.0395 - val_loss: 0.0315\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.044 - ETA: 0s - loss: 0.033 - 0s 165us/sample - loss: 0.0328 - val_loss: 0.0259\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.027 - 0s 163us/sample - loss: 0.0272 - val_loss: 0.0215\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.023 - 0s 168us/sample - loss: 0.0226 - val_loss: 0.0178\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.019 - 0s 163us/sample - loss: 0.0189 - val_loss: 0.0150\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.016 - 0s 171us/sample - loss: 0.0159 - val_loss: 0.0128\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 0s 165us/sample - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 0s 159us/sample - loss: 0.0118 - val_loss: 0.0098\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.010 - 0s 171us/sample - loss: 0.0104 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 134us/sample - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.008 - 0s 156us/sample - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.007 - 0s 155us/sample - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.007 - 0s 148us/sample - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.007 - 0s 155us/sample - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.007 - 0s 155us/sample - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 134us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0068 - val_loss: 0.0072\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 151us/sample - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 153us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 154us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 169us/sample - loss: 0.0066 - val_loss: 0.0073\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 165us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 128us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 120us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 111us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 157us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 161us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 175us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 162us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 165us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0066 - val_loss: 0.0074\n",
      "Model: \"sequential_297\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2079 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_2080 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2081 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2082 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2083 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2084 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2085 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.082 - 0s 986us/sample - loss: 0.0707 - val_loss: 0.0370\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.032 - 0s 189us/sample - loss: 0.0186 - val_loss: 0.0167\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 103us/sample - loss: 0.0076 - val_loss: 0.0095\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 101us/sample - loss: 0.0071 - val_loss: 0.0099\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 108us/sample - loss: 0.0070 - val_loss: 0.0098\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 107us/sample - loss: 0.0069 - val_loss: 0.0105\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 112us/sample - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 112us/sample - loss: 0.0068 - val_loss: 0.0105\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 117us/sample - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0067 - val_loss: 0.0105\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0067 - val_loss: 0.0097\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 122us/sample - loss: 0.0067 - val_loss: 0.0096\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 145us/sample - loss: 0.0067 - val_loss: 0.0089\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 134us/sample - loss: 0.0066 - val_loss: 0.0107\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0102\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 126us/sample - loss: 0.0066 - val_loss: 0.0116\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0066 - val_loss: 0.0090\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 124us/sample - loss: 0.0064 - val_loss: 0.0102\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0065 - val_loss: 0.0096\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 114us/sample - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0064 - val_loss: 0.0099\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 144us/sample - loss: 0.0064 - val_loss: 0.0091\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - ETA: 0s - loss: 0.006 - 0s 175us/sample - loss: 0.0064 - val_loss: 0.0092\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0063 - val_loss: 0.0094\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 130us/sample - loss: 0.0064 - val_loss: 0.0091\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 121us/sample - loss: 0.0063 - val_loss: 0.0093\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 101us/sample - loss: 0.0063 - val_loss: 0.0096\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 101us/sample - loss: 0.0063 - val_loss: 0.0092\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 105us/sample - loss: 0.0062 - val_loss: 0.0088\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 107us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 111us/sample - loss: 0.0063 - val_loss: 0.0086\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 103us/sample - loss: 0.0062 - val_loss: 0.0095\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 107us/sample - loss: 0.0062 - val_loss: 0.0091\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 102us/sample - loss: 0.0061 - val_loss: 0.0096\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 103us/sample - loss: 0.0063 - val_loss: 0.0089\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 100us/sample - loss: 0.0065 - val_loss: 0.0101\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 113us/sample - loss: 0.0063 - val_loss: 0.0089\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 102us/sample - loss: 0.0061 - val_loss: 0.0091\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 113us/sample - loss: 0.0061 - val_loss: 0.0093\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0062 - val_loss: 0.0098\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 119us/sample - loss: 0.0061 - val_loss: 0.0093\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 128us/sample - loss: 0.0061 - val_loss: 0.0095\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0061 - val_loss: 0.0085\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 130us/sample - loss: 0.0061 - val_loss: 0.0095\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 130us/sample - loss: 0.0060 - val_loss: 0.0091\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 146us/sample - loss: 0.0060 - val_loss: 0.0091\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 124us/sample - loss: 0.0060 - val_loss: 0.0091\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 137us/sample - loss: 0.0060 - val_loss: 0.0085\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0061 - val_loss: 0.0091\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 115us/sample - loss: 0.0060 - val_loss: 0.0090\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 120us/sample - loss: 0.0063 - val_loss: 0.0101\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 123us/sample - loss: 0.0060 - val_loss: 0.0091\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.005 - 0s 142us/sample - loss: 0.0059 - val_loss: 0.0089\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 103us/sample - loss: 0.0059 - val_loss: 0.0089\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 97us/sample - loss: 0.0059 - val_loss: 0.0089\n",
      "Epoch 58/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 105us/sample - loss: 0.0059 - val_loss: 0.0092\n",
      "Epoch 59/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 103us/sample - loss: 0.0059 - val_loss: 0.0092\n",
      "Epoch 60/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0059 - val_loss: 0.0095\n",
      "Epoch 61/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 109us/sample - loss: 0.0059 - val_loss: 0.0097\n",
      "Epoch 62/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 113us/sample - loss: 0.0061 - val_loss: 0.0096\n",
      "Model: \"sequential_298\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2086 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_2087 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2088 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2089 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2090 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2091 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2092 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.078 - 1s 1ms/sample - loss: 0.0456 - val_loss: 0.0131\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.011 - 0s 126us/sample - loss: 0.0092 - val_loss: 0.0116\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 122us/sample - loss: 0.0076 - val_loss: 0.0098\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0073 - val_loss: 0.0102\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 163us/sample - loss: 0.0072 - val_loss: 0.0092\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.007 - 0s 150us/sample - loss: 0.0071 - val_loss: 0.0096\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 117us/sample - loss: 0.0069 - val_loss: 0.0102\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 115us/sample - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 124us/sample - loss: 0.0068 - val_loss: 0.0108\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 103us/sample - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 109us/sample - loss: 0.0066 - val_loss: 0.0095\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 126us/sample - loss: 0.0067 - val_loss: 0.0103\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.002 - 0s 109us/sample - loss: 0.0068 - val_loss: 0.0083\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - 0s 101us/sample - loss: 0.0065 - val_loss: 0.0106\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 124us/sample - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 149us/sample - loss: 0.0064 - val_loss: 0.0094\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 110us/sample - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 103us/sample - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 101us/sample - loss: 0.0064 - val_loss: 0.0095\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 98us/sample - loss: 0.0064 - val_loss: 0.0089\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 113us/sample - loss: 0.0064 - val_loss: 0.0097\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0065 - val_loss: 0.0081\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 154us/sample - loss: 0.0064 - val_loss: 0.0089\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0064 - val_loss: 0.0093\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0063 - val_loss: 0.0079\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 118us/sample - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0063 - val_loss: 0.0095\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 142us/sample - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0062 - val_loss: 0.0090\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0066 - val_loss: 0.0090\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 35/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 36/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 148us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 37/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 145us/sample - loss: 0.0061 - val_loss: 0.0087\n",
      "Epoch 38/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 152us/sample - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 39/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 122us/sample - loss: 0.0061 - val_loss: 0.0088\n",
      "Epoch 40/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 119us/sample - loss: 0.0061 - val_loss: 0.0081\n",
      "Epoch 41/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 120us/sample - loss: 0.0061 - val_loss: 0.0091\n",
      "Epoch 42/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 117us/sample - loss: 0.0061 - val_loss: 0.0090\n",
      "Epoch 43/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 126us/sample - loss: 0.0062 - val_loss: 0.0097\n",
      "Epoch 44/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 154us/sample - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 45/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.006 - 0s 150us/sample - loss: 0.0061 - val_loss: 0.0086\n",
      "Epoch 46/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 126us/sample - loss: 0.0061 - val_loss: 0.0091\n",
      "Epoch 47/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 122us/sample - loss: 0.0061 - val_loss: 0.0091\n",
      "Epoch 48/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 122us/sample - loss: 0.0060 - val_loss: 0.0087\n",
      "Epoch 49/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 127us/sample - loss: 0.0059 - val_loss: 0.0104\n",
      "Epoch 50/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 116us/sample - loss: 0.0061 - val_loss: 0.0095\n",
      "Epoch 51/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 119us/sample - loss: 0.0060 - val_loss: 0.0093\n",
      "Epoch 52/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 117us/sample - loss: 0.0061 - val_loss: 0.0087\n",
      "Epoch 53/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 128us/sample - loss: 0.0060 - val_loss: 0.0092\n",
      "Epoch 54/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 114us/sample - loss: 0.0059 - val_loss: 0.0105\n",
      "Epoch 55/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 120us/sample - loss: 0.0061 - val_loss: 0.0084\n",
      "Epoch 56/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0059 - val_loss: 0.0082\n",
      "Epoch 57/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 124us/sample - loss: 0.0059 - val_loss: 0.0092\n",
      "Model: \"sequential_299\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2093 (Dense)           (None, 16)                208       \n",
      "_________________________________________________________________\n",
      "dense_2094 (Dense)           (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2095 (Dense)           (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2096 (Dense)           (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2097 (Dense)           (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_2098 (Dense)           (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2099 (Dense)           (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,465\n",
      "Trainable params: 1,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 484 samples, validate on 101 samples\n",
      "Epoch 1/150\n",
      "484/484 [==============================] - ETA: 3s - loss: 0.091 - 1s 1ms/sample - loss: 0.0552 - val_loss: 0.0118\n",
      "Epoch 2/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.009 - 0s 148us/sample - loss: 0.0096 - val_loss: 0.0105\n",
      "Epoch 3/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 120us/sample - loss: 0.0076 - val_loss: 0.0090\n",
      "Epoch 4/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 104us/sample - loss: 0.0069 - val_loss: 0.0078\n",
      "Epoch 5/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 105us/sample - loss: 0.0070 - val_loss: 0.0083\n",
      "Epoch 6/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - 0s 117us/sample - loss: 0.0068 - val_loss: 0.0086\n",
      "Epoch 7/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 121us/sample - loss: 0.0067 - val_loss: 0.0084\n",
      "Epoch 8/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 130us/sample - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 9/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 113us/sample - loss: 0.0066 - val_loss: 0.0084\n",
      "Epoch 10/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 127us/sample - loss: 0.0065 - val_loss: 0.0081\n",
      "Epoch 11/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - 0s 122us/sample - loss: 0.0065 - val_loss: 0.0086\n",
      "Epoch 12/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.010 - 0s 130us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 13/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - 0s 120us/sample - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 14/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - 0s 117us/sample - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 15/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - 0s 119us/sample - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 16/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - 0s 130us/sample - loss: 0.0064 - val_loss: 0.0086\n",
      "Epoch 17/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 154us/sample - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 18/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.005 - 0s 190us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 19/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 185us/sample - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 20/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 177us/sample - loss: 0.0063 - val_loss: 0.0085\n",
      "Epoch 21/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 155us/sample - loss: 0.0064 - val_loss: 0.0082\n",
      "Epoch 22/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 183us/sample - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 23/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.003 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0062 - val_loss: 0.0091\n",
      "Epoch 24/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 25/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 159us/sample - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 26/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 171us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 27/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0061 - val_loss: 0.0089\n",
      "Epoch 28/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.006 - ETA: 0s - loss: 0.006 - 0s 165us/sample - loss: 0.0062 - val_loss: 0.0089\n",
      "Epoch 29/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.007 - ETA: 0s - loss: 0.005 - 0s 165us/sample - loss: 0.0060 - val_loss: 0.0084\n",
      "Epoch 30/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.008 - ETA: 0s - loss: 0.006 - 0s 165us/sample - loss: 0.0060 - val_loss: 0.0095\n",
      "Epoch 31/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 167us/sample - loss: 0.0062 - val_loss: 0.0095\n",
      "Epoch 32/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.005 - ETA: 0s - loss: 0.006 - 0s 169us/sample - loss: 0.0062 - val_loss: 0.0087\n",
      "Epoch 33/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.004 - ETA: 0s - loss: 0.006 - 0s 163us/sample - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 34/150\n",
      "484/484 [==============================] - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.006 - 0s 175us/sample - loss: 0.0060 - val_loss: 0.0088\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(testFeature).reshape(testTarget.shape[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ae5dcbddc8>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(result)\n",
    "plt.plot(testTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27804601836204534"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27453590001451345"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTarget.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTarget.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
